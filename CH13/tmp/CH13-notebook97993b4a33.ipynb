{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Main chapter 13 homework"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "raw_text = open('/kaggle/input/dlhwch13/2600-0.txt', 'r', encoding=\"utf8\").read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "print(raw_text[:200])\n",
    "\n",
    "all_words = raw_text.split()\n",
    "unique_words = list(set(all_words))\n",
    "print(f'Number of unique words: {len(unique_words)}')\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print(f'Total characters: {n_chars}')\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print(f'Total vocabulary (unique characters): {n_vocab}')\n",
    "print(chars)\n",
    "\n",
    "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "print(char_to_index)\n",
    "\n",
    "seq_length = 160\n",
    "n_seq = int(n_chars / seq_length)\n",
    "\n",
    "X = np.zeros((n_seq, seq_length, n_vocab))\n",
    "Y = np.zeros((n_seq, seq_length, n_vocab))\n",
    "\n",
    "for i in range(n_seq):\n",
    "    x_sequence = raw_text[i * seq_length: (i + 1) * seq_length]\n",
    "    x_sequence_ohe = np.zeros((seq_length, n_vocab))\n",
    "    for j in range(seq_length):\n",
    "        char = x_sequence[j]\n",
    "        index = char_to_index[char]\n",
    "        x_sequence_ohe[j][index] = 1.\n",
    "    X[i] = x_sequence_ohe\n",
    "\n",
    "    y_sequence = raw_text[i * seq_length + 1: (i + 1) * seq_length + 1]\n",
    "    y_sequence_ohe = np.zeros((seq_length, n_vocab))\n",
    "    for j in range(seq_length):\n",
    "        char = y_sequence[j]\n",
    "        index = char_to_index[char]\n",
    "        y_sequence_ohe[j][index] = 1.\n",
    "    Y[i] = y_sequence_ohe\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "batch_size = 100\n",
    "hidden_units = 700\n",
    "n_epoch = 300\n",
    "dropout = 0.4\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(hidden_units, input_shape=(None, n_vocab), return_sequences=True, dropout=dropout))\n",
    "model.add(layers.LSTM(hidden_units, return_sequences=True, dropout=dropout))\n",
    "model.add(layers.TimeDistributed(layers.Dense(n_vocab, activation='softmax')))\n",
    "\n",
    "optimizer = optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "filepath = \"weights/weights_epoch_{epoch:03d}_loss_{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', min_delta=0, patience=50, verbose=1, mode='min')\n",
    "\n",
    "\n",
    "def generate_text(model, gen_length, n_vocab, index_to_char):\n",
    "    index = np.random.randint(n_vocab)\n",
    "    y_char = [index_to_char[index]]\n",
    "    X = np.zeros((1, gen_length, n_vocab))\n",
    "    for i in range(gen_length):\n",
    "        X[0, i, index] = 1.\n",
    "        indices = np.argmax(model.predict(X[:, max(0, i - 99):i + 1, :])[0], 1)\n",
    "        index = indices[-1]\n",
    "        y_char.append(index_to_char[index])\n",
    "    return ''.join(y_char)\n",
    "\n",
    "\n",
    "class ResultChecker(Callback):\n",
    "    def __init__(self, model, N, gen_length):\n",
    "        self.model = model\n",
    "        self.N = N\n",
    "        self.gen_length = gen_length\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.N == 0:\n",
    "            result = generate_text(self.model, self.gen_length, n_vocab, index_to_char)\n",
    "            print('\\nMy War and Peace:\\n' + result)\n",
    "\n",
    "\n",
    "result_checker = ResultChecker(model, 10, 500)\n",
    "\n",
    "model.fit(X, Y, batch_size=batch_size, verbose=1, epochs=n_epoch, callbacks=[result_checker, checkpoint, early_stop])\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-16T04:01:39.299610Z",
     "iopub.execute_input": "2022-07-16T04:01:39.300488Z",
     "iopub.status.idle": "2022-07-16T06:41:25.966258Z",
     "shell.execute_reply.started": "2022-07-16T04:01:39.300344Z",
     "shell.execute_reply": "2022-07-16T06:41:25.964371Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "﻿chapter i\n\n“well, prince, so genoa and lucca are now just family estates of the\nbuonapartes. but i warn you, if you don’t tell me that this means war,\nif you still try to defend the infamies and horr\nNumber of unique words: 40095\nTotal characters: 3220047\nTotal vocabulary (unique characters): 80\n['\\n', ' ', '!', '\"', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'á', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ï', 'ó', 'ô', 'ö', 'ú', 'ü', 'ý', 'œ', '—', '‘', '’', '“', '”', '\\ufeff']\n{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '=': 26, '?': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53, 'à': 54, 'á': 55, 'â': 56, 'ä': 57, 'æ': 58, 'ç': 59, 'è': 60, 'é': 61, 'ê': 62, 'ë': 63, 'í': 64, 'î': 65, 'ï': 66, 'ó': 67, 'ô': 68, 'ö': 69, 'ú': 70, 'ü': 71, 'ý': 72, 'œ': 73, '—': 74, '‘': 75, '’': 76, '“': 77, '”': 78, '\\ufeff': 79}\n(20125, 160, 80)\n(20125, 160, 80)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-07-16 04:01:52.555620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 04:01:52.671135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 04:01:52.671936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 04:01:52.674189: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-16 04:01:52.674562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 04:01:52.675679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 04:01:52.676784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 04:01:55.211139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 04:01:55.212075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 04:01:55.212723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 04:01:55.213301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, None, 700)         2186800   \n_________________________________________________________________\nlstm_1 (LSTM)                (None, None, 700)         3922800   \n_________________________________________________________________\ntime_distributed (TimeDistri (None, None, 80)          56080     \n=================================================================\nTotal params: 6,165,680\nTrainable params: 6,165,680\nNon-trainable params: 0\n_________________________________________________________________\nNone\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-07-16 04:01:57.407005: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1030400000 exceeds 10% of free system memory.\n2022-07-16 04:01:58.821666: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1030400000 exceeds 10% of free system memory.\n2022-07-16 04:02:00.005722: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1030400000 exceeds 10% of free system memory.\n2022-07-16 04:02:00.773789: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1030400000 exceeds 10% of free system memory.\n2022-07-16 04:02:01.546006: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1/301\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-07-16 04:02:04.997128: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "202/202 [==============================] - 34s 145ms/step - loss: 2.8897\n\nMy War and Peace:\nà the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule the soule\n\nEpoch 00001: loss improved from inf to 2.88968, saving model to weights/weights_epoch_001_loss_2.8897.hdf5\nEpoch 2/301\n202/202 [==============================] - 29s 146ms/step - loss: 2.4346\n\nEpoch 00002: loss improved from 2.88968 to 2.43457, saving model to weights/weights_epoch_002_loss_2.4346.hdf5\nEpoch 3/301\n202/202 [==============================] - 29s 145ms/step - loss: 2.2739\n\nEpoch 00003: loss improved from 2.43457 to 2.27391, saving model to weights/weights_epoch_003_loss_2.2739.hdf5\nEpoch 4/301\n202/202 [==============================] - 29s 145ms/step - loss: 2.1810\n\nEpoch 00004: loss improved from 2.27391 to 2.18102, saving model to weights/weights_epoch_004_loss_2.1810.hdf5\nEpoch 5/301\n202/202 [==============================] - 29s 145ms/step - loss: 2.1155\n\nEpoch 00005: loss improved from 2.18102 to 2.11552, saving model to weights/weights_epoch_005_loss_2.1155.hdf5\nEpoch 6/301\n202/202 [==============================] - 29s 145ms/step - loss: 2.0686\n\nEpoch 00006: loss improved from 2.11552 to 2.06855, saving model to weights/weights_epoch_006_loss_2.0686.hdf5\nEpoch 7/301\n202/202 [==============================] - 29s 145ms/step - loss: 2.0327\n\nEpoch 00007: loss improved from 2.06855 to 2.03275, saving model to weights/weights_epoch_007_loss_2.0327.hdf5\nEpoch 8/301\n202/202 [==============================] - 29s 145ms/step - loss: 2.0036\n\nEpoch 00008: loss improved from 2.03275 to 2.00358, saving model to weights/weights_epoch_008_loss_2.0036.hdf5\nEpoch 9/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.9817\n\nEpoch 00009: loss improved from 2.00358 to 1.98167, saving model to weights/weights_epoch_009_loss_1.9817.hdf5\nEpoch 10/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.9630\n\nEpoch 00010: loss improved from 1.98167 to 1.96297, saving model to weights/weights_epoch_010_loss_1.9630.hdf5\nEpoch 11/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.9461\n\nMy War and Peace:\nd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the count sadd the \n\nEpoch 00011: loss improved from 1.96297 to 1.94605, saving model to weights/weights_epoch_011_loss_1.9461.hdf5\nEpoch 12/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.9317\n\nEpoch 00012: loss improved from 1.94605 to 1.93166, saving model to weights/weights_epoch_012_loss_1.9317.hdf5\nEpoch 13/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.9195\n\nEpoch 00013: loss improved from 1.93166 to 1.91951, saving model to weights/weights_epoch_013_loss_1.9195.hdf5\nEpoch 14/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.9074\n\nEpoch 00014: loss improved from 1.91951 to 1.90744, saving model to weights/weights_epoch_014_loss_1.9074.hdf5\nEpoch 15/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.8980\n\nEpoch 00015: loss improved from 1.90744 to 1.89798, saving model to weights/weights_epoch_015_loss_1.8980.hdf5\nEpoch 16/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8890\n\nEpoch 00016: loss improved from 1.89798 to 1.88903, saving model to weights/weights_epoch_016_loss_1.8890.hdf5\nEpoch 17/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8825\n\nEpoch 00017: loss improved from 1.88903 to 1.88251, saving model to weights/weights_epoch_017_loss_1.8825.hdf5\nEpoch 18/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8735\n\nEpoch 00018: loss improved from 1.88251 to 1.87347, saving model to weights/weights_epoch_018_loss_1.8735.hdf5\nEpoch 19/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8655\n\nEpoch 00019: loss improved from 1.87347 to 1.86551, saving model to weights/weights_epoch_019_loss_1.8655.hdf5\nEpoch 20/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8592\n\nEpoch 00020: loss improved from 1.86551 to 1.85917, saving model to weights/weights_epoch_020_loss_1.8592.hdf5\nEpoch 21/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8512\n\nMy War and Peace:\n1805\n\n\n\n\n\nchapter xii\n\nthe most important position of the forest to the rostóvs’ army was a soldier who was so that the french could not be a battle and the same time to the rostóvs’ and the soldiers were always seemed to him that the emperor had been an answer to the room and the soldiers were straight to the room.\n\n“the emperor said the countess was so saying to him and the same time to the\nsoldiers who was so so fall and so that the emperor had been an answer. the soldiers were to the room and\n\nEpoch 00021: loss improved from 1.85917 to 1.85120, saving model to weights/weights_epoch_021_loss_1.8512.hdf5\nEpoch 22/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8474\n\nEpoch 00022: loss improved from 1.85120 to 1.84743, saving model to weights/weights_epoch_022_loss_1.8474.hdf5\nEpoch 23/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.8406\n\nEpoch 00023: loss improved from 1.84743 to 1.84056, saving model to weights/weights_epoch_023_loss_1.8406.hdf5\nEpoch 24/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8353\n\nEpoch 00024: loss improved from 1.84056 to 1.83527, saving model to weights/weights_epoch_024_loss_1.8353.hdf5\nEpoch 25/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8295\n\nEpoch 00025: loss improved from 1.83527 to 1.82952, saving model to weights/weights_epoch_025_loss_1.8295.hdf5\nEpoch 26/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.8261\n\nEpoch 00026: loss improved from 1.82952 to 1.82613, saving model to weights/weights_epoch_026_loss_1.8261.hdf5\nEpoch 27/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.8198\n\nEpoch 00027: loss improved from 1.82613 to 1.81984, saving model to weights/weights_epoch_027_loss_1.8198.hdf5\nEpoch 28/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.8164\n\nEpoch 00028: loss improved from 1.81984 to 1.81637, saving model to weights/weights_epoch_028_loss_1.8164.hdf5\nEpoch 29/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.8133\n\nEpoch 00029: loss improved from 1.81637 to 1.81333, saving model to weights/weights_epoch_029_loss_1.8133.hdf5\nEpoch 30/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.8097\n\nEpoch 00030: loss improved from 1.81333 to 1.80967, saving model to weights/weights_epoch_030_loss_1.8097.hdf5\nEpoch 31/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.8057\n\nMy War and Peace:\n68öö88881818\n\n\n\n\n\nchapter i\n\nthe enemy’s face was a song time to say that the emperor had been so song\nto him to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to himself to him\n\nEpoch 00031: loss improved from 1.80967 to 1.80565, saving model to weights/weights_epoch_031_loss_1.8057.hdf5\nEpoch 32/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.8030\n\nEpoch 00032: loss improved from 1.80565 to 1.80303, saving model to weights/weights_epoch_032_loss_1.8030.hdf5\nEpoch 33/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7987\n\nEpoch 00033: loss improved from 1.80303 to 1.79868, saving model to weights/weights_epoch_033_loss_1.7987.hdf5\nEpoch 34/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7955\n\nEpoch 00034: loss improved from 1.79868 to 1.79552, saving model to weights/weights_epoch_034_loss_1.7955.hdf5\nEpoch 35/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7941\n\nEpoch 00035: loss improved from 1.79552 to 1.79412, saving model to weights/weights_epoch_035_loss_1.7941.hdf5\nEpoch 36/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7896\n\nEpoch 00036: loss improved from 1.79412 to 1.78961, saving model to weights/weights_epoch_036_loss_1.7896.hdf5\nEpoch 37/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7861\n\nEpoch 00037: loss improved from 1.78961 to 1.78611, saving model to weights/weights_epoch_037_loss_1.7861.hdf5\nEpoch 38/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7857\n\nEpoch 00038: loss improved from 1.78611 to 1.78571, saving model to weights/weights_epoch_038_loss_1.7857.hdf5\nEpoch 39/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7810\n\nEpoch 00039: loss improved from 1.78571 to 1.78100, saving model to weights/weights_epoch_039_loss_1.7810.hdf5\nEpoch 40/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7784\n\nEpoch 00040: loss improved from 1.78100 to 1.77840, saving model to weights/weights_epoch_040_loss_1.7784.hdf5\nEpoch 41/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7755\n\nMy War and Peace:\n\n\n“what a strange man was the same tome of the countess?” said the\ncountess, “i am so dear from the french army.”\n\n“what do you think? i won’t know what i say that i want to say that i want to see the countess, i don’t know what i want to say that i want to see the countess. i am so song to her and that i want to see them. i want to see you to see them and then a shall not be a money for you. i will see them and there is no longer to be a man with you, my dear fellow, i don’t know what i want to \n\nEpoch 00041: loss improved from 1.77840 to 1.77553, saving model to weights/weights_epoch_041_loss_1.7755.hdf5\nEpoch 42/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7743\n\nEpoch 00042: loss improved from 1.77553 to 1.77434, saving model to weights/weights_epoch_042_loss_1.7743.hdf5\nEpoch 43/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7726\n\nEpoch 00043: loss improved from 1.77434 to 1.77256, saving model to weights/weights_epoch_043_loss_1.7726.hdf5\nEpoch 44/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7696\n\nEpoch 00044: loss improved from 1.77256 to 1.76957, saving model to weights/weights_epoch_044_loss_1.7696.hdf5\nEpoch 45/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7660\n\nEpoch 00045: loss improved from 1.76957 to 1.76596, saving model to weights/weights_epoch_045_loss_1.7660.hdf5\nEpoch 46/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7658\n\nEpoch 00046: loss improved from 1.76596 to 1.76576, saving model to weights/weights_epoch_046_loss_1.7658.hdf5\nEpoch 47/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7627\n\nEpoch 00047: loss improved from 1.76576 to 1.76270, saving model to weights/weights_epoch_047_loss_1.7627.hdf5\nEpoch 48/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7592\n\nEpoch 00048: loss improved from 1.76270 to 1.75921, saving model to weights/weights_epoch_048_loss_1.7592.hdf5\nEpoch 49/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7579\n\nEpoch 00049: loss improved from 1.75921 to 1.75787, saving model to weights/weights_epoch_049_loss_1.7579.hdf5\nEpoch 50/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7565\n\nEpoch 00050: loss improved from 1.75787 to 1.75645, saving model to weights/weights_epoch_050_loss_1.7565.hdf5\nEpoch 51/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7547\n\nMy War and Peace:\nked and the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the sounds of the\n\nEpoch 00051: loss improved from 1.75645 to 1.75474, saving model to weights/weights_epoch_051_loss_1.7547.hdf5\nEpoch 52/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7539\n\nEpoch 00052: loss improved from 1.75474 to 1.75387, saving model to weights/weights_epoch_052_loss_1.7539.hdf5\nEpoch 53/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7521\n\nEpoch 00053: loss improved from 1.75387 to 1.75215, saving model to weights/weights_epoch_053_loss_1.7521.hdf5\nEpoch 54/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7495\n\nEpoch 00054: loss improved from 1.75215 to 1.74947, saving model to weights/weights_epoch_054_loss_1.7495.hdf5\nEpoch 55/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7470\n\nEpoch 00055: loss improved from 1.74947 to 1.74698, saving model to weights/weights_epoch_055_loss_1.7470.hdf5\nEpoch 56/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7468\n\nEpoch 00056: loss improved from 1.74698 to 1.74675, saving model to weights/weights_epoch_056_loss_1.7468.hdf5\nEpoch 57/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7436\n\nEpoch 00057: loss improved from 1.74675 to 1.74360, saving model to weights/weights_epoch_057_loss_1.7436.hdf5\nEpoch 58/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7427\n\nEpoch 00058: loss improved from 1.74360 to 1.74266, saving model to weights/weights_epoch_058_loss_1.7427.hdf5\nEpoch 59/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7410\n\nEpoch 00059: loss improved from 1.74266 to 1.74102, saving model to weights/weights_epoch_059_loss_1.7410.hdf5\nEpoch 60/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7409\n\nEpoch 00060: loss improved from 1.74102 to 1.74094, saving model to weights/weights_epoch_060_loss_1.7409.hdf5\nEpoch 61/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7380\n\nMy War and Peace:\n) the same thing that had been so sarry to see the princess and the old prince to the princess. “i shall never see you to see you, but i was not a sacrifice to her. i was a song time to see her. i was a song time to see her. i was a song time to see her. i was a song time to see her. i was a song time to see her. i was a song time to see her. i was a song time to see her. i was a song time to see her. i was a song time to see her. i was a song time to see her. i was a song time to see her. i was \n\nEpoch 00061: loss improved from 1.74094 to 1.73802, saving model to weights/weights_epoch_061_loss_1.7380.hdf5\nEpoch 62/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7371\n\nEpoch 00062: loss improved from 1.73802 to 1.73712, saving model to weights/weights_epoch_062_loss_1.7371.hdf5\nEpoch 63/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7353\n\nEpoch 00063: loss improved from 1.73712 to 1.73530, saving model to weights/weights_epoch_063_loss_1.7353.hdf5\nEpoch 64/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7347\n\nEpoch 00064: loss improved from 1.73530 to 1.73470, saving model to weights/weights_epoch_064_loss_1.7347.hdf5\nEpoch 65/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7324\n\nEpoch 00065: loss improved from 1.73470 to 1.73239, saving model to weights/weights_epoch_065_loss_1.7324.hdf5\nEpoch 66/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7309\n\nEpoch 00066: loss improved from 1.73239 to 1.73093, saving model to weights/weights_epoch_066_loss_1.7309.hdf5\nEpoch 67/301\n202/202 [==============================] - 30s 148ms/step - loss: 1.7317\n\nEpoch 00067: loss did not improve from 1.73093\nEpoch 68/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7289\n\nEpoch 00068: loss improved from 1.73093 to 1.72888, saving model to weights/weights_epoch_068_loss_1.7289.hdf5\nEpoch 69/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7272\n\nEpoch 00069: loss improved from 1.72888 to 1.72724, saving model to weights/weights_epoch_069_loss_1.7272.hdf5\nEpoch 70/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7258\n\nEpoch 00070: loss improved from 1.72724 to 1.72583, saving model to weights/weights_epoch_070_loss_1.7258.hdf5\nEpoch 71/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7249\n\nMy War and Peace:\nëv and the officer was sitting in the same way.\n\n“what a strange commander in chief was saying a word of the russian army,\nand the emperor was so sure that the emperor was so sure that he\nwas so sure that he was a serious expression of his father’s army\nand his son who was so sure that he was so sorry for him to say that\nthe emperor was so sure that he was a serious expression of his\nface and he saw that he was a soldier who had been an answer to\nhim and he was so sure that he was a soldier who h\n\nEpoch 00071: loss improved from 1.72583 to 1.72486, saving model to weights/weights_epoch_071_loss_1.7249.hdf5\nEpoch 72/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7242\n\nEpoch 00072: loss improved from 1.72486 to 1.72425, saving model to weights/weights_epoch_072_loss_1.7242.hdf5\nEpoch 73/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7238\n\nEpoch 00073: loss improved from 1.72425 to 1.72378, saving model to weights/weights_epoch_073_loss_1.7238.hdf5\nEpoch 74/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7217\n\nEpoch 00074: loss improved from 1.72378 to 1.72172, saving model to weights/weights_epoch_074_loss_1.7217.hdf5\nEpoch 75/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7201\n\nEpoch 00075: loss improved from 1.72172 to 1.72009, saving model to weights/weights_epoch_075_loss_1.7201.hdf5\nEpoch 76/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7201\n\nEpoch 00076: loss improved from 1.72009 to 1.72008, saving model to weights/weights_epoch_076_loss_1.7201.hdf5\nEpoch 77/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7183\n\nEpoch 00077: loss improved from 1.72008 to 1.71832, saving model to weights/weights_epoch_077_loss_1.7183.hdf5\nEpoch 78/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7180\n\nEpoch 00078: loss improved from 1.71832 to 1.71799, saving model to weights/weights_epoch_078_loss_1.7180.hdf5\nEpoch 79/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7146\n\nEpoch 00079: loss improved from 1.71799 to 1.71458, saving model to weights/weights_epoch_079_loss_1.7146.hdf5\nEpoch 80/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7164\n\nEpoch 00080: loss did not improve from 1.71458\nEpoch 81/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7137\n\nMy War and Peace:\nœ the same time the same thing was so stronger than the french army was a stranger and the same thing that had been sent to the russian army and the french army was a soldier who was standing by the french army was to be able to see the project gutenberg-tm electronic works and the position of the people is the consequence of the people is the consequence of the people is the consequence of the people is the consequence of the people is the consequence of the people is the consequence of the peop\n\nEpoch 00081: loss improved from 1.71458 to 1.71371, saving model to weights/weights_epoch_081_loss_1.7137.hdf5\nEpoch 82/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7129\n\nEpoch 00082: loss improved from 1.71371 to 1.71294, saving model to weights/weights_epoch_082_loss_1.7129.hdf5\nEpoch 83/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7091\n\nEpoch 00083: loss improved from 1.71294 to 1.70915, saving model to weights/weights_epoch_083_loss_1.7091.hdf5\nEpoch 84/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7123\n\nEpoch 00084: loss did not improve from 1.70915\nEpoch 85/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7099\n\nEpoch 00085: loss did not improve from 1.70915\nEpoch 86/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7087\n\nEpoch 00086: loss improved from 1.70915 to 1.70871, saving model to weights/weights_epoch_086_loss_1.7087.hdf5\nEpoch 87/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.7068\n\nEpoch 00087: loss improved from 1.70871 to 1.70683, saving model to weights/weights_epoch_087_loss_1.7068.hdf5\nEpoch 88/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7079\n\nEpoch 00088: loss did not improve from 1.70683\nEpoch 89/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7084\n\nEpoch 00089: loss did not improve from 1.70683\nEpoch 90/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7056\n\nEpoch 00090: loss improved from 1.70683 to 1.70558, saving model to weights/weights_epoch_090_loss_1.7056.hdf5\nEpoch 91/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7050\n\nMy War and Peace:\nbe an angry and a soldier who had been sent to the room.\n\n“what a strange toing! i won’t let him go and say that i was a soldier\nwho has been an interval in the corner of the room. the countess\nwas sitting at the door and went to the door and went to the\ndoor.\n\n“i have not yet seen him and want to see you, your excellency,”\nsaid the countess, “i am very glad to see you to go and well you\nthat i was a song time.”\n\n“i have not yet seen him and want to see you, your excellency,”\nsaid the countess, “\n\nEpoch 00091: loss improved from 1.70558 to 1.70500, saving model to weights/weights_epoch_091_loss_1.7050.hdf5\nEpoch 92/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.7058\n\nEpoch 00092: loss did not improve from 1.70500\nEpoch 93/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7022\n\nEpoch 00093: loss improved from 1.70500 to 1.70220, saving model to weights/weights_epoch_093_loss_1.7022.hdf5\nEpoch 94/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7012\n\nEpoch 00094: loss improved from 1.70220 to 1.70117, saving model to weights/weights_epoch_094_loss_1.7012.hdf5\nEpoch 95/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7010\n\nEpoch 00095: loss improved from 1.70117 to 1.70101, saving model to weights/weights_epoch_095_loss_1.7010.hdf5\nEpoch 96/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7001\n\nEpoch 00096: loss improved from 1.70101 to 1.70011, saving model to weights/weights_epoch_096_loss_1.7001.hdf5\nEpoch 97/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.7003\n\nEpoch 00097: loss did not improve from 1.70011\nEpoch 98/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6992\n\nEpoch 00098: loss improved from 1.70011 to 1.69917, saving model to weights/weights_epoch_098_loss_1.6992.hdf5\nEpoch 99/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6971\n\nEpoch 00099: loss improved from 1.69917 to 1.69712, saving model to weights/weights_epoch_099_loss_1.6971.hdf5\nEpoch 100/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6962\n\nEpoch 00100: loss improved from 1.69712 to 1.69622, saving model to weights/weights_epoch_100_loss_1.6962.hdf5\nEpoch 101/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6964\n\nMy War and Peace:\nœ the countess was a stranger and an angry sound of sorrow. “i have not yet seen her and was a good time to see her. i will tell you that i was not a single and and i want to know what i have not seen in the country.”\n\n“i have not seen the countess, i will talk about it,” said prince\nandrew, “i will tell you that i was not a single and and i want\nto see you to meet her. i will tell you that i was not a single\nand and i want to know what i say to her.”\n\n“i have not seen the countess, i will talk a\n\nEpoch 00101: loss did not improve from 1.69622\nEpoch 102/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6955\n\nEpoch 00102: loss improved from 1.69622 to 1.69547, saving model to weights/weights_epoch_102_loss_1.6955.hdf5\nEpoch 103/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6945\n\nEpoch 00103: loss improved from 1.69547 to 1.69447, saving model to weights/weights_epoch_103_loss_1.6945.hdf5\nEpoch 104/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6922\n\nEpoch 00104: loss improved from 1.69447 to 1.69219, saving model to weights/weights_epoch_104_loss_1.6922.hdf5\nEpoch 105/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6924\n\nEpoch 00105: loss did not improve from 1.69219\nEpoch 106/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6924\n\nEpoch 00106: loss did not improve from 1.69219\nEpoch 107/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6914\n\nEpoch 00107: loss improved from 1.69219 to 1.69137, saving model to weights/weights_epoch_107_loss_1.6914.hdf5\nEpoch 108/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6910\n\nEpoch 00108: loss improved from 1.69137 to 1.69098, saving model to weights/weights_epoch_108_loss_1.6910.hdf5\nEpoch 109/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6906\n\nEpoch 00109: loss improved from 1.69098 to 1.69065, saving model to weights/weights_epoch_109_loss_1.6906.hdf5\nEpoch 110/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6912\n\nEpoch 00110: loss did not improve from 1.69065\nEpoch 111/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6878\n\nMy War and Peace:\n% the same thing that had been an austerlitz and the same time the soldiers were all sinting at the soldiers and the soldiers were all sinting at the soldiers and the soldiers were all sinting at the soldiers and the soldiers were all sinting at the soldiers and the soldiers were all sinting at the soldiers and the soldiers were all sinting at the soldiers and the soldiers were all sinting at the soldiers and the soldiers were all sinting at the soldiers and the soldiers were all sinting at the s\n\nEpoch 00111: loss improved from 1.69065 to 1.68777, saving model to weights/weights_epoch_111_loss_1.6878.hdf5\nEpoch 112/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6887\n\nEpoch 00112: loss did not improve from 1.68777\nEpoch 113/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6891\n\nEpoch 00113: loss did not improve from 1.68777\nEpoch 114/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6862\n\nEpoch 00114: loss improved from 1.68777 to 1.68623, saving model to weights/weights_epoch_114_loss_1.6862.hdf5\nEpoch 115/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6882\n\nEpoch 00115: loss did not improve from 1.68623\nEpoch 116/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6853\n\nEpoch 00116: loss improved from 1.68623 to 1.68533, saving model to weights/weights_epoch_116_loss_1.6853.hdf5\nEpoch 117/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6859\n\nEpoch 00117: loss did not improve from 1.68533\nEpoch 118/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6839\n\nEpoch 00118: loss improved from 1.68533 to 1.68391, saving model to weights/weights_epoch_118_loss_1.6839.hdf5\nEpoch 119/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6840\n\nEpoch 00119: loss did not improve from 1.68391\nEpoch 120/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6835\n\nEpoch 00120: loss improved from 1.68391 to 1.68345, saving model to weights/weights_epoch_120_loss_1.6835.hdf5\nEpoch 121/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6815\n\nMy War and Peace:\n4) the countess was sitting in the same way to the countess.\n\n“i have not seen him and i want to see you to her that i was so serious\nand i have not seen him and i won’t leave him and say that i was\nso serious. i am not a good thing to see him. i will see him and i\nwant to see him. i will see him and well me that i was so serious. i\nwant to see him and to do it all the same.”\n\n“yes, yes, i know ” said prince andrew, “i know i have not yet seen him\nand i will do it. i am not a good thing to see yo\n\nEpoch 00121: loss improved from 1.68345 to 1.68145, saving model to weights/weights_epoch_121_loss_1.6815.hdf5\nEpoch 122/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6810\n\nEpoch 00122: loss improved from 1.68145 to 1.68097, saving model to weights/weights_epoch_122_loss_1.6810.hdf5\nEpoch 123/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6810\n\nEpoch 00123: loss did not improve from 1.68097\nEpoch 124/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6829\n\nEpoch 00124: loss did not improve from 1.68097\nEpoch 125/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6807\n\nEpoch 00125: loss improved from 1.68097 to 1.68075, saving model to weights/weights_epoch_125_loss_1.6807.hdf5\nEpoch 126/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6811\n\nEpoch 00126: loss did not improve from 1.68075\nEpoch 127/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6799\n\nEpoch 00127: loss improved from 1.68075 to 1.67993, saving model to weights/weights_epoch_127_loss_1.6799.hdf5\nEpoch 128/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6774\n\nEpoch 00128: loss improved from 1.67993 to 1.67735, saving model to weights/weights_epoch_128_loss_1.6774.hdf5\nEpoch 129/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6794\n\nEpoch 00129: loss did not improve from 1.67735\nEpoch 130/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6754\n\nEpoch 00130: loss improved from 1.67735 to 1.67539, saving model to weights/weights_epoch_130_loss_1.6754.hdf5\nEpoch 131/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6756\n\nMy War and Peace:\ne to the countess, and the countess was so serious that the countess was a soldier who had been an answer to her mother, and the countess was already an answer to the drawing room. the countess was sitting at the door and the countess went up to her and said to herself and said to herself. “i will see you to her and i will see her and i will see her and i will see her and i will see her and say that i was asking to be an answer.”\n\n“i don’t know what i say to me,” said princess mary. “i am not\na g\n\nEpoch 00131: loss did not improve from 1.67539\nEpoch 132/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6756\n\nEpoch 00132: loss did not improve from 1.67539\nEpoch 133/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6754\n\nEpoch 00133: loss improved from 1.67539 to 1.67537, saving model to weights/weights_epoch_133_loss_1.6754.hdf5\nEpoch 134/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6745\n\nEpoch 00134: loss improved from 1.67537 to 1.67446, saving model to weights/weights_epoch_134_loss_1.6745.hdf5\nEpoch 135/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6720\n\nEpoch 00135: loss improved from 1.67446 to 1.67203, saving model to weights/weights_epoch_135_loss_1.6720.hdf5\nEpoch 136/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6744\n\nEpoch 00136: loss did not improve from 1.67203\nEpoch 137/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6734\n\nEpoch 00137: loss did not improve from 1.67203\nEpoch 138/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6741\n\nEpoch 00138: loss did not improve from 1.67203\nEpoch 139/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6715\n\nEpoch 00139: loss improved from 1.67203 to 1.67150, saving model to weights/weights_epoch_139_loss_1.6715.hdf5\nEpoch 140/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6710\n\nEpoch 00140: loss improved from 1.67150 to 1.67099, saving model to weights/weights_epoch_140_loss_1.6710.hdf5\nEpoch 141/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6694\n\nMy War and Peace:\n) “and the emperor alexander did not know what to do and was about to say to himself. “i am sorry for him to go and see him and i will do it. i will see him and say that i was a soldier who has been a company of the commander in chief. i am a soldier who has been a commander in chief in the same way. and the emperor alexander is a very strong and animated and angry in his face.\n\n“i am sorry for you to see you to me a bit of the count’s and\ni will go and see him.”\n\n“yes, i say, i will do it all th\n\nEpoch 00141: loss improved from 1.67099 to 1.66944, saving model to weights/weights_epoch_141_loss_1.6694.hdf5\nEpoch 142/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.6707\n\nEpoch 00142: loss did not improve from 1.66944\nEpoch 143/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6702\n\nEpoch 00143: loss did not improve from 1.66944\nEpoch 144/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6685\n\nEpoch 00144: loss improved from 1.66944 to 1.66847, saving model to weights/weights_epoch_144_loss_1.6685.hdf5\nEpoch 145/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6688\n\nEpoch 00145: loss did not improve from 1.66847\nEpoch 146/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.6697\n\nEpoch 00146: loss did not improve from 1.66847\nEpoch 147/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6693\n\nEpoch 00147: loss did not improve from 1.66847\nEpoch 148/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6678\n\nEpoch 00148: loss improved from 1.66847 to 1.66779, saving model to weights/weights_epoch_148_loss_1.6678.hdf5\nEpoch 149/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6687\n\nEpoch 00149: loss did not improve from 1.66779\nEpoch 150/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6674\n\nEpoch 00150: loss improved from 1.66779 to 1.66735, saving model to weights/weights_epoch_150_loss_1.6674.hdf5\nEpoch 151/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6653\n\nMy War and Peace:\nâte and the same thing that had been sent to his mother and the countess was already an anger and the countess was so strange to her and that he was already an angel and that he was always so strange to him and that he was not a single and indefinite and in a state of separate and indifference. he was a so that he was not a single and indefinite and indefinite and an intelligent man. he was a soldier who had been able to realize that he was saying about his son’s face and the sound of the soldier\n\nEpoch 00151: loss improved from 1.66735 to 1.66530, saving model to weights/weights_epoch_151_loss_1.6653.hdf5\nEpoch 152/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6664\n\nEpoch 00152: loss did not improve from 1.66530\nEpoch 153/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6653\n\nEpoch 00153: loss did not improve from 1.66530\nEpoch 154/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6650\n\nEpoch 00154: loss improved from 1.66530 to 1.66497, saving model to weights/weights_epoch_154_loss_1.6650.hdf5\nEpoch 155/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6667\n\nEpoch 00155: loss did not improve from 1.66497\nEpoch 156/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6648\n\nEpoch 00156: loss improved from 1.66497 to 1.66477, saving model to weights/weights_epoch_156_loss_1.6648.hdf5\nEpoch 157/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6621\n\nEpoch 00157: loss improved from 1.66477 to 1.66210, saving model to weights/weights_epoch_157_loss_1.6621.hdf5\nEpoch 158/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6629\n\nEpoch 00158: loss did not improve from 1.66210\nEpoch 159/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6629\n\nEpoch 00159: loss did not improve from 1.66210\nEpoch 160/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6631\n\nEpoch 00160: loss did not improve from 1.66210\nEpoch 161/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6614\n\nMy War and Peace:\npersonal attention to the french army was the first to see the french army were to be able to restrain himself and the result of the french army was the first to set the rest of the french army were to be able to restrain himself and the emperor alexander did not answer and to the emperor and the emperor and the officer of the service. “what a strange and indefinite men,” he added, addressing the servants who was sitting in a side of the room.\n\n“what a salended man, you’ve come to the countess,” \n\nEpoch 00161: loss improved from 1.66210 to 1.66142, saving model to weights/weights_epoch_161_loss_1.6614.hdf5\nEpoch 162/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6600\n\nEpoch 00162: loss improved from 1.66142 to 1.66004, saving model to weights/weights_epoch_162_loss_1.6600.hdf5\nEpoch 163/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6602\n\nEpoch 00163: loss did not improve from 1.66004\nEpoch 164/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6600\n\nEpoch 00164: loss improved from 1.66004 to 1.66002, saving model to weights/weights_epoch_164_loss_1.6600.hdf5\nEpoch 165/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6582\n\nEpoch 00165: loss improved from 1.66002 to 1.65823, saving model to weights/weights_epoch_165_loss_1.6582.hdf5\nEpoch 166/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6578\n\nEpoch 00166: loss improved from 1.65823 to 1.65777, saving model to weights/weights_epoch_166_loss_1.6578.hdf5\nEpoch 167/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6596\n\nEpoch 00167: loss did not improve from 1.65777\nEpoch 168/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6575\n\nEpoch 00168: loss improved from 1.65777 to 1.65747, saving model to weights/weights_epoch_168_loss_1.6575.hdf5\nEpoch 169/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6584\n\nEpoch 00169: loss did not improve from 1.65747\nEpoch 170/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6586\n\nEpoch 00170: loss did not improve from 1.65747\nEpoch 171/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6575\n\nMy War and Peace:\ns and the same thing that had been sent for the first time. the countess was already bearing the room and went to the door.\n\n“no, i will tell you the countess,” said natásha. “i know you\nwon’t believe it.”\n\n“i know you won’t believe it, but i want to know what i say to say.”\n\n“yes, yes,” said prince andrew, “i know you won’t know what i have\ndone to.”\n\n“i know that i say that i was a great man, but i will tell you the whole\narmy. i will tell you that i was a great man, but i will tell you\nthe who\n\nEpoch 00171: loss did not improve from 1.65747\nEpoch 172/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6567\n\nEpoch 00172: loss improved from 1.65747 to 1.65674, saving model to weights/weights_epoch_172_loss_1.6567.hdf5\nEpoch 173/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6580\n\nEpoch 00173: loss did not improve from 1.65674\nEpoch 174/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6581\n\nEpoch 00174: loss did not improve from 1.65674\nEpoch 175/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6553\n\nEpoch 00175: loss improved from 1.65674 to 1.65533, saving model to weights/weights_epoch_175_loss_1.6553.hdf5\nEpoch 176/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6552\n\nEpoch 00176: loss improved from 1.65533 to 1.65525, saving model to weights/weights_epoch_176_loss_1.6552.hdf5\nEpoch 177/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6547\n\nEpoch 00177: loss improved from 1.65525 to 1.65475, saving model to weights/weights_epoch_177_loss_1.6547.hdf5\nEpoch 178/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6545\n\nEpoch 00178: loss improved from 1.65475 to 1.65448, saving model to weights/weights_epoch_178_loss_1.6545.hdf5\nEpoch 179/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6549\n\nEpoch 00179: loss did not improve from 1.65448\nEpoch 180/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6531\n\nEpoch 00180: loss improved from 1.65448 to 1.65311, saving model to weights/weights_epoch_180_loss_1.6531.hdf5\nEpoch 181/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6542\n\nMy War and Peace:\n9 the emperor said to himself: “i am as a sarerable commander in chief is a very strongly consciousness of his will, and the emperor alexander did not answer the emperor and the officers and the officers and the soldiers.\n\n“what are you saying?” said the officer, turning to prince andrew.\n\n“what are you saying?” said the officer with a smile.\n\n“i am sorry for you, many heart is a soldier who has been an answer to\nthe emperor alexander,” said prince andrew, “i am as you are\nan angel and i have not\n\nEpoch 00181: loss did not improve from 1.65311\nEpoch 182/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6513\n\nEpoch 00182: loss improved from 1.65311 to 1.65129, saving model to weights/weights_epoch_182_loss_1.6513.hdf5\nEpoch 183/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6531\n\nEpoch 00183: loss did not improve from 1.65129\nEpoch 184/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6512\n\nEpoch 00184: loss improved from 1.65129 to 1.65121, saving model to weights/weights_epoch_184_loss_1.6512.hdf5\nEpoch 185/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6526\n\nEpoch 00185: loss did not improve from 1.65121\nEpoch 186/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6515\n\nEpoch 00186: loss did not improve from 1.65121\nEpoch 187/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6515\n\nEpoch 00187: loss did not improve from 1.65121\nEpoch 188/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6505\n\nEpoch 00188: loss improved from 1.65121 to 1.65051, saving model to weights/weights_epoch_188_loss_1.6505.hdf5\nEpoch 189/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6496\n\nEpoch 00189: loss improved from 1.65051 to 1.64964, saving model to weights/weights_epoch_189_loss_1.6496.hdf5\nEpoch 190/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6504\n\nEpoch 00190: loss did not improve from 1.64964\nEpoch 191/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6496\n\nMy War and Peace:\nhe same time and the same thing that had been sent to him. he was always the same to him and that he was all the same to him and that he was all the same to him and that he was all the same to him and that he was all the same to him and that he was all the same to him and that he was all the same to him and that he was all the same to him and that he was all the same to him and that he was all the same to him and that he was all the same to him and that he was all the same to him and that he was \n\nEpoch 00191: loss improved from 1.64964 to 1.64960, saving model to weights/weights_epoch_191_loss_1.6496.hdf5\nEpoch 192/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6496\n\nEpoch 00192: loss did not improve from 1.64960\nEpoch 193/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6487\n\nEpoch 00193: loss improved from 1.64960 to 1.64872, saving model to weights/weights_epoch_193_loss_1.6487.hdf5\nEpoch 194/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6480\n\nEpoch 00194: loss improved from 1.64872 to 1.64802, saving model to weights/weights_epoch_194_loss_1.6480.hdf5\nEpoch 195/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6473\n\nEpoch 00195: loss improved from 1.64802 to 1.64725, saving model to weights/weights_epoch_195_loss_1.6473.hdf5\nEpoch 196/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6486\n\nEpoch 00196: loss did not improve from 1.64725\nEpoch 197/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6468\n\nEpoch 00197: loss improved from 1.64725 to 1.64677, saving model to weights/weights_epoch_197_loss_1.6468.hdf5\nEpoch 198/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6457\n\nEpoch 00198: loss improved from 1.64677 to 1.64568, saving model to weights/weights_epoch_198_loss_1.6457.hdf5\nEpoch 199/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6453\n\nEpoch 00199: loss improved from 1.64568 to 1.64526, saving model to weights/weights_epoch_199_loss_1.6453.hdf5\nEpoch 200/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6450\n\nEpoch 00200: loss improved from 1.64526 to 1.64499, saving model to weights/weights_epoch_200_loss_1.6450.hdf5\nEpoch 201/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6461\n\nMy War and Peace:\ny the same time and the same thing that had been so serious to him and the expression of his face caused him.\n\n“well, and what do you think? what do you think? what do you think,\ndon’t you see?” said the countess, “i know you won’t get anything\nto do.”\n\n“yes, yes,” said pierre, “i know you won’t see you to see you.”\n\n“yes, yes, yes,” said prince andrew, “i am afraid of him.”\n\n“i don’t know what i say, but i am not a soldier in the same time.”\n\n“yes, yes, i am afraid of him,” said prince andrew, “\n\nEpoch 00201: loss did not improve from 1.64499\nEpoch 202/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6454\n\nEpoch 00202: loss did not improve from 1.64499\nEpoch 203/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6459\n\nEpoch 00203: loss did not improve from 1.64499\nEpoch 204/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6444\n\nEpoch 00204: loss improved from 1.64499 to 1.64444, saving model to weights/weights_epoch_204_loss_1.6444.hdf5\nEpoch 205/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6445\n\nEpoch 00205: loss did not improve from 1.64444\nEpoch 206/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6445\n\nEpoch 00206: loss did not improve from 1.64444\nEpoch 207/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6451\n\nEpoch 00207: loss did not improve from 1.64444\nEpoch 208/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6446\n\nEpoch 00208: loss did not improve from 1.64444\nEpoch 209/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6424\n\nEpoch 00209: loss improved from 1.64444 to 1.64237, saving model to weights/weights_epoch_209_loss_1.6424.hdf5\nEpoch 210/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6449\n\nEpoch 00210: loss did not improve from 1.64237\nEpoch 211/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6423\n\nMy War and Peace:\nn the same way that the emperor said that he was a very strong of importance of the emperor alexander’s will, and the emperor alexander did not know what to do with him, and that the emperor was the emperor alexander to the emperor alexander  and the emperor said that he was a very strong of the emperor alexander’s will, and that the emperor had already been the emperor alexander himself with the emperor and the emperor alexander  and the emperor alexander did not answer the colonel of the comman\n\nEpoch 00211: loss improved from 1.64237 to 1.64229, saving model to weights/weights_epoch_211_loss_1.6423.hdf5\nEpoch 212/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6404\n\nEpoch 00212: loss improved from 1.64229 to 1.64037, saving model to weights/weights_epoch_212_loss_1.6404.hdf5\nEpoch 213/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6428\n\nEpoch 00213: loss did not improve from 1.64037\nEpoch 214/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6432\n\nEpoch 00214: loss did not improve from 1.64037\nEpoch 215/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6413\n\nEpoch 00215: loss did not improve from 1.64037\nEpoch 216/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6403\n\nEpoch 00216: loss improved from 1.64037 to 1.64027, saving model to weights/weights_epoch_216_loss_1.6403.hdf5\nEpoch 217/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6408\n\nEpoch 00217: loss did not improve from 1.64027\nEpoch 218/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6398\n\nEpoch 00218: loss improved from 1.64027 to 1.63981, saving model to weights/weights_epoch_218_loss_1.6398.hdf5\nEpoch 219/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6399\n\nEpoch 00219: loss did not improve from 1.63981\nEpoch 220/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6401\n\nEpoch 00220: loss did not improve from 1.63981\nEpoch 221/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6382\n\nMy War and Peace:\nment of the commander in chief was an anger and the french army was already accompanied by a coldier and the regiment. the french had been received and the regiment had been received and the regiment had been received and the regiment had been at the end of the first days of the twenty-sixth of august, the french had been received and the rest of the french army was a certain detachment of the french army and the rest of the french army and the rest of the french army was a very bad to the empero\n\nEpoch 00221: loss improved from 1.63981 to 1.63825, saving model to weights/weights_epoch_221_loss_1.6382.hdf5\nEpoch 222/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6386\n\nEpoch 00222: loss did not improve from 1.63825\nEpoch 223/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6389\n\nEpoch 00223: loss did not improve from 1.63825\nEpoch 224/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6388\n\nEpoch 00224: loss did not improve from 1.63825\nEpoch 225/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6381\n\nEpoch 00225: loss improved from 1.63825 to 1.63810, saving model to weights/weights_epoch_225_loss_1.6381.hdf5\nEpoch 226/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6386\n\nEpoch 00226: loss did not improve from 1.63810\nEpoch 227/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6356\n\nEpoch 00227: loss improved from 1.63810 to 1.63560, saving model to weights/weights_epoch_227_loss_1.6356.hdf5\nEpoch 228/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6377\n\nEpoch 00228: loss did not improve from 1.63560\nEpoch 229/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6377\n\nEpoch 00229: loss did not improve from 1.63560\nEpoch 230/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6376\n\nEpoch 00230: loss did not improve from 1.63560\nEpoch 231/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6355\n\nMy War and Peace:\n's to the emperor alexander himself with the same tenderness and self-confident smile of his face and his face was evidently fearing to say something.\n\n“well, and you? what do you think?” said prince andrew, “i am not\nto blame for your but to see you, and i am not to blame for you,\nand i am not to blame for you.”\n\n“yes, i saw it all, and i will do it all and want to see you,”\nsaid prince andrew, “i am not to blame for your but to say\nthat i have not yet seen him. i am afraid of him. i will see hi\n\nEpoch 00231: loss improved from 1.63560 to 1.63549, saving model to weights/weights_epoch_231_loss_1.6355.hdf5\nEpoch 232/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6367\n\nEpoch 00232: loss did not improve from 1.63549\nEpoch 233/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6357\n\nEpoch 00233: loss did not improve from 1.63549\nEpoch 234/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6350\n\nEpoch 00234: loss improved from 1.63549 to 1.63505, saving model to weights/weights_epoch_234_loss_1.6350.hdf5\nEpoch 235/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6368\n\nEpoch 00235: loss did not improve from 1.63505\nEpoch 236/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6346\n\nEpoch 00236: loss improved from 1.63505 to 1.63459, saving model to weights/weights_epoch_236_loss_1.6346.hdf5\nEpoch 237/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6332\n\nEpoch 00237: loss improved from 1.63459 to 1.63320, saving model to weights/weights_epoch_237_loss_1.6332.hdf5\nEpoch 238/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6336\n\nEpoch 00238: loss did not improve from 1.63320\nEpoch 239/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6349\n\nEpoch 00239: loss did not improve from 1.63320\nEpoch 240/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6322\n\nEpoch 00240: loss improved from 1.63320 to 1.63224, saving model to weights/weights_epoch_240_loss_1.6322.hdf5\nEpoch 241/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6349\n\nMy War and Peace:\nly and sincerely and silently in the same place which was still straight toward him and the sound of the sound of the sound of soldiers who were standing before the road. the soldiers were standing before them. the soldiers were standing before them. the soldiers were standing before them. the soldiers were standing before them. the soldiers were standing before them. the soldiers were standing before them. the soldiers were standing before them. the soldiers were standing before them. the soldie\n\nEpoch 00241: loss did not improve from 1.63224\nEpoch 242/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6326\n\nEpoch 00242: loss did not improve from 1.63224\nEpoch 243/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6318\n\nEpoch 00243: loss improved from 1.63224 to 1.63181, saving model to weights/weights_epoch_243_loss_1.6318.hdf5\nEpoch 244/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6326\n\nEpoch 00244: loss did not improve from 1.63181\nEpoch 245/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6326\n\nEpoch 00245: loss did not improve from 1.63181\nEpoch 246/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6312\n\nEpoch 00246: loss improved from 1.63181 to 1.63120, saving model to weights/weights_epoch_246_loss_1.6312.hdf5\nEpoch 247/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6324\n\nEpoch 00247: loss did not improve from 1.63120\nEpoch 248/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.6298\n\nEpoch 00248: loss improved from 1.63120 to 1.62979, saving model to weights/weights_epoch_248_loss_1.6298.hdf5\nEpoch 249/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6304\n\nEpoch 00249: loss did not improve from 1.62979\nEpoch 250/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6310\n\nEpoch 00250: loss did not improve from 1.62979\nEpoch 251/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6310\n\nMy War and Peace:\nch and so on. the soldiers were standing before the cathedral of the french army who were all conscious of the fact that the french army was the first time the emperor alexander did not know what to do. he was about to reply, but he was about to say something, but he was not an angel and angry and interrupted him.\n\n“what is it? what do you think? ‘as i said the countess, with a\nsmile of plaim and shining eyes.\n\n“well, and how do you think? while i have not seen the princess?”\n\n“i am so glad to se\n\nEpoch 00251: loss did not improve from 1.62979\nEpoch 252/301\n202/202 [==============================] - 29s 146ms/step - loss: 1.6297\n\nEpoch 00252: loss improved from 1.62979 to 1.62975, saving model to weights/weights_epoch_252_loss_1.6297.hdf5\nEpoch 253/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6295\n\nEpoch 00253: loss improved from 1.62975 to 1.62947, saving model to weights/weights_epoch_253_loss_1.6295.hdf5\nEpoch 254/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6306\n\nEpoch 00254: loss did not improve from 1.62947\nEpoch 255/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6319\n\nEpoch 00255: loss did not improve from 1.62947\nEpoch 256/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6297\n\nEpoch 00256: loss did not improve from 1.62947\nEpoch 257/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6289\n\nEpoch 00257: loss improved from 1.62947 to 1.62890, saving model to weights/weights_epoch_257_loss_1.6289.hdf5\nEpoch 258/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6279\n\nEpoch 00258: loss improved from 1.62890 to 1.62792, saving model to weights/weights_epoch_258_loss_1.6279.hdf5\nEpoch 259/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6279\n\nEpoch 00259: loss did not improve from 1.62792\nEpoch 260/301\n202/202 [==============================] - 29s 145ms/step - loss: 1.6285\n\nEpoch 00260: loss did not improve from 1.62792\nEpoch 261/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6274\n\nMy War and Peace:\nène was always seeking a long time and then she said this the countess was always seeking and the same tone and the countess was always to blame to her and to her own sorrow.\n\n“i have not seen them to the countess,” said prince andrew, “i know\nthat i can’t believe it and will not be able to say it. i will\ntell you that i shall not be able to say it. i will do anything\nabout it. i will do it all about it. i will do anything about it.\ni will do it all about it. i will do it all about it. i will tel\n\nEpoch 00261: loss improved from 1.62792 to 1.62741, saving model to weights/weights_epoch_261_loss_1.6274.hdf5\nEpoch 262/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6267\n\nEpoch 00262: loss improved from 1.62741 to 1.62669, saving model to weights/weights_epoch_262_loss_1.6267.hdf5\nEpoch 263/301\n202/202 [==============================] - 30s 146ms/step - loss: 1.6284\n\nEpoch 00263: loss did not improve from 1.62669\nEpoch 264/301\n202/202 [==============================] - 30s 151ms/step - loss: 1.6272\n\nEpoch 00264: loss did not improve from 1.62669\nEpoch 265/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6281\n\nEpoch 00265: loss did not improve from 1.62669\nEpoch 266/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6273\n\nEpoch 00266: loss did not improve from 1.62669\nEpoch 267/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6270\n\nEpoch 00267: loss did not improve from 1.62669\nEpoch 268/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6254\n\nEpoch 00268: loss improved from 1.62669 to 1.62542, saving model to weights/weights_epoch_268_loss_1.6254.hdf5\nEpoch 269/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6251\n\nEpoch 00269: loss improved from 1.62542 to 1.62507, saving model to weights/weights_epoch_269_loss_1.6251.hdf5\nEpoch 270/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6253\n\nEpoch 00270: loss did not improve from 1.62507\nEpoch 271/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6250\n\nMy War and Peace:\nurse and the same way the soldiers were standing before the carriage and stopped at the foot of the room.\n\n“there now, where are you going to see the count?” said the countess,\nwith a smile that she was always seared and the countess was always\nseared by the same thing that had been sent for the princess. the\ncountess was sitting at the door and went up to her mother. and\nshe sat down beside her and took her hand.\n\n“why do you say that i was told to me? you are a great deal, and then\ni was alread\n\nEpoch 00271: loss improved from 1.62507 to 1.62503, saving model to weights/weights_epoch_271_loss_1.6250.hdf5\nEpoch 272/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6257\n\nEpoch 00272: loss did not improve from 1.62503\nEpoch 273/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6261\n\nEpoch 00273: loss did not improve from 1.62503\nEpoch 274/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6245\n\nEpoch 00274: loss improved from 1.62503 to 1.62451, saving model to weights/weights_epoch_274_loss_1.6245.hdf5\nEpoch 275/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6239\n\nEpoch 00275: loss improved from 1.62451 to 1.62390, saving model to weights/weights_epoch_275_loss_1.6239.hdf5\nEpoch 276/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6247\n\nEpoch 00276: loss did not improve from 1.62390\nEpoch 277/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6234\n\nEpoch 00277: loss improved from 1.62390 to 1.62341, saving model to weights/weights_epoch_277_loss_1.6234.hdf5\nEpoch 278/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6236\n\nEpoch 00278: loss did not improve from 1.62341\nEpoch 279/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6235\n\nEpoch 00279: loss did not improve from 1.62341\nEpoch 280/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6231\n\nEpoch 00280: loss improved from 1.62341 to 1.62308, saving model to weights/weights_epoch_280_loss_1.6231.hdf5\nEpoch 281/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6226\n\nMy War and Peace:\n) the countess was already sitting at the door.\n\n“the countess is a soldier, and i will go and see him and say that i\nwas told to hear the countess.”\n\n“i don’t know what i say, but i am afraid of you, but i want to see\nyou.”\n\n“no, i won’t tell you the whole affair and the emperor alexander and\nhis comrades will be a great man.”\n\n“well, that’s a good thing,” said prince andrew, “i want to tell\nyou that i have not been to see you.”\n\n“i don’t know what i say, but i am afraid of you, but i want to se\n\nEpoch 00281: loss improved from 1.62308 to 1.62257, saving model to weights/weights_epoch_281_loss_1.6226.hdf5\nEpoch 282/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6227\n\nEpoch 00282: loss did not improve from 1.62257\nEpoch 283/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6217\n\nEpoch 00283: loss improved from 1.62257 to 1.62171, saving model to weights/weights_epoch_283_loss_1.6217.hdf5\nEpoch 284/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6241\n\nEpoch 00284: loss did not improve from 1.62171\nEpoch 285/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6228\n\nEpoch 00285: loss did not improve from 1.62171\nEpoch 286/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6229\n\nEpoch 00286: loss did not improve from 1.62171\nEpoch 287/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6226\n\nEpoch 00287: loss did not improve from 1.62171\nEpoch 288/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6229\n\nEpoch 00288: loss did not improve from 1.62171\nEpoch 289/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6232\n\nEpoch 00289: loss did not improve from 1.62171\nEpoch 290/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6210\n\nEpoch 00290: loss improved from 1.62171 to 1.62104, saving model to weights/weights_epoch_290_loss_1.6210.hdf5\nEpoch 291/301\n202/202 [==============================] - 29s 143ms/step - loss: 1.6228\n\nMy War and Peace:\nóv had been there and the enemy was a great man. the officer was so strong that the french had been told that the french had been destroyed. the french had already been the regimental commander with a single and animated face and seemed to him that he was already an anger and the same thing that had been so but of the country.\n\n“i am sorry for you, but i want to see you a little fate,” said\nprince andrew, “i am so glad to see you a should be a good thing\nto you. i am going to see you and that i w\n\nEpoch 00291: loss did not improve from 1.62104\nEpoch 292/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6214\n\nEpoch 00292: loss did not improve from 1.62104\nEpoch 293/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6213\n\nEpoch 00293: loss did not improve from 1.62104\nEpoch 294/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6198\n\nEpoch 00294: loss improved from 1.62104 to 1.61984, saving model to weights/weights_epoch_294_loss_1.6198.hdf5\nEpoch 295/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6214\n\nEpoch 00295: loss did not improve from 1.61984\nEpoch 296/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6186\n\nEpoch 00296: loss improved from 1.61984 to 1.61864, saving model to weights/weights_epoch_296_loss_1.6186.hdf5\nEpoch 297/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6194\n\nEpoch 00297: loss did not improve from 1.61864\nEpoch 298/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6189\n\nEpoch 00298: loss did not improve from 1.61864\nEpoch 299/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6185\n\nEpoch 00299: loss improved from 1.61864 to 1.61852, saving model to weights/weights_epoch_299_loss_1.6185.hdf5\nEpoch 300/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6188\n\nEpoch 00300: loss did not improve from 1.61852\nEpoch 301/301\n202/202 [==============================] - 29s 144ms/step - loss: 1.6180\n\nMy War and Peace:\nà la gentre,  * said the countess, “i want to know you with the countess’ marriage.”\n\n“i don’t know what i say it will be so and that i am not to blame,”\nsaid princess mary. “he is so glad to see you, and i am so glad to\nsay that i am not a grandfather. i am so glad to have a soldier\nto hear him and have a serious man who has been to be sent to\nthe country. but he was a great sense of menting and the same thing\nthat had been so beautiful and the expression of his face caused\nhis face and he felt \n\nEpoch 00301: loss improved from 1.61852 to 1.61802, saving model to weights/weights_epoch_301_loss_1.6180.hdf5\n",
     "output_type": "stream"
    },
    {
     "execution_count": 1,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f9530472fd0>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "3 books as input data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nimport numpy as np\nimport os\nimport pickle\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom string import punctuation\nfrom tensorflow.keras import layers, models, losses, optimizers\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\nimport requests\n\nFirstBook = open('/kaggle/input/dlhwch13/Little Women.txt', 'r', encoding=\"utf8\").read()\nSecondBook = open('/kaggle/input/dlhwch13/THE CATCHER IN THE RYE .txt', 'r', encoding=\"utf8\").read()\nThirdBook = open('/kaggle/input/dlhwch13/THE HUNCHBACK OF NOTRE DAME.txt', 'r', encoding=\"utf8\").read()\n\ntraining_file = FirstBook + SecondBook + ThirdBook\nraw_text = training_file.lower()\n\nprint(raw_text[:200])\n\nall_words = raw_text.split()\nunique_words = list(set(all_words))\nprint(f'Number of unique words: {len(unique_words)}')\nn_chars = len(raw_text)\nprint(f'Total characters: {n_chars}')\n\nchars = sorted(list(set(raw_text)))\nn_vocab = len(chars)\nprint(f'Total vocabulary (unique characters): {n_vocab}')\nprint(chars)\n\nindex_to_char = dict((i, c) for i, c in enumerate(chars))\nchar_to_index = dict((c, i) for i, c in enumerate(chars))\nprint(char_to_index)\n\nseq_length = 160\nn_seq = int(n_chars / seq_length)\n\nX = np.zeros((n_seq, seq_length, n_vocab))\nY = np.zeros((n_seq, seq_length, n_vocab))\n\nfor i in range(n_seq):\n    x_sequence = raw_text[i * seq_length: (i + 1) * seq_length]\n    x_sequence_ohe = np.zeros((seq_length, n_vocab))\n    for j in range(seq_length):\n        char = x_sequence[j]\n        index = char_to_index[char]\n        x_sequence_ohe[j][index] = 1.\n    X[i] = x_sequence_ohe\n    y_sequence = raw_text[i * seq_length + 1: (i + 1) * seq_length + 1]\n    y_sequence_ohe = np.zeros((seq_length, n_vocab))\n    for j in range(seq_length):\n        char = y_sequence[j]\n        index = char_to_index[char]\n        y_sequence_ohe[j][index] = 1.\n    Y[i] = y_sequence_ohe\n\nprint(X.shape)\nprint(Y.shape)\n\ntf.random.set_seed(42)\nbatch_size = 100\nhidden_units = 700\nn_epoch = 301\ndropout = 0.4\n\nmodel = models.Sequential()\nmodel.add(layers.LSTM(hidden_units, input_shape=(None, n_vocab), return_sequences=True, dropout=dropout))\nmodel.add(layers.LSTM(hidden_units, return_sequences=True, dropout=dropout))\nmodel.add(layers.TimeDistributed(layers.Dense(n_vocab, activation='softmax')))\n\noptimizer = optimizers.RMSprop(learning_rate=0.001)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n\nprint(model.summary())\n\nfilepath = \"weights/weights_epoch_{epoch:03d}_loss_{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\nearly_stop = EarlyStopping(monitor='loss', min_delta=0, patience=50, verbose=1, mode='min')\n\n\ndef generate_text(model, gen_length, n_vocab, index_to_char):\n    index = np.random.randint(n_vocab)\n    y_char = [index_to_char[index]]\n    X = np.zeros((1, gen_length, n_vocab))\n    for i in range(gen_length):\n        X[0, i, index] = 1.\n        indices = np.argmax(model.predict(X[:, max(0, i - 99):i + 1, :])[0], 1)\n        index = indices[-1]\n        y_char.append(index_to_char[index])\n    return ''.join(y_char)\n\n\nclass ResultChecker(Callback):\n    def __init__(self, model, N, gen_length):\n        self.model = model\n        self.N = N\n        self.gen_length = gen_length\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.N == 0:\n            result = generate_text(self.model, self.gen_length, n_vocab, index_to_char)\n            print('\\nMy result with 3 selected books:\\n' + result)\n\n\nresult_checker = ResultChecker(model, 10, 500)\n\nmodel.fit(X, Y, batch_size=batch_size, verbose=1, epochs=n_epoch, callbacks=[result_checker, checkpoint, early_stop])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-16T07:00:45.856629Z",
     "iopub.execute_input": "2022-07-16T07:00:45.857056Z",
     "iopub.status.idle": "2022-07-16T09:08:18.275137Z",
     "shell.execute_reply.started": "2022-07-16T07:00:45.857019Z",
     "shell.execute_reply": "2022-07-16T09:08:18.274115Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "﻿“christmas won’t be christmas without any presents,” grumbled jo, lying\non the rug.\n\n“it’s so dreadful to be poor!” sighed meg, looking down at her old\ndress.\n\n“i don’t think it’s fair for some girls\nNumber of unique words: 41601\nTotal characters: 2480374\nTotal vocabulary (unique characters): 101\n['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'á', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'ï', 'ñ', 'ô', 'ù', 'œ', 'α', 'β', 'γ', 'δ', 'ε', 'η', 'ι', 'κ', 'λ', 'μ', 'ν', 'ο', 'ς', 'τ', 'φ', 'ἀ', 'ὀ', 'ά', 'έ', 'ί', 'ό', 'ῖ', '—', '‘', '’', '“', '”', '\\ufeff']\n{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '?': 27, '@': 28, '[': 29, ']': 30, '_': 31, 'a': 32, 'b': 33, 'c': 34, 'd': 35, 'e': 36, 'f': 37, 'g': 38, 'h': 39, 'i': 40, 'j': 41, 'k': 42, 'l': 43, 'm': 44, 'n': 45, 'o': 46, 'p': 47, 'q': 48, 'r': 49, 's': 50, 't': 51, 'u': 52, 'v': 53, 'w': 54, 'x': 55, 'y': 56, 'z': 57, 'à': 58, 'á': 59, 'â': 60, 'æ': 61, 'ç': 62, 'è': 63, 'é': 64, 'ê': 65, 'ë': 66, 'î': 67, 'ï': 68, 'ñ': 69, 'ô': 70, 'ù': 71, 'œ': 72, 'α': 73, 'β': 74, 'γ': 75, 'δ': 76, 'ε': 77, 'η': 78, 'ι': 79, 'κ': 80, 'λ': 81, 'μ': 82, 'ν': 83, 'ο': 84, 'ς': 85, 'τ': 86, 'φ': 87, 'ἀ': 88, 'ὀ': 89, 'ά': 90, 'έ': 91, 'ί': 92, 'ό': 93, 'ῖ': 94, '—': 95, '‘': 96, '’': 97, '“': 98, '”': 99, '\\ufeff': 100}\n(15502, 160, 101)\n(15502, 160, 101)\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_2 (LSTM)                (None, None, 700)         2245600   \n_________________________________________________________________\nlstm_3 (LSTM)                (None, None, 700)         3922800   \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, None, 101)         70801     \n=================================================================\nTotal params: 6,239,201\nTrainable params: 6,239,201\nNon-trainable params: 0\n_________________________________________________________________\nNone\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-07-16 07:00:54.065644: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1002049280 exceeds 10% of free system memory.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1/301\n156/156 [==============================] - 26s 149ms/step - loss: 2.9844\n\nMy result with 3 selected books:\n] tit in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it in t it\n\nEpoch 00001: loss improved from inf to 2.98441, saving model to weights/weights_epoch_001_loss_2.9844.hdf5\nEpoch 2/301\n156/156 [==============================] - 23s 148ms/step - loss: 2.5547\n\nEpoch 00002: loss improved from 2.98441 to 2.55467, saving model to weights/weights_epoch_002_loss_2.5547.hdf5\nEpoch 3/301\n156/156 [==============================] - 23s 148ms/step - loss: 2.4076\n\nEpoch 00003: loss improved from 2.55467 to 2.40763, saving model to weights/weights_epoch_003_loss_2.4076.hdf5\nEpoch 4/301\n156/156 [==============================] - 23s 148ms/step - loss: 2.3216\n\nEpoch 00004: loss improved from 2.40763 to 2.32155, saving model to weights/weights_epoch_004_loss_2.3216.hdf5\nEpoch 5/301\n156/156 [==============================] - 23s 147ms/step - loss: 2.2622\n\nEpoch 00005: loss improved from 2.32155 to 2.26215, saving model to weights/weights_epoch_005_loss_2.2622.hdf5\nEpoch 6/301\n156/156 [==============================] - 23s 147ms/step - loss: 2.2157\n\nEpoch 00006: loss improved from 2.26215 to 2.21573, saving model to weights/weights_epoch_006_loss_2.2157.hdf5\nEpoch 7/301\n156/156 [==============================] - 23s 147ms/step - loss: 2.1785\n\nEpoch 00007: loss improved from 2.21573 to 2.17850, saving model to weights/weights_epoch_007_loss_2.1785.hdf5\nEpoch 8/301\n156/156 [==============================] - 23s 147ms/step - loss: 2.1462\n\nEpoch 00008: loss improved from 2.17850 to 2.14616, saving model to weights/weights_epoch_008_loss_2.1462.hdf5\nEpoch 9/301\n156/156 [==============================] - 23s 147ms/step - loss: 2.1200\n\nEpoch 00009: loss improved from 2.14616 to 2.11999, saving model to weights/weights_epoch_009_loss_2.1200.hdf5\nEpoch 10/301\n156/156 [==============================] - 23s 147ms/step - loss: 2.0971\n\nEpoch 00010: loss improved from 2.11999 to 2.09707, saving model to weights/weights_epoch_010_loss_2.0971.hdf5\nEpoch 11/301\n156/156 [==============================] - 23s 147ms/step - loss: 2.0797\n\nMy result with 3 selected books:\njo was so such and said to say to hand to make her so say and say to make me and say in the same of the card of the carding of the card of the carding of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardinal said of the cardina\n\nEpoch 00011: loss improved from 2.09707 to 2.07972, saving model to weights/weights_epoch_011_loss_2.0797.hdf5\nEpoch 12/301\n156/156 [==============================] - 23s 148ms/step - loss: 2.0618\n\nEpoch 00012: loss improved from 2.07972 to 2.06180, saving model to weights/weights_epoch_012_loss_2.0618.hdf5\nEpoch 13/301\n156/156 [==============================] - 23s 146ms/step - loss: 2.0473\n\nEpoch 00013: loss improved from 2.06180 to 2.04727, saving model to weights/weights_epoch_013_loss_2.0473.hdf5\nEpoch 14/301\n156/156 [==============================] - 23s 147ms/step - loss: 2.0334\n\nEpoch 00014: loss improved from 2.04727 to 2.03335, saving model to weights/weights_epoch_014_loss_2.0334.hdf5\nEpoch 15/301\n156/156 [==============================] - 23s 147ms/step - loss: 2.0213\n\nEpoch 00015: loss improved from 2.03335 to 2.02128, saving model to weights/weights_epoch_015_loss_2.0213.hdf5\nEpoch 16/301\n156/156 [==============================] - 23s 146ms/step - loss: 2.0120\n\nEpoch 00016: loss improved from 2.02128 to 2.01201, saving model to weights/weights_epoch_016_loss_2.0120.hdf5\nEpoch 17/301\n156/156 [==============================] - 23s 146ms/step - loss: 2.0031\n\nEpoch 00017: loss improved from 2.01201 to 2.00314, saving model to weights/weights_epoch_017_loss_2.0031.hdf5\nEpoch 18/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9926\n\nEpoch 00018: loss improved from 2.00314 to 1.99258, saving model to weights/weights_epoch_018_loss_1.9926.hdf5\nEpoch 19/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9827\n\nEpoch 00019: loss improved from 1.99258 to 1.98273, saving model to weights/weights_epoch_019_loss_1.9827.hdf5\nEpoch 20/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.9766\n\nEpoch 00020: loss improved from 1.98273 to 1.97658, saving model to weights/weights_epoch_020_loss_1.9766.hdf5\nEpoch 21/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9689\n\nMy result with 3 selected books:\nxed to the sort of the courts, and the same to see the sound of the courts, and the same to see the sound of the sound of the sound of the courts, and the same to see the sound of the courts, and the same to see the sound of the sound of the sound of the courts, and the same to see the sound of the courts, and the same to see the sound of the sound of the sound of the courts, and the same to see the sound of the courts, and the same to see the sound of the sound of the sound of the courts, and th\n\nEpoch 00021: loss improved from 1.97658 to 1.96895, saving model to weights/weights_epoch_021_loss_1.9689.hdf5\nEpoch 22/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9624\n\nEpoch 00022: loss improved from 1.96895 to 1.96245, saving model to weights/weights_epoch_022_loss_1.9624.hdf5\nEpoch 23/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9541\n\nEpoch 00023: loss improved from 1.96245 to 1.95414, saving model to weights/weights_epoch_023_loss_1.9541.hdf5\nEpoch 24/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9492\n\nEpoch 00024: loss improved from 1.95414 to 1.94923, saving model to weights/weights_epoch_024_loss_1.9492.hdf5\nEpoch 25/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9416\n\nEpoch 00025: loss improved from 1.94923 to 1.94163, saving model to weights/weights_epoch_025_loss_1.9416.hdf5\nEpoch 26/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9378\n\nEpoch 00026: loss improved from 1.94163 to 1.93781, saving model to weights/weights_epoch_026_loss_1.9378.hdf5\nEpoch 27/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9316\n\nEpoch 00027: loss improved from 1.93781 to 1.93161, saving model to weights/weights_epoch_027_loss_1.9316.hdf5\nEpoch 28/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.9277\n\nEpoch 00028: loss improved from 1.93161 to 1.92767, saving model to weights/weights_epoch_028_loss_1.9277.hdf5\nEpoch 29/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9243\n\nEpoch 00029: loss improved from 1.92767 to 1.92427, saving model to weights/weights_epoch_029_loss_1.9243.hdf5\nEpoch 30/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.9194\n\nEpoch 00030: loss improved from 1.92427 to 1.91941, saving model to weights/weights_epoch_030_loss_1.9194.hdf5\nEpoch 31/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9148\n\nMy result with 3 selected books:\n20ç((8) (return) a sort of a project gutenberg-tm electronic works in the project gutenberg-tm electronic works in the project gutenberg-tm electronic works in the project gutenberg-tm electronic works in the project gutenberg-tm electronic works in the project gutenberg-tm electronic works in the project gutenberg-tm electronic works in the project gutenberg-tm electronic works in the project gutenberg-tm electronic works in the project gutenberg-tm electronic works in the project gutenberg-tm e\n\nEpoch 00031: loss improved from 1.91941 to 1.91480, saving model to weights/weights_epoch_031_loss_1.9148.hdf5\nEpoch 32/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9095\n\nEpoch 00032: loss improved from 1.91480 to 1.90954, saving model to weights/weights_epoch_032_loss_1.9095.hdf5\nEpoch 33/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.9079\n\nEpoch 00033: loss improved from 1.90954 to 1.90786, saving model to weights/weights_epoch_033_loss_1.9079.hdf5\nEpoch 34/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.9017\n\nEpoch 00034: loss improved from 1.90786 to 1.90169, saving model to weights/weights_epoch_034_loss_1.9017.hdf5\nEpoch 35/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8982\n\nEpoch 00035: loss improved from 1.90169 to 1.89818, saving model to weights/weights_epoch_035_loss_1.8982.hdf5\nEpoch 36/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8950\n\nEpoch 00036: loss improved from 1.89818 to 1.89501, saving model to weights/weights_epoch_036_loss_1.8950.hdf5\nEpoch 37/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8922\n\nEpoch 00037: loss improved from 1.89501 to 1.89225, saving model to weights/weights_epoch_037_loss_1.8922.hdf5\nEpoch 38/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8891\n\nEpoch 00038: loss improved from 1.89225 to 1.88909, saving model to weights/weights_epoch_038_loss_1.8891.hdf5\nEpoch 39/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8862\n\nEpoch 00039: loss improved from 1.88909 to 1.88617, saving model to weights/weights_epoch_039_loss_1.8862.hdf5\nEpoch 40/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8839\n\nEpoch 00040: loss improved from 1.88617 to 1.88394, saving model to weights/weights_epoch_040_loss_1.8839.hdf5\nEpoch 41/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8783\n\nMy result with 3 selected books:\nù the poor girls were all the window. they were all the time to the poor girls when they were all the time to the poon and then i was a lot of sound of the bast time. i was a lot of sort of sound of the bast time. i was a lot of sort of sound of the bast time. i was a lot of sort of sound of the bast time. i was a lot of sort of sound of the bast time. i was a lot of sort of sound of the bast time. i was a lot of sort of sound of the bast time. i was a lot of sort of sound of the bast time. i was\n\nEpoch 00041: loss improved from 1.88394 to 1.87831, saving model to weights/weights_epoch_041_loss_1.8783.hdf5\nEpoch 42/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8758\n\nEpoch 00042: loss improved from 1.87831 to 1.87576, saving model to weights/weights_epoch_042_loss_1.8758.hdf5\nEpoch 43/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8735\n\nEpoch 00043: loss improved from 1.87576 to 1.87346, saving model to weights/weights_epoch_043_loss_1.8735.hdf5\nEpoch 44/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8727\n\nEpoch 00044: loss improved from 1.87346 to 1.87274, saving model to weights/weights_epoch_044_loss_1.8727.hdf5\nEpoch 45/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8688\n\nEpoch 00045: loss improved from 1.87274 to 1.86882, saving model to weights/weights_epoch_045_loss_1.8688.hdf5\nEpoch 46/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8671\n\nEpoch 00046: loss improved from 1.86882 to 1.86715, saving model to weights/weights_epoch_046_loss_1.8671.hdf5\nEpoch 47/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8629\n\nEpoch 00047: loss improved from 1.86715 to 1.86290, saving model to weights/weights_epoch_047_loss_1.8629.hdf5\nEpoch 48/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8605\n\nEpoch 00048: loss improved from 1.86290 to 1.86050, saving model to weights/weights_epoch_048_loss_1.8605.hdf5\nEpoch 49/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8593\n\nEpoch 00049: loss improved from 1.86050 to 1.85932, saving model to weights/weights_epoch_049_loss_1.8593.hdf5\nEpoch 50/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8552\n\nEpoch 00050: loss improved from 1.85932 to 1.85524, saving model to weights/weights_epoch_050_loss_1.8552.hdf5\nEpoch 51/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8535\n\nMy result with 3 selected books:\n@ they were all alone. they were all alone, they were all right, and they were all sort of stuff. i didn't even like them to do they don't even like them to the trouble with me, though. i didn't even like them to do they don't even like them to the trouble with me, though. i didn't even like them to do they don't even like them to the trouble with me, though. i didn't even like them to do they don't even like them to the trouble with me, though. i didn't even like them to do they don't even like \n\nEpoch 00051: loss improved from 1.85524 to 1.85347, saving model to weights/weights_epoch_051_loss_1.8535.hdf5\nEpoch 52/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8525\n\nEpoch 00052: loss improved from 1.85347 to 1.85249, saving model to weights/weights_epoch_052_loss_1.8525.hdf5\nEpoch 53/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8498\n\nEpoch 00053: loss improved from 1.85249 to 1.84981, saving model to weights/weights_epoch_053_loss_1.8498.hdf5\nEpoch 54/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8472\n\nEpoch 00054: loss improved from 1.84981 to 1.84717, saving model to weights/weights_epoch_054_loss_1.8472.hdf5\nEpoch 55/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8456\n\nEpoch 00055: loss improved from 1.84717 to 1.84561, saving model to weights/weights_epoch_055_loss_1.8456.hdf5\nEpoch 56/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8420\n\nEpoch 00056: loss improved from 1.84561 to 1.84200, saving model to weights/weights_epoch_056_loss_1.8420.hdf5\nEpoch 57/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8421\n\nEpoch 00057: loss did not improve from 1.84200\nEpoch 58/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8411\n\nEpoch 00058: loss improved from 1.84200 to 1.84108, saving model to weights/weights_epoch_058_loss_1.8411.hdf5\nEpoch 59/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8389\n\nEpoch 00059: loss improved from 1.84108 to 1.83887, saving model to weights/weights_epoch_059_loss_1.8389.hdf5\nEpoch 60/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8365\n\nEpoch 00060: loss improved from 1.83887 to 1.83645, saving model to weights/weights_epoch_060_loss_1.8365.hdf5\nEpoch 61/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8344\n\nMy result with 3 selected books:\nête de la boucherie, and the priest and the pointed cantlemen, the portal of the courts, the portal of the police of the courts, the portal of the police of the courts, the portal of the police of the courts, the portal of the police of the courts, the portal of the police of the courts, the portal of the police of the courts, the portal of the police of the courts, the portal of the police of the courts, the portal of the police of the courts, the portal of the police of the courts, the portal o\n\nEpoch 00061: loss improved from 1.83645 to 1.83438, saving model to weights/weights_epoch_061_loss_1.8344.hdf5\nEpoch 62/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.8319\n\nEpoch 00062: loss improved from 1.83438 to 1.83188, saving model to weights/weights_epoch_062_loss_1.8319.hdf5\nEpoch 63/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.8297\n\nEpoch 00063: loss improved from 1.83188 to 1.82969, saving model to weights/weights_epoch_063_loss_1.8297.hdf5\nEpoch 64/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.8279\n\nEpoch 00064: loss improved from 1.82969 to 1.82792, saving model to weights/weights_epoch_064_loss_1.8279.hdf5\nEpoch 65/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.8272\n\nEpoch 00065: loss improved from 1.82792 to 1.82717, saving model to weights/weights_epoch_065_loss_1.8272.hdf5\nEpoch 66/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8272\n\nEpoch 00066: loss did not improve from 1.82717\nEpoch 67/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8237\n\nEpoch 00067: loss improved from 1.82717 to 1.82369, saving model to weights/weights_epoch_067_loss_1.8237.hdf5\nEpoch 68/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8234\n\nEpoch 00068: loss improved from 1.82369 to 1.82340, saving model to weights/weights_epoch_068_loss_1.8234.hdf5\nEpoch 69/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8220\n\nEpoch 00069: loss improved from 1.82340 to 1.82199, saving model to weights/weights_epoch_069_loss_1.8220.hdf5\nEpoch 70/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8189\n\nEpoch 00070: loss improved from 1.82199 to 1.81892, saving model to weights/weights_epoch_070_loss_1.8189.hdf5\nEpoch 71/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8164\n\nMy result with 3 selected books:\nçade, the solemn of the poltain of the cathedral, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-mic\n\nEpoch 00071: loss improved from 1.81892 to 1.81636, saving model to weights/weights_epoch_071_loss_1.8164.hdf5\nEpoch 72/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8186\n\nEpoch 00072: loss did not improve from 1.81636\nEpoch 73/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8159\n\nEpoch 00073: loss improved from 1.81636 to 1.81591, saving model to weights/weights_epoch_073_loss_1.8159.hdf5\nEpoch 74/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8161\n\nEpoch 00074: loss did not improve from 1.81591\nEpoch 75/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8138\n\nEpoch 00075: loss improved from 1.81591 to 1.81383, saving model to weights/weights_epoch_075_loss_1.8138.hdf5\nEpoch 76/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8120\n\nEpoch 00076: loss improved from 1.81383 to 1.81200, saving model to weights/weights_epoch_076_loss_1.8120.hdf5\nEpoch 77/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8095\n\nEpoch 00077: loss improved from 1.81200 to 1.80953, saving model to weights/weights_epoch_077_loss_1.8095.hdf5\nEpoch 78/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8082\n\nEpoch 00078: loss improved from 1.80953 to 1.80818, saving model to weights/weights_epoch_078_loss_1.8082.hdf5\nEpoch 79/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8073\n\nEpoch 00079: loss improved from 1.80818 to 1.80729, saving model to weights/weights_epoch_079_loss_1.8073.hdf5\nEpoch 80/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8077\n\nEpoch 00080: loss did not improve from 1.80729\nEpoch 81/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8044\n\nMy result with 3 selected books:\n—\n\n“i have not a bit of money, i will not have to be a good\nmany things to you, and i have not to be a good\nmany things to you, and i have to have a good\ntime to be a good time, i shall not have to do it,”\nsaid meg, with a smile.\n\n“i think you are always a little girl, i will be a good time, i don’t\nwish to be a good time,” said meg, with a smile.\n\n“i think you are always a little girl, i will be a good time, i don’t\nwish to be a good time,” said meg, with a smile.\n\n“i think you are always a litt\n\nEpoch 00081: loss improved from 1.80729 to 1.80438, saving model to weights/weights_epoch_081_loss_1.8044.hdf5\nEpoch 82/301\n156/156 [==============================] - 23s 149ms/step - loss: 1.8023\n\nEpoch 00082: loss improved from 1.80438 to 1.80229, saving model to weights/weights_epoch_082_loss_1.8023.hdf5\nEpoch 83/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.8019\n\nEpoch 00083: loss improved from 1.80229 to 1.80186, saving model to weights/weights_epoch_083_loss_1.8019.hdf5\nEpoch 84/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7992\n\nEpoch 00084: loss improved from 1.80186 to 1.79916, saving model to weights/weights_epoch_084_loss_1.7992.hdf5\nEpoch 85/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8005\n\nEpoch 00085: loss did not improve from 1.79916\nEpoch 86/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.8005\n\nEpoch 00086: loss did not improve from 1.79916\nEpoch 87/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7993\n\nEpoch 00087: loss did not improve from 1.79916\nEpoch 88/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7975\n\nEpoch 00088: loss improved from 1.79916 to 1.79746, saving model to weights/weights_epoch_088_loss_1.7975.hdf5\nEpoch 89/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7956\n\nEpoch 00089: loss improved from 1.79746 to 1.79564, saving model to weights/weights_epoch_089_loss_1.7956.hdf5\nEpoch 90/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7967\n\nEpoch 00090: loss did not improve from 1.79564\nEpoch 91/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7919\n\nMy result with 3 selected books:\nd the provest of the court of paris, the princess of the portal of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of the courts of \n\nEpoch 00091: loss improved from 1.79564 to 1.79193, saving model to weights/weights_epoch_091_loss_1.7919.hdf5\nEpoch 92/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7940\n\nEpoch 00092: loss did not improve from 1.79193\nEpoch 93/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7909\n\nEpoch 00093: loss improved from 1.79193 to 1.79086, saving model to weights/weights_epoch_093_loss_1.7909.hdf5\nEpoch 94/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7889\n\nEpoch 00094: loss improved from 1.79086 to 1.78889, saving model to weights/weights_epoch_094_loss_1.7889.hdf5\nEpoch 95/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7895\n\nEpoch 00095: loss did not improve from 1.78889\nEpoch 96/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7874\n\nEpoch 00096: loss improved from 1.78889 to 1.78742, saving model to weights/weights_epoch_096_loss_1.7874.hdf5\nEpoch 97/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7868\n\nEpoch 00097: loss improved from 1.78742 to 1.78676, saving model to weights/weights_epoch_097_loss_1.7868.hdf5\nEpoch 98/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7849\n\nEpoch 00098: loss improved from 1.78676 to 1.78492, saving model to weights/weights_epoch_098_loss_1.7849.hdf5\nEpoch 99/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7868\n\nEpoch 00099: loss did not improve from 1.78492\nEpoch 100/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7826\n\nEpoch 00100: loss improved from 1.78492 to 1.78257, saving model to weights/weights_epoch_100_loss_1.7826.hdf5\nEpoch 101/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7834\n\nMy result with 3 selected books:\nἀçç(ïïïïïïïïçïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïïï both and the priest sat towards the captain.\n\n“what is the matter is the matter?” said the priest.\n\n“and that is the matter when i have made a great deal of many things\nto me that i have not to be a great deal of many things to do with the\nsort of manner and mather and more than a could of mind that the\nmother was a great deal of such a face that she was a little\nchild which she had not been so s\n\nEpoch 00101: loss did not improve from 1.78257\nEpoch 102/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7791\n\nEpoch 00102: loss improved from 1.78257 to 1.77911, saving model to weights/weights_epoch_102_loss_1.7791.hdf5\nEpoch 103/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7807\n\nEpoch 00103: loss did not improve from 1.77911\nEpoch 104/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7802\n\nEpoch 00104: loss did not improve from 1.77911\nEpoch 105/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7789\n\nEpoch 00105: loss improved from 1.77911 to 1.77889, saving model to weights/weights_epoch_105_loss_1.7789.hdf5\nEpoch 106/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7787\n\nEpoch 00106: loss improved from 1.77889 to 1.77866, saving model to weights/weights_epoch_106_loss_1.7787.hdf5\nEpoch 107/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7785\n\nEpoch 00107: loss improved from 1.77866 to 1.77853, saving model to weights/weights_epoch_107_loss_1.7785.hdf5\nEpoch 108/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7763\n\nEpoch 00108: loss improved from 1.77853 to 1.77635, saving model to weights/weights_epoch_108_loss_1.7763.hdf5\nEpoch 109/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7745\n\nEpoch 00109: loss improved from 1.77635 to 1.77450, saving model to weights/weights_epoch_109_loss_1.7745.hdf5\nEpoch 110/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7756\n\nEpoch 00110: loss did not improve from 1.77450\nEpoch 111/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7740\n\nMy result with 3 selected books:\n; the sound of the provostship of the courts of the courts of paris, which were the same time to the present day, the cardinal de courts was to be seen in the presence of the provost of paris, whose two hundred and the first were all there, the same crowd was a content with their fares and the rest of the provostship of the courts of the provostship of paris, who had been a treasure of the same time of the provostship of paris, who had been a treasure of the same time of the provostship of the co\n\nEpoch 00111: loss improved from 1.77450 to 1.77404, saving model to weights/weights_epoch_111_loss_1.7740.hdf5\nEpoch 112/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7729\n\nEpoch 00112: loss improved from 1.77404 to 1.77291, saving model to weights/weights_epoch_112_loss_1.7729.hdf5\nEpoch 113/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7745\n\nEpoch 00113: loss did not improve from 1.77291\nEpoch 114/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7721\n\nEpoch 00114: loss improved from 1.77291 to 1.77208, saving model to weights/weights_epoch_114_loss_1.7721.hdf5\nEpoch 115/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7721\n\nEpoch 00115: loss did not improve from 1.77208\nEpoch 116/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7706\n\nEpoch 00116: loss improved from 1.77208 to 1.77064, saving model to weights/weights_epoch_116_loss_1.7706.hdf5\nEpoch 117/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7695\n\nEpoch 00117: loss improved from 1.77064 to 1.76950, saving model to weights/weights_epoch_117_loss_1.7695.hdf5\nEpoch 118/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7676\n\nEpoch 00118: loss improved from 1.76950 to 1.76756, saving model to weights/weights_epoch_118_loss_1.7676.hdf5\nEpoch 119/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7692\n\nEpoch 00119: loss did not improve from 1.76756\nEpoch 120/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7654\n\nEpoch 00120: loss improved from 1.76756 to 1.76544, saving model to weights/weights_epoch_120_loss_1.7654.hdf5\nEpoch 121/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7669\n\nMy result with 3 selected books:\nre the priest’s hands were so long and sorrow. the priest had not been seen in the cold and despair. he was a little condemned with his foot, and he had not been seen in the cold and all. he was all the wine of the cold and all. the only thing i was going to call up and start telling him i was a little bit of a goddam book. i didn't even know it was a pretty good book. i didn't even know it was a pretty good book. i didn't even know it was a pretty good book. i didn't even know it was a pretty go\n\nEpoch 00121: loss did not improve from 1.76544\nEpoch 122/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7650\n\nEpoch 00122: loss improved from 1.76544 to 1.76504, saving model to weights/weights_epoch_122_loss_1.7650.hdf5\nEpoch 123/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7655\n\nEpoch 00123: loss did not improve from 1.76504\nEpoch 124/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7642\n\nEpoch 00124: loss improved from 1.76504 to 1.76418, saving model to weights/weights_epoch_124_loss_1.7642.hdf5\nEpoch 125/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7633\n\nEpoch 00125: loss improved from 1.76418 to 1.76331, saving model to weights/weights_epoch_125_loss_1.7633.hdf5\nEpoch 126/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7615\n\nEpoch 00126: loss improved from 1.76331 to 1.76149, saving model to weights/weights_epoch_126_loss_1.7615.hdf5\nEpoch 127/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7616\n\nEpoch 00127: loss did not improve from 1.76149\nEpoch 128/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7614\n\nEpoch 00128: loss improved from 1.76149 to 1.76144, saving model to weights/weights_epoch_128_loss_1.7614.hdf5\nEpoch 129/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7600\n\nEpoch 00129: loss improved from 1.76144 to 1.76003, saving model to weights/weights_epoch_129_loss_1.7600.hdf5\nEpoch 130/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7579\n\nEpoch 00130: loss improved from 1.76003 to 1.75787, saving model to weights/weights_epoch_130_loss_1.7579.hdf5\nEpoch 131/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7601\n\nMy result with 3 selected books:\n's a few minutes. i didn't even know where the hell to say in the war. i was sort of sorry for a while, though. i sort of cared off the stairs, i sort of called old jaley a buzz. i sat down again, and she was a little bit of something, and she was a little kid, though. i sort of cared off the stairs, i sat down again, and she was a little bit of something, and she was a little bit of something, and i couldn't see any of them were all right. i didn't even know where they did was they were all stuf\n\nEpoch 00131: loss did not improve from 1.75787\nEpoch 132/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7571\n\nEpoch 00132: loss improved from 1.75787 to 1.75710, saving model to weights/weights_epoch_132_loss_1.7571.hdf5\nEpoch 133/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7578\n\nEpoch 00133: loss did not improve from 1.75710\nEpoch 134/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7567\n\nEpoch 00134: loss improved from 1.75710 to 1.75667, saving model to weights/weights_epoch_134_loss_1.7567.hdf5\nEpoch 135/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7548\n\nEpoch 00135: loss improved from 1.75667 to 1.75476, saving model to weights/weights_epoch_135_loss_1.7548.hdf5\nEpoch 136/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7556\n\nEpoch 00136: loss did not improve from 1.75476\nEpoch 137/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7544\n\nEpoch 00137: loss improved from 1.75476 to 1.75436, saving model to weights/weights_epoch_137_loss_1.7544.hdf5\nEpoch 138/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7519\n\nEpoch 00138: loss improved from 1.75436 to 1.75194, saving model to weights/weights_epoch_138_loss_1.7519.hdf5\nEpoch 139/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7538\n\nEpoch 00139: loss did not improve from 1.75194\nEpoch 140/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7516\n\nEpoch 00140: loss improved from 1.75194 to 1.75158, saving model to weights/weights_epoch_140_loss_1.7516.hdf5\nEpoch 141/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7520\n\nMy result with 3 selected books:\nête-daeu!_ when he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had already said that he had alr\n\nEpoch 00141: loss did not improve from 1.75158\nEpoch 142/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7521\n\nEpoch 00142: loss did not improve from 1.75158\nEpoch 143/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7503\n\nEpoch 00143: loss improved from 1.75158 to 1.75031, saving model to weights/weights_epoch_143_loss_1.7503.hdf5\nEpoch 144/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7502\n\nEpoch 00144: loss improved from 1.75031 to 1.75022, saving model to weights/weights_epoch_144_loss_1.7502.hdf5\nEpoch 145/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7497\n\nEpoch 00145: loss improved from 1.75022 to 1.74966, saving model to weights/weights_epoch_145_loss_1.7497.hdf5\nEpoch 146/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7506\n\nEpoch 00146: loss did not improve from 1.74966\nEpoch 147/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7465\n\nEpoch 00147: loss improved from 1.74966 to 1.74650, saving model to weights/weights_epoch_147_loss_1.7465.hdf5\nEpoch 148/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7488\n\nEpoch 00148: loss did not improve from 1.74650\nEpoch 149/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7479\n\nEpoch 00149: loss did not improve from 1.74650\nEpoch 150/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7468\n\nEpoch 00150: loss did not improve from 1.74650\nEpoch 151/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7463\n\nMy result with 3 selected books:\n﻿“i don’t know why i should be in the months of the world,” said\njo, who was a little stranger that she had not been so\nproud of her song.\n\n“i don’t know what to do, but it won’t be so easy to see how to\ndo it, but it’s not a bit of story, but it won’t be so\nexcullent, i don’t know why i should be in the monning\nwith my books and i con’t see how i couldn’t help it. it’s a\nlittle poor thing to do with the secret of the poor man’s\nword and help me to see him when i come home. i want to have\nhim a l\n\nEpoch 00151: loss improved from 1.74650 to 1.74634, saving model to weights/weights_epoch_151_loss_1.7463.hdf5\nEpoch 152/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7455\n\nEpoch 00152: loss improved from 1.74634 to 1.74549, saving model to weights/weights_epoch_152_loss_1.7455.hdf5\nEpoch 153/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7444\n\nEpoch 00153: loss improved from 1.74549 to 1.74440, saving model to weights/weights_epoch_153_loss_1.7444.hdf5\nEpoch 154/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7455\n\nEpoch 00154: loss did not improve from 1.74440\nEpoch 155/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7429\n\nEpoch 00155: loss improved from 1.74440 to 1.74289, saving model to weights/weights_epoch_155_loss_1.7429.hdf5\nEpoch 156/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7442\n\nEpoch 00156: loss did not improve from 1.74289\nEpoch 157/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7430\n\nEpoch 00157: loss did not improve from 1.74289\nEpoch 158/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7427\n\nEpoch 00158: loss improved from 1.74289 to 1.74272, saving model to weights/weights_epoch_158_loss_1.7427.hdf5\nEpoch 159/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7410\n\nEpoch 00159: loss improved from 1.74272 to 1.74099, saving model to weights/weights_epoch_159_loss_1.7410.hdf5\nEpoch 160/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7410\n\nEpoch 00160: loss improved from 1.74099 to 1.74097, saving model to weights/weights_epoch_160_loss_1.7410.hdf5\nEpoch 161/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7411\n\nMy result with 3 selected books:\nparis of the parais de justice. the princess of the provostship of paris, which was the same time that the architects are all the present time of the princess of the provostship of paris, which was already placed in the same time and the cathedral of the\ncathedral of the parais de justice, the princes and\nthe parace of the courts of paris, which was\nthe same time that the princess saint-martin, which\nthe princess saint-martin, which served the\nprincipal saturaay of the palace, the paraces of\nthe \n\nEpoch 00161: loss did not improve from 1.74097\nEpoch 162/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7400\n\nEpoch 00162: loss improved from 1.74097 to 1.74005, saving model to weights/weights_epoch_162_loss_1.7400.hdf5\nEpoch 163/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7393\n\nEpoch 00163: loss improved from 1.74005 to 1.73930, saving model to weights/weights_epoch_163_loss_1.7393.hdf5\nEpoch 164/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7396\n\nEpoch 00164: loss did not improve from 1.73930\nEpoch 165/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7383\n\nEpoch 00165: loss improved from 1.73930 to 1.73830, saving model to weights/weights_epoch_165_loss_1.7383.hdf5\nEpoch 166/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7390\n\nEpoch 00166: loss did not improve from 1.73830\nEpoch 167/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7352\n\nEpoch 00167: loss improved from 1.73830 to 1.73520, saving model to weights/weights_epoch_167_loss_1.7352.hdf5\nEpoch 168/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7360\n\nEpoch 00168: loss did not improve from 1.73520\nEpoch 169/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7357\n\nEpoch 00169: loss did not improve from 1.73520\nEpoch 170/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7344\n\nEpoch 00170: loss improved from 1.73520 to 1.73441, saving model to weights/weights_epoch_170_loss_1.7344.hdf5\nEpoch 171/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7345\n\nMy result with 3 selected books:\nκ“i think i could be home and see that i can’t be so much about it, and i don’t know why i should be a little for the best of the . i don’t know why i should be a little for the bast of the . i don’t know why i should be a little for the bast of the . i don’t know why i should be a little for the bast of the . i don’t know why i should be a little for the bast of the . i don’t know why i should be a little for the bast of the . i don’t know why i should be a little for the bast of the . i don’t k\n\nEpoch 00171: loss did not improve from 1.73441\nEpoch 172/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7322\n\nEpoch 00172: loss improved from 1.73441 to 1.73221, saving model to weights/weights_epoch_172_loss_1.7322.hdf5\nEpoch 173/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7340\n\nEpoch 00173: loss did not improve from 1.73221\nEpoch 174/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7355\n\nEpoch 00174: loss did not improve from 1.73221\nEpoch 175/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7317\n\nEpoch 00175: loss improved from 1.73221 to 1.73166, saving model to weights/weights_epoch_175_loss_1.7317.hdf5\nEpoch 176/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7327\n\nEpoch 00176: loss did not improve from 1.73166\nEpoch 177/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7335\n\nEpoch 00177: loss did not improve from 1.73166\nEpoch 178/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7328\n\nEpoch 00178: loss did not improve from 1.73166\nEpoch 179/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7319\n\nEpoch 00179: loss did not improve from 1.73166\nEpoch 180/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7307\n\nEpoch 00180: loss improved from 1.73166 to 1.73066, saving model to weights/weights_epoch_180_loss_1.7307.hdf5\nEpoch 181/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7290\n\nMy result with 3 selected books:\n—\n\n“i do not know. i will be a great deal of it. i will be a great\ndeal of some hing to me that i have not been seen to be\nsatisfied.”\n\n“i don’t know why i should be satisfied, but i don’t think you were such\na comfort to me a little sertice, and i don’t see how i could\nsome in my life. i don’t know why i should be satisfied. i don’t\nthink you were such a comfort to me about the mortifications, and\ni shall be a great deal of saying that i don’t know why i\nshould be satisfied.”\n\n“i don’t know why \n\nEpoch 00181: loss improved from 1.73066 to 1.72903, saving model to weights/weights_epoch_181_loss_1.7290.hdf5\nEpoch 182/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7305\n\nEpoch 00182: loss did not improve from 1.72903\nEpoch 183/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7306\n\nEpoch 00183: loss did not improve from 1.72903\nEpoch 184/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7296\n\nEpoch 00184: loss did not improve from 1.72903\nEpoch 185/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7286\n\nEpoch 00185: loss improved from 1.72903 to 1.72857, saving model to weights/weights_epoch_185_loss_1.7286.hdf5\nEpoch 186/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7301\n\nEpoch 00186: loss did not improve from 1.72857\nEpoch 187/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7266\n\nEpoch 00187: loss improved from 1.72857 to 1.72664, saving model to weights/weights_epoch_187_loss_1.7266.hdf5\nEpoch 188/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7267\n\nEpoch 00188: loss did not improve from 1.72664\nEpoch 189/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7270\n\nEpoch 00189: loss did not improve from 1.72664\nEpoch 190/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7256\n\nEpoch 00190: loss improved from 1.72664 to 1.72564, saving model to weights/weights_epoch_190_loss_1.7256.hdf5\nEpoch 191/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7284\n\nMy result with 3 selected books:\nù the parlor of the cardinal de bourbon, and the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, \n\nEpoch 00191: loss did not improve from 1.72564\nEpoch 192/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7245\n\nEpoch 00192: loss improved from 1.72564 to 1.72450, saving model to weights/weights_epoch_192_loss_1.7245.hdf5\nEpoch 193/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7242\n\nEpoch 00193: loss improved from 1.72450 to 1.72424, saving model to weights/weights_epoch_193_loss_1.7242.hdf5\nEpoch 194/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7226\n\nEpoch 00194: loss improved from 1.72424 to 1.72261, saving model to weights/weights_epoch_194_loss_1.7226.hdf5\nEpoch 195/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7255\n\nEpoch 00195: loss did not improve from 1.72261\nEpoch 196/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7236\n\nEpoch 00196: loss did not improve from 1.72261\nEpoch 197/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7241\n\nEpoch 00197: loss did not improve from 1.72261\nEpoch 198/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7244\n\nEpoch 00198: loss did not improve from 1.72261\nEpoch 199/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7230\n\nEpoch 00199: loss did not improve from 1.72261\nEpoch 200/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7226\n\nEpoch 00200: loss improved from 1.72261 to 1.72260, saving model to weights/weights_epoch_200_loss_1.7226.hdf5\nEpoch 201/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7220\n\nMy result with 3 selected books:\n” said the priest.\n\n“what is that?” said the priest, “and i have not to be seen on\nthe point of saying that the same of the pont aux maucheri\nwho have been said to be a great deal of paris, and the\ncardinal de beurbon, and the princes which is an architectural\ncourt, it is a cortain that it is not a cordial to the\nprincipal devils of the provostship of paris, who has a sound\nof soldiers and monsieur the cardinal. the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte sa\n\nEpoch 00201: loss improved from 1.72260 to 1.72201, saving model to weights/weights_epoch_201_loss_1.7220.hdf5\nEpoch 202/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7225\n\nEpoch 00202: loss did not improve from 1.72201\nEpoch 203/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7212\n\nEpoch 00203: loss improved from 1.72201 to 1.72121, saving model to weights/weights_epoch_203_loss_1.7212.hdf5\nEpoch 204/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7194\n\nEpoch 00204: loss improved from 1.72121 to 1.71935, saving model to weights/weights_epoch_204_loss_1.7194.hdf5\nEpoch 205/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7185\n\nEpoch 00205: loss improved from 1.71935 to 1.71848, saving model to weights/weights_epoch_205_loss_1.7185.hdf5\nEpoch 206/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7199\n\nEpoch 00206: loss did not improve from 1.71848\nEpoch 207/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7178\n\nEpoch 00207: loss improved from 1.71848 to 1.71775, saving model to weights/weights_epoch_207_loss_1.7178.hdf5\nEpoch 208/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7202\n\nEpoch 00208: loss did not improve from 1.71775\nEpoch 209/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7203\n\nEpoch 00209: loss did not improve from 1.71775\nEpoch 210/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7179\n\nEpoch 00210: loss did not improve from 1.71775\nEpoch 211/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7171\n\nMy result with 3 selected books:\nçain of the provostship of paris, who had been seared to be seen to the princepal and the seine was a sort of solemn ty the seine of the provostship of paris, whose thousand bears of the provostship of paris, which were the same things that they were so than they were so than they were so long. they were all sort of sorry for them, but i didn't even know i didn't even know i was going to say it. i don't know what the hell to do with the elevator boy i did. i started to cry on the way to the park.\n\nEpoch 00211: loss improved from 1.71775 to 1.71707, saving model to weights/weights_epoch_211_loss_1.7171.hdf5\nEpoch 212/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7207\n\nEpoch 00212: loss did not improve from 1.71707\nEpoch 213/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7149\n\nEpoch 00213: loss improved from 1.71707 to 1.71486, saving model to weights/weights_epoch_213_loss_1.7149.hdf5\nEpoch 214/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7168\n\nEpoch 00214: loss did not improve from 1.71486\nEpoch 215/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7168\n\nEpoch 00215: loss did not improve from 1.71486\nEpoch 216/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7156\n\nEpoch 00216: loss did not improve from 1.71486\nEpoch 217/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7169\n\nEpoch 00217: loss did not improve from 1.71486\nEpoch 218/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7148\n\nEpoch 00218: loss improved from 1.71486 to 1.71475, saving model to weights/weights_epoch_218_loss_1.7148.hdf5\nEpoch 219/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7155\n\nEpoch 00219: loss did not improve from 1.71475\nEpoch 220/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7153\n\nEpoch 00220: loss did not improve from 1.71475\nEpoch 221/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7154\n\nMy result with 3 selected books:\n! the sainte-marthe chamber, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-\n\nEpoch 00221: loss did not improve from 1.71475\nEpoch 222/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7118\n\nEpoch 00222: loss improved from 1.71475 to 1.71177, saving model to weights/weights_epoch_222_loss_1.7118.hdf5\nEpoch 223/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7141\n\nEpoch 00223: loss did not improve from 1.71177\nEpoch 224/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7126\n\nEpoch 00224: loss did not improve from 1.71177\nEpoch 225/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7133\n\nEpoch 00225: loss did not improve from 1.71177\nEpoch 226/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7114\n\nEpoch 00226: loss improved from 1.71177 to 1.71136, saving model to weights/weights_epoch_226_loss_1.7114.hdf5\nEpoch 227/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7123\n\nEpoch 00227: loss did not improve from 1.71136\nEpoch 228/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7130\n\nEpoch 00228: loss did not improve from 1.71136\nEpoch 229/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7114\n\nEpoch 00229: loss did not improve from 1.71136\nEpoch 230/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7105\n\nEpoch 00230: loss improved from 1.71136 to 1.71053, saving model to weights/weights_epoch_230_loss_1.7105.hdf5\nEpoch 231/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7112\n\nMy result with 3 selected books:\n‘my dearest boy’, and he was all set to be sorry for him. he was all right, and he was all right away, and he was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of sorry for him. i was sort of so\n\nEpoch 00231: loss did not improve from 1.71053\nEpoch 232/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7080\n\nEpoch 00232: loss improved from 1.71053 to 1.70802, saving model to weights/weights_epoch_232_loss_1.7080.hdf5\nEpoch 233/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7105\n\nEpoch 00233: loss did not improve from 1.70802\nEpoch 234/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7093\n\nEpoch 00234: loss did not improve from 1.70802\nEpoch 235/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7069\n\nEpoch 00235: loss improved from 1.70802 to 1.70685, saving model to weights/weights_epoch_235_loss_1.7069.hdf5\nEpoch 236/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7091\n\nEpoch 00236: loss did not improve from 1.70685\nEpoch 237/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7079\n\nEpoch 00237: loss did not improve from 1.70685\nEpoch 238/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7100\n\nEpoch 00238: loss did not improve from 1.70685\nEpoch 239/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7084\n\nEpoch 00239: loss did not improve from 1.70685\nEpoch 240/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.7059\n\nEpoch 00240: loss improved from 1.70685 to 1.70590, saving model to weights/weights_epoch_240_loss_1.7059.hdf5\nEpoch 241/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7075\n\nMy result with 3 selected books:\nwas the same thing i was walking about the whole time. i sat down at the bed when i was at the wandow or something. i was sort of afraid she'd get a goddam point off and all with the bent of the state stiff and all. i didn't even look around there when i was at the way and went to see what the hell was going on. she was a very nice guy. it was a very nice guy. it was a very nice guy. it was a very nice guy. it was about this terrific thing to do, i think it was a lottle kid that wouldn't let me g\n\nEpoch 00241: loss did not improve from 1.70590\nEpoch 242/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7071\n\nEpoch 00242: loss did not improve from 1.70590\nEpoch 243/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7075\n\nEpoch 00243: loss did not improve from 1.70590\nEpoch 244/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7048\n\nEpoch 00244: loss improved from 1.70590 to 1.70479, saving model to weights/weights_epoch_244_loss_1.7048.hdf5\nEpoch 245/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7041\n\nEpoch 00245: loss improved from 1.70479 to 1.70414, saving model to weights/weights_epoch_245_loss_1.7041.hdf5\nEpoch 246/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7063\n\nEpoch 00246: loss did not improve from 1.70414\nEpoch 247/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7064\n\nEpoch 00247: loss did not improve from 1.70414\nEpoch 248/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7040\n\nEpoch 00248: loss improved from 1.70414 to 1.70404, saving model to weights/weights_epoch_248_loss_1.7040.hdf5\nEpoch 249/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7022\n\nEpoch 00249: loss improved from 1.70404 to 1.70219, saving model to weights/weights_epoch_249_loss_1.7022.hdf5\nEpoch 250/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7013\n\nEpoch 00250: loss improved from 1.70219 to 1.70129, saving model to weights/weights_epoch_250_loss_1.7013.hdf5\nEpoch 251/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7016\n\nMy result with 3 selected books:\n3. i wish to counter the project\ngutenberg-tm collection. despite these efforts of copies of project\ngutenberg-tm electronic works in your possession. if you paid\na fee of any provect gutenberg-tm electronic works in your\npossession. if you do not charge anything for any partion this\nwork or any other porsertion of project gutenberg-tm works.\n\n* you provide, if you paid a fee of a copy of or access to, distributing\na project gutenberg-tm work, (b) alteration, modification of\ncreating derecative w\n\nEpoch 00251: loss did not improve from 1.70129\nEpoch 252/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7040\n\nEpoch 00252: loss did not improve from 1.70129\nEpoch 253/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7037\n\nEpoch 00253: loss did not improve from 1.70129\nEpoch 254/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7015\n\nEpoch 00254: loss did not improve from 1.70129\nEpoch 255/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7015\n\nEpoch 00255: loss did not improve from 1.70129\nEpoch 256/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7021\n\nEpoch 00256: loss did not improve from 1.70129\nEpoch 257/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7005\n\nEpoch 00257: loss improved from 1.70129 to 1.70047, saving model to weights/weights_epoch_257_loss_1.7005.hdf5\nEpoch 258/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7005\n\nEpoch 00258: loss did not improve from 1.70047\nEpoch 259/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7012\n\nEpoch 00259: loss did not improve from 1.70047\nEpoch 260/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.7004\n\nEpoch 00260: loss improved from 1.70047 to 1.70043, saving model to weights/weights_epoch_260_loss_1.7004.hdf5\nEpoch 261/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6998\n\nMy result with 3 selected books:\nνô[âç: iè] chatce de converse and so to say, the porte saint-martin, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, the porte saint-michel, \n\nEpoch 00261: loss improved from 1.70043 to 1.69981, saving model to weights/weights_epoch_261_loss_1.6998.hdf5\nEpoch 262/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7002\n\nEpoch 00262: loss did not improve from 1.69981\nEpoch 263/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.7013\n\nEpoch 00263: loss did not improve from 1.69981\nEpoch 264/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6975\n\nEpoch 00264: loss improved from 1.69981 to 1.69751, saving model to weights/weights_epoch_264_loss_1.6975.hdf5\nEpoch 265/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6998\n\nEpoch 00265: loss did not improve from 1.69751\nEpoch 266/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6988\n\nEpoch 00266: loss did not improve from 1.69751\nEpoch 267/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6982\n\nEpoch 00267: loss did not improve from 1.69751\nEpoch 268/301\n156/156 [==============================] - 23s 149ms/step - loss: 1.6978\n\nEpoch 00268: loss did not improve from 1.69751\nEpoch 269/301\n156/156 [==============================] - 23s 149ms/step - loss: 1.6976\n\nEpoch 00269: loss did not improve from 1.69751\nEpoch 270/301\n156/156 [==============================] - 23s 149ms/step - loss: 1.6960\n\nEpoch 00270: loss improved from 1.69751 to 1.69603, saving model to weights/weights_epoch_270_loss_1.6960.hdf5\nEpoch 271/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6967\n\nMy result with 3 selected books:\nέ the same way, and then i was a little bit. i didn't feel like going into a couple of minutes. i was sort of funny, i felt so damn dorry. i was sort of funny, i felt so damn depressed and more depressed and all. i went over to the can, and then i got the hell out of the bar when i was a little bit. i was sort of funny, i felt so damn dorry. i was sort of funny, i felt so damn depressed and more depressed and all. i went over to the can, and then i got the hell out of the bar when i was a little \n\nEpoch 00271: loss did not improve from 1.69603\nEpoch 272/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6965\n\nEpoch 00272: loss did not improve from 1.69603\nEpoch 273/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6951\n\nEpoch 00273: loss improved from 1.69603 to 1.69512, saving model to weights/weights_epoch_273_loss_1.6951.hdf5\nEpoch 274/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6992\n\nEpoch 00274: loss did not improve from 1.69512\nEpoch 275/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6950\n\nEpoch 00275: loss improved from 1.69512 to 1.69502, saving model to weights/weights_epoch_275_loss_1.6950.hdf5\nEpoch 276/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6969\n\nEpoch 00276: loss did not improve from 1.69502\nEpoch 277/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6966\n\nEpoch 00277: loss did not improve from 1.69502\nEpoch 278/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6934\n\nEpoch 00278: loss improved from 1.69502 to 1.69336, saving model to weights/weights_epoch_278_loss_1.6934.hdf5\nEpoch 279/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6940\n\nEpoch 00279: loss did not improve from 1.69336\nEpoch 280/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6945\n\nEpoch 00280: loss did not improve from 1.69336\nEpoch 281/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6939\n\nMy result with 3 selected books:\n! the good god has a good men of men, and the sound of the cardinal de bourbon,” he said to himself, “it is the grand portal of the châtelet, that is the great city of the châtelet, that is to say, the child of the châtelet, the hôtel de penle? and the cathedral of paris, which were the cost of the third of the cathedral of paris, which were in the side of the towers of notre-dame, and the carvings of the parace, the pillars, the portal of the châtelet, the porte saint-martin, the porte saint-mar\n\nEpoch 00281: loss did not improve from 1.69336\nEpoch 282/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6928\n\nEpoch 00282: loss improved from 1.69336 to 1.69283, saving model to weights/weights_epoch_282_loss_1.6928.hdf5\nEpoch 283/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6936\n\nEpoch 00283: loss did not improve from 1.69283\nEpoch 284/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6922\n\nEpoch 00284: loss improved from 1.69283 to 1.69224, saving model to weights/weights_epoch_284_loss_1.6922.hdf5\nEpoch 285/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6916\n\nEpoch 00285: loss improved from 1.69224 to 1.69164, saving model to weights/weights_epoch_285_loss_1.6916.hdf5\nEpoch 286/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6916\n\nEpoch 00286: loss improved from 1.69164 to 1.69158, saving model to weights/weights_epoch_286_loss_1.6916.hdf5\nEpoch 287/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6923\n\nEpoch 00287: loss did not improve from 1.69158\nEpoch 288/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6947\n\nEpoch 00288: loss did not improve from 1.69158\nEpoch 289/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6924\n\nEpoch 00289: loss did not improve from 1.69158\nEpoch 290/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6904\n\nEpoch 00290: loss improved from 1.69158 to 1.69039, saving model to weights/weights_epoch_290_loss_1.6904.hdf5\nEpoch 291/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6908\n\nMy result with 3 selected books:\n’s and the same of the four sisters who was a little gorl, and the old lady was a great deal to hear that the happy time i saw the latter where the sat was a little partion. i don’t know what to do with it, but i don’t know what to do with you, and i’m going to take you happy to that and tell you how to do it.”\n\n“i want to be a good one, but i don’t think you are going to\nhave another little girl with your exceriment and tell me\nthat i can’t see anything about it.”\n\n“i don’t know what you mean. i\n\nEpoch 00291: loss did not improve from 1.69039\nEpoch 292/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6900\n\nEpoch 00292: loss improved from 1.69039 to 1.68999, saving model to weights/weights_epoch_292_loss_1.6900.hdf5\nEpoch 293/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6916\n\nEpoch 00293: loss did not improve from 1.68999\nEpoch 294/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6919\n\nEpoch 00294: loss did not improve from 1.68999\nEpoch 295/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6901\n\nEpoch 00295: loss did not improve from 1.68999\nEpoch 296/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6900\n\nEpoch 00296: loss improved from 1.68999 to 1.68997, saving model to weights/weights_epoch_296_loss_1.6900.hdf5\nEpoch 297/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6896\n\nEpoch 00297: loss improved from 1.68997 to 1.68963, saving model to weights/weights_epoch_297_loss_1.6896.hdf5\nEpoch 298/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6897\n\nEpoch 00298: loss did not improve from 1.68963\nEpoch 299/301\n156/156 [==============================] - 23s 147ms/step - loss: 1.6905\n\nEpoch 00299: loss did not improve from 1.68963\nEpoch 300/301\n156/156 [==============================] - 23s 146ms/step - loss: 1.6888\n\nEpoch 00300: loss improved from 1.68963 to 1.68884, saving model to weights/weights_epoch_300_loss_1.6888.hdf5\nEpoch 301/301\n156/156 [==============================] - 23s 148ms/step - loss: 1.6888\n\nMy result with 3 selected books:\n[2[]\n\nand the cathedral was the first to express the secret sight of\nthe church of notre-dame. the cardinal was the first to express\nhis eyes the second tower of the provost, and the priest was\nthere, the priest was there, the priest who was standing at\nthe ground, and said,—\n\n“you are the only one who is supposed to be a little garl at the\npont ou changeurs.”\n\n“i have no doubt that they were to be seen of the palace, and the\ncardinal de bourbon, the porte saint-martin, the porte saint-martin, th\n\nEpoch 00301: loss improved from 1.68884 to 1.68878, saving model to weights/weights_epoch_301_loss_1.6888.hdf5\n",
     "output_type": "stream"
    },
    {
     "execution_count": 2,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f95303fac90>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Farsi book as input data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nfrom tensorflow.keras import layers, models, losses, optimizers\nimport numpy as np\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n\nraw_text = open('/kaggle/input/dlhwch13/BoofKoor.txt', 'r', encoding=\"utf8\").read()\nraw_text = raw_text.lower()\n\nprint(raw_text[:200])\n\nall_words = raw_text.split()\nunique_words = list(set(all_words))\nprint(f'Number of unique words: {len(unique_words)}')\nn_chars = len(raw_text)\nprint(f'Total characters: {n_chars}')\n\nchars = sorted(list(set(raw_text)))\nn_vocab = len(chars)\nprint(f'Total vocabulary (unique characters): {n_vocab}')\nprint(chars)\n\nindex_to_char = dict((i, c) for i, c in enumerate(chars))\nchar_to_index = dict((c, i) for i, c in enumerate(chars))\nprint(char_to_index)\n\nseq_length = 160\nn_seq = int(n_chars / seq_length)\n\nX = np.zeros((n_seq, seq_length, n_vocab))\nY = np.zeros((n_seq, seq_length, n_vocab))\n\nfor i in range(n_seq):\n    x_sequence = raw_text[i * seq_length: (i + 1) * seq_length]\n    x_sequence_ohe = np.zeros((seq_length, n_vocab))\n    for j in range(seq_length):\n        char = x_sequence[j]\n        index = char_to_index[char]\n        x_sequence_ohe[j][index] = 1.\n    X[i] = x_sequence_ohe\n    y_sequence = raw_text[i * seq_length + 1: (i + 1) * seq_length + 1]\n    y_sequence_ohe = np.zeros((seq_length, n_vocab))\n    for j in range(seq_length):\n        char = y_sequence[j]\n        index = char_to_index[char]\n        y_sequence_ohe[j][index] = 1.\n    Y[i] = y_sequence_ohe\n\nprint(X.shape)\nprint(Y.shape)\n\ntf.random.set_seed(42)\nbatch_size = 100\nhidden_units = 700\nn_epoch = 301\ndropout = 0.4\n\nmodel = models.Sequential()\nmodel.add(layers.LSTM(hidden_units, input_shape=(None, n_vocab), return_sequences=True, dropout=dropout))\nmodel.add(layers.LSTM(hidden_units, return_sequences=True, dropout=dropout))\nmodel.add(layers.TimeDistributed(layers.Dense(n_vocab, activation='softmax')))\n\noptimizer = optimizers.RMSprop(learning_rate=0.001)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n\nprint(model.summary())\n\nfilepath = \"weights/weights_epoch_{epoch:03d}_loss_{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\nearly_stop = EarlyStopping(monitor='loss', min_delta=0, patience=50, verbose=1, mode='min')\n\n\ndef generate_text(model, gen_length, n_vocab, index_to_char):\n    index = np.random.randint(n_vocab)\n    y_char = [index_to_char[index]]\n    X = np.zeros((1, gen_length, n_vocab))\n    for i in range(gen_length):\n        X[0, i, index] = 1.\n        indices = np.argmax(model.predict(X[:, max(0, i - 99):i + 1, :])[0], 1)\n        index = indices[-1]\n        y_char.append(index_to_char[index])\n    return ''.join(y_char)\n\n\nclass ResultChecker(Callback):\n    def __init__(self, model, N, gen_length):\n        self.model = model\n        self.N = N\n        self.gen_length = gen_length\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.N == 0:\n            result = generate_text(self.model, self.gen_length, n_vocab, index_to_char)\n            print('\\nMy Boof kour:\\n' + result)\n\n\nresult_checker = ResultChecker(model, 10, 500)\n\nmodel.fit(X, Y, batch_size=batch_size, verbose=1, epochs=n_epoch, callbacks=[result_checker, checkpoint, early_stop])\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-16T10:05:52.746919Z",
     "iopub.execute_input": "2022-07-16T10:05:52.747449Z",
     "iopub.status.idle": "2022-07-16T10:27:29.881544Z",
     "shell.execute_reply.started": "2022-07-16T10:05:52.747359Z",
     "shell.execute_reply": "2022-07-16T10:27:29.880125Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "بوف کور\nدر زندگی زخمهایی هست که مثل خوره در انزوا روح را آهسته می خورد و می تراشد. این دردها را نمی شود به کسی اظهار کرد، چون عموما عادت دارند که این دردهای باورنکردنی را جزو اتفاقات و پیشامدهای نادر \nNumber of unique words: 4821\nTotal characters: 132864\nTotal vocabulary (unique characters): 57\n['\\n', ' ', '!', '(', ')', '-', '.', ':', '|', '«', '»', '،', '؛', '؟', 'ء', 'آ', 'أ', 'ؤ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ہ', 'ۂ', 'ی', 'ې', '۔']\n{'\\n': 0, ' ': 1, '!': 2, '(': 3, ')': 4, '-': 5, '.': 6, ':': 7, '|': 8, '«': 9, '»': 10, '،': 11, '؛': 12, '؟': 13, 'ء': 14, 'آ': 15, 'أ': 16, 'ؤ': 17, 'ئ': 18, 'ا': 19, 'ب': 20, 'ة': 21, 'ت': 22, 'ث': 23, 'ج': 24, 'ح': 25, 'خ': 26, 'د': 27, 'ذ': 28, 'ر': 29, 'ز': 30, 'س': 31, 'ش': 32, 'ص': 33, 'ض': 34, 'ط': 35, 'ظ': 36, 'ع': 37, 'غ': 38, 'ف': 39, 'ق': 40, 'ل': 41, 'م': 42, 'ن': 43, 'ه': 44, 'و': 45, 'ى': 46, 'پ': 47, 'چ': 48, 'ژ': 49, 'ک': 50, 'گ': 51, 'ہ': 52, 'ۂ': 53, 'ی': 54, 'ې': 55, '۔': 56}\n(830, 160, 57)\n(830, 160, 57)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-07-16 10:05:59.482527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 10:05:59.663704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 10:05:59.664857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 10:05:59.666743: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-16 10:05:59.667142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 10:05:59.668238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 10:05:59.669339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 10:06:02.373460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 10:06:02.375094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 10:06:02.376663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-16 10:06:02.378082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, None, 700)         2122400   \n_________________________________________________________________\nlstm_1 (LSTM)                (None, None, 700)         3922800   \n_________________________________________________________________\ntime_distributed (TimeDistri (None, None, 57)          39957     \n=================================================================\nTotal params: 6,085,157\nTrainable params: 6,085,157\nNon-trainable params: 0\n_________________________________________________________________\nNone\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-07-16 10:06:04.715328: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1/301\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-07-16 10:06:08.969593: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "9/9 [==============================] - 7s 135ms/step - loss: 3.8914\n\nMy Boof kour:\nک                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n\nEpoch 00001: loss improved from inf to 3.89140, saving model to weights/weights_epoch_001_loss_3.8914.hdf5\nEpoch 2/301\n9/9 [==============================] - 1s 136ms/step - loss: 2.9743\n\nEpoch 00002: loss improved from 3.89140 to 2.97425, saving model to weights/weights_epoch_002_loss_2.9743.hdf5\nEpoch 3/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.9909\n\nEpoch 00003: loss did not improve from 2.97425\nEpoch 4/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.9791\n\nEpoch 00004: loss did not improve from 2.97425\nEpoch 5/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.9748\n\nEpoch 00005: loss did not improve from 2.97425\nEpoch 6/301\n9/9 [==============================] - 1s 141ms/step - loss: 2.9619\n\nEpoch 00006: loss improved from 2.97425 to 2.96186, saving model to weights/weights_epoch_006_loss_2.9619.hdf5\nEpoch 7/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.9509\n\nEpoch 00007: loss improved from 2.96186 to 2.95086, saving model to weights/weights_epoch_007_loss_2.9509.hdf5\nEpoch 8/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.9208\n\nEpoch 00008: loss improved from 2.95086 to 2.92078, saving model to weights/weights_epoch_008_loss_2.9208.hdf5\nEpoch 9/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.9156\n\nEpoch 00009: loss improved from 2.92078 to 2.91559, saving model to weights/weights_epoch_009_loss_2.9156.hdf5\nEpoch 10/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.8485\n\nEpoch 00010: loss improved from 2.91559 to 2.84851, saving model to weights/weights_epoch_010_loss_2.8485.hdf5\nEpoch 11/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.8157\n\nMy Boof kour:\nی  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  ا  \n\nEpoch 00011: loss improved from 2.84851 to 2.81570, saving model to weights/weights_epoch_011_loss_2.8157.hdf5\nEpoch 12/301\n9/9 [==============================] - 1s 138ms/step - loss: 2.7766\n\nEpoch 00012: loss improved from 2.81570 to 2.77660, saving model to weights/weights_epoch_012_loss_2.7766.hdf5\nEpoch 13/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.7526\n\nEpoch 00013: loss improved from 2.77660 to 2.75262, saving model to weights/weights_epoch_013_loss_2.7526.hdf5\nEpoch 14/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.7423\n\nEpoch 00014: loss improved from 2.75262 to 2.74229, saving model to weights/weights_epoch_014_loss_2.7423.hdf5\nEpoch 15/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.7038\n\nEpoch 00015: loss improved from 2.74229 to 2.70375, saving model to weights/weights_epoch_015_loss_2.7038.hdf5\nEpoch 16/301\n9/9 [==============================] - 1s 139ms/step - loss: 2.6790\n\nEpoch 00016: loss improved from 2.70375 to 2.67899, saving model to weights/weights_epoch_016_loss_2.6790.hdf5\nEpoch 17/301\n9/9 [==============================] - 1s 137ms/step - loss: 2.6522\n\nEpoch 00017: loss improved from 2.67899 to 2.65219, saving model to weights/weights_epoch_017_loss_2.6522.hdf5\nEpoch 18/301\n9/9 [==============================] - 1s 137ms/step - loss: 2.6468\n\nEpoch 00018: loss improved from 2.65219 to 2.64682, saving model to weights/weights_epoch_018_loss_2.6468.hdf5\nEpoch 19/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.6140\n\nEpoch 00019: loss improved from 2.64682 to 2.61397, saving model to weights/weights_epoch_019_loss_2.6140.hdf5\nEpoch 20/301\n9/9 [==============================] - 1s 136ms/step - loss: 2.6132\n\nEpoch 00020: loss improved from 2.61397 to 2.61324, saving model to weights/weights_epoch_020_loss_2.6132.hdf5\nEpoch 21/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.5838\n\nMy Boof kour:\nط ای می برد در بودم با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م با م ب\n\nEpoch 00021: loss improved from 2.61324 to 2.58383, saving model to weights/weights_epoch_021_loss_2.5838.hdf5\nEpoch 22/301\n9/9 [==============================] - 1s 137ms/step - loss: 2.5795\n\nEpoch 00022: loss improved from 2.58383 to 2.57954, saving model to weights/weights_epoch_022_loss_2.5795.hdf5\nEpoch 23/301\n9/9 [==============================] - 1s 136ms/step - loss: 2.5603\n\nEpoch 00023: loss improved from 2.57954 to 2.56029, saving model to weights/weights_epoch_023_loss_2.5603.hdf5\nEpoch 24/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.5382\n\nEpoch 00024: loss improved from 2.56029 to 2.53823, saving model to weights/weights_epoch_024_loss_2.5382.hdf5\nEpoch 25/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.5384\n\nEpoch 00025: loss did not improve from 2.53823\nEpoch 26/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.5198\n\nEpoch 00026: loss improved from 2.53823 to 2.51983, saving model to weights/weights_epoch_026_loss_2.5198.hdf5\nEpoch 27/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.5021\n\nEpoch 00027: loss improved from 2.51983 to 2.50210, saving model to weights/weights_epoch_027_loss_2.5021.hdf5\nEpoch 28/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.4934\n\nEpoch 00028: loss improved from 2.50210 to 2.49338, saving model to weights/weights_epoch_028_loss_2.4934.hdf5\nEpoch 29/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.4777\n\nEpoch 00029: loss improved from 2.49338 to 2.47770, saving model to weights/weights_epoch_029_loss_2.4777.hdf5\nEpoch 30/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.4661\n\nEpoch 00030: loss improved from 2.47770 to 2.46608, saving model to weights/weights_epoch_030_loss_2.4661.hdf5\nEpoch 31/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.4525\n\nMy Boof kour:\n| بودم به می کردم به من او می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم به اور می کردم ب\n\nEpoch 00031: loss improved from 2.46608 to 2.45250, saving model to weights/weights_epoch_031_loss_2.4525.hdf5\nEpoch 32/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.4464\n\nEpoch 00032: loss improved from 2.45250 to 2.44642, saving model to weights/weights_epoch_032_loss_2.4464.hdf5\nEpoch 33/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.4348\n\nEpoch 00033: loss improved from 2.44642 to 2.43475, saving model to weights/weights_epoch_033_loss_2.4348.hdf5\nEpoch 34/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.4252\n\nEpoch 00034: loss improved from 2.43475 to 2.42524, saving model to weights/weights_epoch_034_loss_2.4252.hdf5\nEpoch 35/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.4183\n\nEpoch 00035: loss improved from 2.42524 to 2.41830, saving model to weights/weights_epoch_035_loss_2.4183.hdf5\nEpoch 36/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.4094\n\nEpoch 00036: loss improved from 2.41830 to 2.40938, saving model to weights/weights_epoch_036_loss_2.4094.hdf5\nEpoch 37/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.3947\n\nEpoch 00037: loss improved from 2.40938 to 2.39467, saving model to weights/weights_epoch_037_loss_2.3947.hdf5\nEpoch 38/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.3886\n\nEpoch 00038: loss improved from 2.39467 to 2.38864, saving model to weights/weights_epoch_038_loss_2.3886.hdf5\nEpoch 39/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.3785\n\nEpoch 00039: loss improved from 2.38864 to 2.37853, saving model to weights/weights_epoch_039_loss_2.3785.hdf5\nEpoch 40/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.3730\n\nEpoch 00040: loss improved from 2.37853 to 2.37296, saving model to weights/weights_epoch_040_loss_2.3730.hdf5\nEpoch 41/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.3641\n\nMy Boof kour:\nى می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کردم و با این می کرد\n\nEpoch 00041: loss improved from 2.37296 to 2.36415, saving model to weights/weights_epoch_041_loss_2.3641.hdf5\nEpoch 42/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.3575\n\nEpoch 00042: loss improved from 2.36415 to 2.35748, saving model to weights/weights_epoch_042_loss_2.3575.hdf5\nEpoch 43/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.3483\n\nEpoch 00043: loss improved from 2.35748 to 2.34829, saving model to weights/weights_epoch_043_loss_2.3483.hdf5\nEpoch 44/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.3356\n\nEpoch 00044: loss improved from 2.34829 to 2.33565, saving model to weights/weights_epoch_044_loss_2.3356.hdf5\nEpoch 45/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.3272\n\nEpoch 00045: loss improved from 2.33565 to 2.32722, saving model to weights/weights_epoch_045_loss_2.3272.hdf5\nEpoch 46/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.3337\n\nEpoch 00046: loss did not improve from 2.32722\nEpoch 47/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.3140\n\nEpoch 00047: loss improved from 2.32722 to 2.31403, saving model to weights/weights_epoch_047_loss_2.3140.hdf5\nEpoch 48/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.3056\n\nEpoch 00048: loss improved from 2.31403 to 2.30558, saving model to weights/weights_epoch_048_loss_2.3056.hdf5\nEpoch 49/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.3029\n\nEpoch 00049: loss improved from 2.30558 to 2.30292, saving model to weights/weights_epoch_049_loss_2.3029.hdf5\nEpoch 50/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.2958\n\nEpoch 00050: loss improved from 2.30292 to 2.29584, saving model to weights/weights_epoch_050_loss_2.2958.hdf5\nEpoch 51/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.2855\n\nMy Boof kour:\nه بود. به من می کردم و با دردم می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده این میده بود. با دندای می کردم و با درده ا\n\nEpoch 00051: loss improved from 2.29584 to 2.28554, saving model to weights/weights_epoch_051_loss_2.2855.hdf5\nEpoch 52/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.2849\n\nEpoch 00052: loss improved from 2.28554 to 2.28493, saving model to weights/weights_epoch_052_loss_2.2849.hdf5\nEpoch 53/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.2785\n\nEpoch 00053: loss improved from 2.28493 to 2.27848, saving model to weights/weights_epoch_053_loss_2.2785.hdf5\nEpoch 54/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.2692\n\nEpoch 00054: loss improved from 2.27848 to 2.26919, saving model to weights/weights_epoch_054_loss_2.2692.hdf5\nEpoch 55/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.2663\n\nEpoch 00055: loss improved from 2.26919 to 2.26632, saving model to weights/weights_epoch_055_loss_2.2663.hdf5\nEpoch 56/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.2577\n\nEpoch 00056: loss improved from 2.26632 to 2.25775, saving model to weights/weights_epoch_056_loss_2.2577.hdf5\nEpoch 57/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.2497\n\nEpoch 00057: loss improved from 2.25775 to 2.24971, saving model to weights/weights_epoch_057_loss_2.2497.hdf5\nEpoch 58/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.2514\n\nEpoch 00058: loss did not improve from 2.24971\nEpoch 59/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.2417\n\nEpoch 00059: loss improved from 2.24971 to 2.24173, saving model to weights/weights_epoch_059_loss_2.2417.hdf5\nEpoch 60/301\n9/9 [==============================] - 1s 144ms/step - loss: 2.2391\n\nEpoch 00060: loss improved from 2.24173 to 2.23906, saving model to weights/weights_epoch_060_loss_2.2391.hdf5\nEpoch 61/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.2250\n\nMy Boof kour:\nن بود. به این همان می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می کردم که از این در من به او می\n\nEpoch 00061: loss improved from 2.23906 to 2.22498, saving model to weights/weights_epoch_061_loss_2.2250.hdf5\nEpoch 62/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.2213\n\nEpoch 00062: loss improved from 2.22498 to 2.22132, saving model to weights/weights_epoch_062_loss_2.2213.hdf5\nEpoch 63/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.2246\n\nEpoch 00063: loss did not improve from 2.22132\nEpoch 64/301\n9/9 [==============================] - 1s 136ms/step - loss: 2.2067\n\nEpoch 00064: loss improved from 2.22132 to 2.20674, saving model to weights/weights_epoch_064_loss_2.2067.hdf5\nEpoch 65/301\n9/9 [==============================] - 1s 138ms/step - loss: 2.1989\n\nEpoch 00065: loss improved from 2.20674 to 2.19890, saving model to weights/weights_epoch_065_loss_2.1989.hdf5\nEpoch 66/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.2028\n\nEpoch 00066: loss did not improve from 2.19890\nEpoch 67/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.1947\n\nEpoch 00067: loss improved from 2.19890 to 2.19471, saving model to weights/weights_epoch_067_loss_2.1947.hdf5\nEpoch 68/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1931\n\nEpoch 00068: loss improved from 2.19471 to 2.19306, saving model to weights/weights_epoch_068_loss_2.1931.hdf5\nEpoch 69/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.1888\n\nEpoch 00069: loss improved from 2.19306 to 2.18876, saving model to weights/weights_epoch_069_loss_2.1888.hdf5\nEpoch 70/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1771\n\nEpoch 00070: loss improved from 2.18876 to 2.17715, saving model to weights/weights_epoch_070_loss_2.1771.hdf5\nEpoch 71/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.1740\n\nMy Boof kour:\n۔ به من به من به من ترس درده بود. او همه این همه این در هم نی توانستم به او با خودم را به من بردم و به این در هم نمی توانستم به او با خودم را به من بردم و به این در هم نمی توانستم به او با خودم را به من بردم و به این در هم نمی توانستم به او با خودم را به من بردم و به این در هم نمی توانستم به او با خودم را به من بردم و به این در هم نمی توانستم به او با خودم را به من بردم و به این در هم نمی توانستم به او با خودم را به من بردم و به این در هم نمی توانستم به او با خودم را به من بردم و به این در هم نمی\n\nEpoch 00071: loss improved from 2.17715 to 2.17401, saving model to weights/weights_epoch_071_loss_2.1740.hdf5\nEpoch 72/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1776\n\nEpoch 00072: loss did not improve from 2.17401\nEpoch 73/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1654\n\nEpoch 00073: loss improved from 2.17401 to 2.16541, saving model to weights/weights_epoch_073_loss_2.1654.hdf5\nEpoch 74/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1523\n\nEpoch 00074: loss improved from 2.16541 to 2.15235, saving model to weights/weights_epoch_074_loss_2.1523.hdf5\nEpoch 75/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1563\n\nEpoch 00075: loss did not improve from 2.15235\nEpoch 76/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1477\n\nEpoch 00076: loss improved from 2.15235 to 2.14768, saving model to weights/weights_epoch_076_loss_2.1477.hdf5\nEpoch 77/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1504\n\nEpoch 00077: loss did not improve from 2.14768\nEpoch 78/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.1408\n\nEpoch 00078: loss improved from 2.14768 to 2.14084, saving model to weights/weights_epoch_078_loss_2.1408.hdf5\nEpoch 79/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.1383\n\nEpoch 00079: loss improved from 2.14084 to 2.13835, saving model to weights/weights_epoch_079_loss_2.1383.hdf5\nEpoch 80/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1287\n\nEpoch 00080: loss improved from 2.13835 to 2.12873, saving model to weights/weights_epoch_080_loss_2.1287.hdf5\nEpoch 81/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.1249\n\nMy Boof kour:\nآن بود که به او با در آن او به من بود که از این بود که به من بود که از در من بود که به من بود که از این در اطاقم به من بود که از در من بود که به من بود که از در من بود که به من بود که از این در اطاقم به من بود که از در من بود که به من بود که از در من بود که به من بود که از این در اطاقم به من بود که از در من بود که به من بود که از در من بود که به من بود که از این در اطاقم به من بود که از در من بود که به من بود که از در من بود که به من بود که از این در اطاقم به من بود که از در من بود که به من بود ک\n\nEpoch 00081: loss improved from 2.12873 to 2.12494, saving model to weights/weights_epoch_081_loss_2.1249.hdf5\nEpoch 82/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.1183\n\nEpoch 00082: loss improved from 2.12494 to 2.11826, saving model to weights/weights_epoch_082_loss_2.1183.hdf5\nEpoch 83/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1197\n\nEpoch 00083: loss did not improve from 2.11826\nEpoch 84/301\n9/9 [==============================] - 1s 137ms/step - loss: 2.1018\n\nEpoch 00084: loss improved from 2.11826 to 2.10183, saving model to weights/weights_epoch_084_loss_2.1018.hdf5\nEpoch 85/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.1022\n\nEpoch 00085: loss did not improve from 2.10183\nEpoch 86/301\n9/9 [==============================] - 1s 133ms/step - loss: 2.0957\n\nEpoch 00086: loss improved from 2.10183 to 2.09570, saving model to weights/weights_epoch_086_loss_2.0957.hdf5\nEpoch 87/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0985\n\nEpoch 00087: loss did not improve from 2.09570\nEpoch 88/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0902\n\nEpoch 00088: loss improved from 2.09570 to 2.09015, saving model to weights/weights_epoch_088_loss_2.0902.hdf5\nEpoch 89/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0923\n\nEpoch 00089: loss did not improve from 2.09015\nEpoch 90/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.0829\n\nEpoch 00090: loss improved from 2.09015 to 2.08285, saving model to weights/weights_epoch_090_loss_2.0829.hdf5\nEpoch 91/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0709\n\nMy Boof kour:\nگی مردم بود که به هم می آود و به این حرکت برای من کرده بود و به این احسیس به من نگاه کردم بود که به روی سرو روی سرو را به من گذاشته بود و به این او را به من تولید کرد. بعد از این حرکت به من نگاه کرده بودند. به نظرم آمد که او را به من تولید می کرد. من هم به من ترسید که به من نگاه کردم بود که به روی سرو روی سرو را به من گذاشته بود و به این او را به من تولید کرد. بعد از این حرکت به من نگاه کرده بودند. به نظرم آمد که او را به من تولید می کرد. من هم به من ترسید که به من نگاه کردم بود که به روی سرو روی\n\nEpoch 00091: loss improved from 2.08285 to 2.07088, saving model to weights/weights_epoch_091_loss_2.0709.hdf5\nEpoch 92/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.0609\n\nEpoch 00092: loss improved from 2.07088 to 2.06087, saving model to weights/weights_epoch_092_loss_2.0609.hdf5\nEpoch 93/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.0677\n\nEpoch 00093: loss did not improve from 2.06087\nEpoch 94/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.0603\n\nEpoch 00094: loss improved from 2.06087 to 2.06031, saving model to weights/weights_epoch_094_loss_2.0603.hdf5\nEpoch 95/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0621\n\nEpoch 00095: loss did not improve from 2.06031\nEpoch 96/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0528\n\nEpoch 00096: loss improved from 2.06031 to 2.05284, saving model to weights/weights_epoch_096_loss_2.0528.hdf5\nEpoch 97/301\n9/9 [==============================] - 1s 137ms/step - loss: 2.0456\n\nEpoch 00097: loss improved from 2.05284 to 2.04563, saving model to weights/weights_epoch_097_loss_2.0456.hdf5\nEpoch 98/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0412\n\nEpoch 00098: loss improved from 2.04563 to 2.04123, saving model to weights/weights_epoch_098_loss_2.0412.hdf5\nEpoch 99/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0424\n\nEpoch 00099: loss did not improve from 2.04123\nEpoch 100/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0327\n\nEpoch 00100: loss improved from 2.04123 to 2.03268, saving model to weights/weights_epoch_100_loss_2.0327.hdf5\nEpoch 101/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0284\n\nMy Boof kour:\n؟ من همه این او را در میان من بود و می توانستم به او می گفتم: «بیا این است که مرا می کردم که به روی سیاه به در می توانستم بکنم. به قدری که به خودم حرف می کردم. من همین وقت از میان مرده بود که به من نگاه می کردم که به من ترسید می کردم که به من ترسید می کردم که به او می توانستم بکنم که به خودم حرف می کردم. من همین وقت از میان مرده بود که به من نگاه می کردم که به من ترسید می کردم که به من ترسید می کردم که به او می توانستم بکنم که به خودم حرف می کردم. من همین وقت از میان مرده بود که به من نگاه می کرد\n\nEpoch 00101: loss improved from 2.03268 to 2.02836, saving model to weights/weights_epoch_101_loss_2.0284.hdf5\nEpoch 102/301\n9/9 [==============================] - 1s 138ms/step - loss: 2.0220\n\nEpoch 00102: loss improved from 2.02836 to 2.02202, saving model to weights/weights_epoch_102_loss_2.0220.hdf5\nEpoch 103/301\n9/9 [==============================] - 1s 140ms/step - loss: 2.0203\n\nEpoch 00103: loss improved from 2.02202 to 2.02033, saving model to weights/weights_epoch_103_loss_2.0203.hdf5\nEpoch 104/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.0148\n\nEpoch 00104: loss improved from 2.02033 to 2.01476, saving model to weights/weights_epoch_104_loss_2.0148.hdf5\nEpoch 105/301\n9/9 [==============================] - 1s 135ms/step - loss: 2.0078\n\nEpoch 00105: loss improved from 2.01476 to 2.00780, saving model to weights/weights_epoch_105_loss_2.0078.hdf5\nEpoch 106/301\n9/9 [==============================] - 1s 134ms/step - loss: 2.0054\n\nEpoch 00106: loss improved from 2.00780 to 2.00542, saving model to weights/weights_epoch_106_loss_2.0054.hdf5\nEpoch 107/301\n9/9 [==============================] - 1s 139ms/step - loss: 1.9957\n\nEpoch 00107: loss improved from 2.00542 to 1.99571, saving model to weights/weights_epoch_107_loss_1.9957.hdf5\nEpoch 108/301\n9/9 [==============================] - 1s 139ms/step - loss: 1.9889\n\nEpoch 00108: loss improved from 1.99571 to 1.98886, saving model to weights/weights_epoch_108_loss_1.9889.hdf5\nEpoch 109/301\n9/9 [==============================] - 1s 138ms/step - loss: 1.9909\n\nEpoch 00109: loss did not improve from 1.98886\nEpoch 110/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.9801\n\nEpoch 00110: loss improved from 1.98886 to 1.98007, saving model to weights/weights_epoch_110_loss_1.9801.hdf5\nEpoch 111/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.9746\n\nMy Boof kour:\n«به او می آمد که او با او بی انداخته بودم که او با به خودم آمدم برای می کردم. به نظرم آمد که این دنیای را به اندازه بود که از او با در آوردم، به نظرم آمد که این دنیای را به او با یک درده بودم که او با به او می گفتم: «این در چشمه به من به من نگاه کردم بودم که او با با خودم حرف می کردم. به قدری که به تن آدم راست می کردم که او تا به این من به دست داده بودم که او با به خودم آمدم برای می کردم که او با او بی اندازه بود و با او می گفتم: «بیا او تاریک بیرون می کردم. بعد از این دسته به او با یک دریای با د\n\nEpoch 00111: loss improved from 1.98007 to 1.97462, saving model to weights/weights_epoch_111_loss_1.9746.hdf5\nEpoch 112/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.9716\n\nEpoch 00112: loss improved from 1.97462 to 1.97162, saving model to weights/weights_epoch_112_loss_1.9716.hdf5\nEpoch 113/301\n9/9 [==============================] - 1s 138ms/step - loss: 1.9736\n\nEpoch 00113: loss did not improve from 1.97162\nEpoch 114/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.9691\n\nEpoch 00114: loss improved from 1.97162 to 1.96908, saving model to weights/weights_epoch_114_loss_1.9691.hdf5\nEpoch 115/301\n9/9 [==============================] - 1s 137ms/step - loss: 1.9626\n\nEpoch 00115: loss improved from 1.96908 to 1.96258, saving model to weights/weights_epoch_115_loss_1.9626.hdf5\nEpoch 116/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.9653\n\nEpoch 00116: loss did not improve from 1.96258\nEpoch 117/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.9573\n\nEpoch 00117: loss improved from 1.96258 to 1.95729, saving model to weights/weights_epoch_117_loss_1.9573.hdf5\nEpoch 118/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.9479\n\nEpoch 00118: loss improved from 1.95729 to 1.94787, saving model to weights/weights_epoch_118_loss_1.9479.hdf5\nEpoch 119/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.9470\n\nEpoch 00119: loss improved from 1.94787 to 1.94700, saving model to weights/weights_epoch_119_loss_1.9470.hdf5\nEpoch 120/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.9346\n\nEpoch 00120: loss improved from 1.94700 to 1.93457, saving model to weights/weights_epoch_120_loss_1.9346.hdf5\nEpoch 121/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.9381\n\nMy Boof kour:\nگی می کردم که او می توانست با خودم برگی می کرد. مثل اینکه در این من و یا در این من را به من ترسید کرد. با اورتی که می خواستم از خودم برگی می کردم. در این وقت از خودم به من نگاه می کرد. مثل اینکه در این من و و می کرد و می توانست با خودم می گفتم: «این در تاریک می خوریم، حالا نخوریم کی خوریم؟»»\nمن هست که این مرده ها به من نگاه می کرد. مثل اینکه در این من و یا در این من را به من ترسید کرد. با اورتی که می خواستم از خودم برگی می کردم. در این وقت از خودم به من نگاه می کرد. مثل اینکه در این من و و می کرد\n\nEpoch 00121: loss did not improve from 1.93457\nEpoch 122/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.9325\n\nEpoch 00122: loss improved from 1.93457 to 1.93252, saving model to weights/weights_epoch_122_loss_1.9325.hdf5\nEpoch 123/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.9253\n\nEpoch 00123: loss improved from 1.93252 to 1.92527, saving model to weights/weights_epoch_123_loss_1.9253.hdf5\nEpoch 124/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.9156\n\nEpoch 00124: loss improved from 1.92527 to 1.91555, saving model to weights/weights_epoch_124_loss_1.9156.hdf5\nEpoch 125/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.9102\n\nEpoch 00125: loss improved from 1.91555 to 1.91015, saving model to weights/weights_epoch_125_loss_1.9102.hdf5\nEpoch 126/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.9089\n\nEpoch 00126: loss improved from 1.91015 to 1.90886, saving model to weights/weights_epoch_126_loss_1.9089.hdf5\nEpoch 127/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.9043\n\nEpoch 00127: loss improved from 1.90886 to 1.90428, saving model to weights/weights_epoch_127_loss_1.9043.hdf5\nEpoch 128/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.8992\n\nEpoch 00128: loss improved from 1.90428 to 1.89921, saving model to weights/weights_epoch_128_loss_1.8992.hdf5\nEpoch 129/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8960\n\nEpoch 00129: loss improved from 1.89921 to 1.89600, saving model to weights/weights_epoch_129_loss_1.8960.hdf5\nEpoch 130/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8876\n\nEpoch 00130: loss improved from 1.89600 to 1.88765, saving model to weights/weights_epoch_130_loss_1.8876.hdf5\nEpoch 131/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8920\n\nMy Boof kour:\n| در این وقت از من در من تولید شده بود که این احتیاج به اطاق بود که من به هم می آمد و در این اطاق به من نگاه می کرد. با این احظه جن دی که در من تولید کرد. بالاخره از این صورت او را در من تولید کرد. با اخره افکار و او را با دنیای در من پیدا شده بود و با اینکه به خودم آمدم بی ترتی به او بگویم « از این بود که این دنیای جرو و با انداز می کرد. بعد از اینکه به خودم آمدم بی ارتی به او بگویم « از این بود که این دنیای جرو و با اندازه بود. اولا از این بود که از در چه اطاقم ترسناکی می کرد. من این احتیاج به \n\nEpoch 00131: loss did not improve from 1.88765\nEpoch 132/301\n9/9 [==============================] - 1s 137ms/step - loss: 1.8783\n\nEpoch 00132: loss improved from 1.88765 to 1.87829, saving model to weights/weights_epoch_132_loss_1.8783.hdf5\nEpoch 133/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8730\n\nEpoch 00133: loss improved from 1.87829 to 1.87297, saving model to weights/weights_epoch_133_loss_1.8730.hdf5\nEpoch 134/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.8695\n\nEpoch 00134: loss improved from 1.87297 to 1.86952, saving model to weights/weights_epoch_134_loss_1.8695.hdf5\nEpoch 135/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8701\n\nEpoch 00135: loss did not improve from 1.86952\nEpoch 136/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8520\n\nEpoch 00136: loss improved from 1.86952 to 1.85203, saving model to weights/weights_epoch_136_loss_1.8520.hdf5\nEpoch 137/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8597\n\nEpoch 00137: loss did not improve from 1.85203\nEpoch 138/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8478\n\nEpoch 00138: loss improved from 1.85203 to 1.84779, saving model to weights/weights_epoch_138_loss_1.8478.hdf5\nEpoch 139/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8405\n\nEpoch 00139: loss improved from 1.84779 to 1.84049, saving model to weights/weights_epoch_139_loss_1.8405.hdf5\nEpoch 140/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.8364\n\nEpoch 00140: loss improved from 1.84049 to 1.83636, saving model to weights/weights_epoch_140_loss_1.8364.hdf5\nEpoch 141/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.8323\n\nMy Boof kour:\nصورت به من ترسید کرد. با اخره کو که برای خودم را برای کادم و در روی سرو روی کاغذ بودم و به این او را برای من بودم که به من نگاه کردم بودم در یک روی خور کرده بودم که به من نگاه کردم بودم در یک روی خودم برد بی کردم که جلو چشمم بود که به من افتاده بودم که به نظر می آمد که در عال و تاریک بود که می توانستم بکنم که به من نگاه کردم بودم در اطاق خودم بودم که به روی پیش سوز روی پیش سوز را بر کشیدم، ولی پیدا کردم که بر روی روی سیوه ام بیدار کردم و در میان دوی که برای خودم را به من آیدم و در این موقع به من \n\nEpoch 00141: loss improved from 1.83636 to 1.83225, saving model to weights/weights_epoch_141_loss_1.8323.hdf5\nEpoch 142/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.8304\n\nEpoch 00142: loss improved from 1.83225 to 1.83040, saving model to weights/weights_epoch_142_loss_1.8304.hdf5\nEpoch 143/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.8326\n\nEpoch 00143: loss did not improve from 1.83040\nEpoch 144/301\n9/9 [==============================] - 1s 138ms/step - loss: 1.8164\n\nEpoch 00144: loss improved from 1.83040 to 1.81640, saving model to weights/weights_epoch_144_loss_1.8164.hdf5\nEpoch 145/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.8200\n\nEpoch 00145: loss did not improve from 1.81640\nEpoch 146/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8169\n\nEpoch 00146: loss did not improve from 1.81640\nEpoch 147/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.8086\n\nEpoch 00147: loss improved from 1.81640 to 1.80856, saving model to weights/weights_epoch_147_loss_1.8086.hdf5\nEpoch 148/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7999\n\nEpoch 00148: loss improved from 1.80856 to 1.79992, saving model to weights/weights_epoch_148_loss_1.7999.hdf5\nEpoch 149/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7942\n\nEpoch 00149: loss improved from 1.79992 to 1.79423, saving model to weights/weights_epoch_149_loss_1.7942.hdf5\nEpoch 150/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7889\n\nEpoch 00150: loss improved from 1.79423 to 1.78885, saving model to weights/weights_epoch_150_loss_1.7889.hdf5\nEpoch 151/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.7749\n\nMy Boof kour:\nست که مرا به خودش می گرفت. ولی این دست از جست دایه ام بی تراند و در عین حال برایم می کردند. من هم هم با خودم حرف می زدم. در این وقت از خودم بی ترسیدم و در سر را بودم که به من نگاه کرد. مثل اینکه من خواب می روید. از پیر مرد قصاب روز و چیزه به در آورده بود. من به این شرار که نظر می کردم مثل این بود که او را به تن آدم راست می کرد. من هم هم شا با خودم حرف می زدم. در این وقت از خودم بی ترسیدم و در سر را بودم که به من نگاه کرد. مثل اینکه من خواب می روید. از پیر مرد قصاب روز و چیزه به در آورده بود. من ب\n\nEpoch 00151: loss improved from 1.78885 to 1.77488, saving model to weights/weights_epoch_151_loss_1.7749.hdf5\nEpoch 152/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.7830\n\nEpoch 00152: loss did not improve from 1.77488\nEpoch 153/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7875\n\nEpoch 00153: loss did not improve from 1.77488\nEpoch 154/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7803\n\nEpoch 00154: loss did not improve from 1.77488\nEpoch 155/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7728\n\nEpoch 00155: loss improved from 1.77488 to 1.77279, saving model to weights/weights_epoch_155_loss_1.7728.hdf5\nEpoch 156/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7653\n\nEpoch 00156: loss improved from 1.77279 to 1.76529, saving model to weights/weights_epoch_156_loss_1.7653.hdf5\nEpoch 157/301\n9/9 [==============================] - 1s 139ms/step - loss: 1.7568\n\nEpoch 00157: loss improved from 1.76529 to 1.75676, saving model to weights/weights_epoch_157_loss_1.7568.hdf5\nEpoch 158/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.7488\n\nEpoch 00158: loss improved from 1.75676 to 1.74879, saving model to weights/weights_epoch_158_loss_1.7488.hdf5\nEpoch 159/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.7516\n\nEpoch 00159: loss did not improve from 1.74879\nEpoch 160/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7475\n\nEpoch 00160: loss improved from 1.74879 to 1.74755, saving model to weights/weights_epoch_160_loss_1.7475.hdf5\nEpoch 161/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7465\n\nMy Boof kour:\nثل مرده بود و با او برزری به در آورده بود و با این لحظه آرام و روشنائی پرید و پریان می کرد و با حالت خودم برفیدم و در چشم ایم را باز کردم و در را روی سینه ام گذاشتم. از این بود که باید به او برفته بودم و در این اطاق را با دیدار اطاقم رفتم در اطاقم پیرمرد خنزرپنزری جلو اطاقم را با دست دردم و بی ارتیار او را گرفتم. از پیاله خودم را که در آوردم، در چادم را برگرم کردم و در را روی سیاه ام گذاشتم. در این وقت از خودم برای می کردم که در عین حال برایم محوو و در کسی بود. برای اینکه به خودم را برداشتم و در \n\nEpoch 00161: loss improved from 1.74755 to 1.74651, saving model to weights/weights_epoch_161_loss_1.7465.hdf5\nEpoch 162/301\n9/9 [==============================] - 1s 137ms/step - loss: 1.7368\n\nEpoch 00162: loss improved from 1.74651 to 1.73684, saving model to weights/weights_epoch_162_loss_1.7368.hdf5\nEpoch 163/301\n9/9 [==============================] - 1s 138ms/step - loss: 1.7275\n\nEpoch 00163: loss improved from 1.73684 to 1.72752, saving model to weights/weights_epoch_163_loss_1.7275.hdf5\nEpoch 164/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.7297\n\nEpoch 00164: loss did not improve from 1.72752\nEpoch 165/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7253\n\nEpoch 00165: loss improved from 1.72752 to 1.72532, saving model to weights/weights_epoch_165_loss_1.7253.hdf5\nEpoch 166/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.7119\n\nEpoch 00166: loss improved from 1.72532 to 1.71186, saving model to weights/weights_epoch_166_loss_1.7119.hdf5\nEpoch 167/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7163\n\nEpoch 00167: loss did not improve from 1.71186\nEpoch 168/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7094\n\nEpoch 00168: loss improved from 1.71186 to 1.70939, saving model to weights/weights_epoch_168_loss_1.7094.hdf5\nEpoch 169/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7067\n\nEpoch 00169: loss improved from 1.70939 to 1.70671, saving model to weights/weights_epoch_169_loss_1.7067.hdf5\nEpoch 170/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.7014\n\nEpoch 00170: loss improved from 1.70671 to 1.70141, saving model to weights/weights_epoch_170_loss_1.7014.hdf5\nEpoch 171/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.6802\n\nMy Boof kour:\nې در اور ماده می شد و می گذشتند و مرا می خواندند: «بیا بریم را می خوریم، حالا نخوریم کی خوریم؟ »\nمن هراسان خودم را به روی سینه ام گفتم. اطاقم تو را میدادم، ولی پیر رد خنزرپنزری جلو اطاقم پیدا می شد، ولی از در چه روی مرده بودم که در ماریکی آن ای ترسناک و از این حالت می کردم که میان تنم مو می شود. این در هایم مرا به هم نفس می کردم مثل اینکه از من دار او را در آن می روشدند و از زمان می کردند. مرده او بود. او افتاده بودم که او را به زنین می رستند و شاید ام از آن بوده است. آیا این ساعت هم مقامی بودم ک\n\nEpoch 00171: loss improved from 1.70141 to 1.68018, saving model to weights/weights_epoch_171_loss_1.6802.hdf5\nEpoch 172/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.6887\n\nEpoch 00172: loss did not improve from 1.68018\nEpoch 173/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.6691\n\nEpoch 00173: loss improved from 1.68018 to 1.66906, saving model to weights/weights_epoch_173_loss_1.6691.hdf5\nEpoch 174/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.6730\n\nEpoch 00174: loss did not improve from 1.66906\nEpoch 175/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.6715\n\nEpoch 00175: loss did not improve from 1.66906\nEpoch 176/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.6710\n\nEpoch 00176: loss did not improve from 1.66906\nEpoch 177/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.6626\n\nEpoch 00177: loss improved from 1.66906 to 1.66265, saving model to weights/weights_epoch_177_loss_1.6626.hdf5\nEpoch 178/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.6559\n\nEpoch 00178: loss improved from 1.66265 to 1.65587, saving model to weights/weights_epoch_178_loss_1.6559.hdf5\nEpoch 179/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.6505\n\nEpoch 00179: loss improved from 1.65587 to 1.65053, saving model to weights/weights_epoch_179_loss_1.6505.hdf5\nEpoch 180/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.6538\n\nEpoch 00180: loss did not improve from 1.65053\nEpoch 181/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.6516\n\nMy Boof kour:\nثل مرده متحوک و با اشته بود. به نظرم آمد که تا او نم تا هستم با نم تنها بود. من هر سر نورت به من نگاه کردم بودم در اطاقم به سرف رفتم جلو پستو اطاقم را باز کردم، در چطاقم را باز کردم، دیدم او بیدار بودم و در اطاقم به در گز دنیای بریده بودم که در این وقت از خودم برداشتم، از این صارته شده بودم که او می دانستم و در این اطاق به من نگاه بکند  در این وقت از خودم برای من او را در میان او به من نگاه می کرد. چند نقش ترسناکی به است او را به دیدا مجام می کرد و به او حس کرده بودند. دست ای می رفتم و با خودم فر\n\nEpoch 00181: loss did not improve from 1.65053\nEpoch 182/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.6374\n\nEpoch 00182: loss improved from 1.65053 to 1.63744, saving model to weights/weights_epoch_182_loss_1.6374.hdf5\nEpoch 183/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.6360\n\nEpoch 00183: loss improved from 1.63744 to 1.63604, saving model to weights/weights_epoch_183_loss_1.6360.hdf5\nEpoch 184/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.6252\n\nEpoch 00184: loss improved from 1.63604 to 1.62516, saving model to weights/weights_epoch_184_loss_1.6252.hdf5\nEpoch 185/301\n9/9 [==============================] - 1s 132ms/step - loss: 1.6355\n\nEpoch 00185: loss did not improve from 1.62516\nEpoch 186/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.6268\n\nEpoch 00186: loss did not improve from 1.62516\nEpoch 187/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.6239\n\nEpoch 00187: loss improved from 1.62516 to 1.62391, saving model to weights/weights_epoch_187_loss_1.6239.hdf5\nEpoch 188/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.6127\n\nEpoch 00188: loss improved from 1.62391 to 1.61269, saving model to weights/weights_epoch_188_loss_1.6127.hdf5\nEpoch 189/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.6021\n\nEpoch 00189: loss improved from 1.61269 to 1.60212, saving model to weights/weights_epoch_189_loss_1.6021.hdf5\nEpoch 190/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.5965\n\nEpoch 00190: loss improved from 1.60212 to 1.59651, saving model to weights/weights_epoch_190_loss_1.5965.hdf5\nEpoch 191/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.6018\n\nMy Boof kour:\nکرده بودم که به این گزلیک دسته استخوانی را در است اطاق من کند، مثل اینکه عالای که برای پندان بود برای اینکه بتوانم به دست درود به دنیال برای خودش پیر برده بود و با اینکه به اطاق خودش بیفتانند و در سایه ام به در بالیک من در آورده بود و با اینکه به او بگله بوده باشد. از این باع من اعمقل و تنشاک بود. چشمهایم به هم رفت. این احمای باشت به این دقیقه های برای خودش بود. بعد از آنکه به اطاق خودم بیرون می آمد! شاید برای او بعد از او می توانست با خودم بگریم. ولی تصمیم را تاشتم و به دستم از روی سینه ام برداش\n\nEpoch 00191: loss did not improve from 1.59651\nEpoch 192/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5911\n\nEpoch 00192: loss improved from 1.59651 to 1.59109, saving model to weights/weights_epoch_192_loss_1.5911.hdf5\nEpoch 193/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.5899\n\nEpoch 00193: loss improved from 1.59109 to 1.58992, saving model to weights/weights_epoch_193_loss_1.5899.hdf5\nEpoch 194/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.5850\n\nEpoch 00194: loss improved from 1.58992 to 1.58501, saving model to weights/weights_epoch_194_loss_1.5850.hdf5\nEpoch 195/301\n9/9 [==============================] - 1s 139ms/step - loss: 1.5682\n\nEpoch 00195: loss improved from 1.58501 to 1.56820, saving model to weights/weights_epoch_195_loss_1.5682.hdf5\nEpoch 196/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5716\n\nEpoch 00196: loss did not improve from 1.56820\nEpoch 197/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5728\n\nEpoch 00197: loss did not improve from 1.56820\nEpoch 198/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.5543\n\nEpoch 00198: loss improved from 1.56820 to 1.55425, saving model to weights/weights_epoch_198_loss_1.5543.hdf5\nEpoch 199/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.5573\n\nEpoch 00199: loss did not improve from 1.55425\nEpoch 200/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5466\n\nEpoch 00200: loss improved from 1.55425 to 1.54660, saving model to weights/weights_epoch_200_loss_1.5466.hdf5\nEpoch 201/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5487\n\nMy Boof kour:\nگاه می کرد. بعد نگاه خرک و تن ان انگیزی بود. برای اینکه بتوانم به آنجا به دقی آن اختاده بودم. از این احتیاج به نظرم می آمد که طال قبان ای بود و به یک نگاه می کند و با این حالت می اندازند. به خودش می آید که او می نداخید با او می گذشت. این این دالت مرکم به این شکل گو هم بیفته بود. اما از این احراجات به خندش من بی وخواد می کرد. به طوری که این من ا را نمی توانستم بکنم؟ با من این احتیاج به این حالت من این احمقها را با خاده ام است. او این حرکت مرام ا موهای ژولی و غریب و طریعی که از من درست می کرد و با \n\nEpoch 00201: loss did not improve from 1.54660\nEpoch 202/301\n9/9 [==============================] - 1s 138ms/step - loss: 1.5466\n\nEpoch 00202: loss did not improve from 1.54660\nEpoch 203/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.5281\n\nEpoch 00203: loss improved from 1.54660 to 1.52811, saving model to weights/weights_epoch_203_loss_1.5281.hdf5\nEpoch 204/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5306\n\nEpoch 00204: loss did not improve from 1.52811\nEpoch 205/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5354\n\nEpoch 00205: loss did not improve from 1.52811\nEpoch 206/301\n9/9 [==============================] - 1s 138ms/step - loss: 1.5234\n\nEpoch 00206: loss improved from 1.52811 to 1.52343, saving model to weights/weights_epoch_206_loss_1.5234.hdf5\nEpoch 207/301\n9/9 [==============================] - 1s 140ms/step - loss: 1.5132\n\nEpoch 00207: loss improved from 1.52343 to 1.51320, saving model to weights/weights_epoch_207_loss_1.5132.hdf5\nEpoch 208/301\n9/9 [==============================] - 1s 138ms/step - loss: 1.5160\n\nEpoch 00208: loss did not improve from 1.51320\nEpoch 209/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5119\n\nEpoch 00209: loss improved from 1.51320 to 1.51191, saving model to weights/weights_epoch_209_loss_1.5119.hdf5\nEpoch 210/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5017\n\nEpoch 00210: loss improved from 1.51191 to 1.50172, saving model to weights/weights_epoch_210_loss_1.5017.hdf5\nEpoch 211/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.5016\n\nMy Boof kour:\nغل خودش مذل می کردم که به مرا تاریک می کرد و یک پرته هوز به من نمی توانستم بکنم که اگ ما به طرف من آمد. من هر هوز روزنو سرخ به هم نرو رفته بود. من به سر بالیک رفتم. جلو پرهه ای رو را کرفتم، دیدم لا رود را با گردنم - را در آغوش کشیدم، ولی این سرعت هم برای مرده بود. من هر سرز و روز را به دیوار عرق کرده پیرمرد با چالاکی مخصوصی بود که به من نگاه می کرد. من هرچه به خودش را نمی دیدم، چون بی اراده زندگی بودم را به درت آورده بودم. می خواستم این دکوی که میتهه بود در یک مال هم بردم به به نظر می آمد که سا ع\n\nEpoch 00211: loss improved from 1.50172 to 1.50164, saving model to weights/weights_epoch_211_loss_1.5016.hdf5\nEpoch 212/301\n9/9 [==============================] - 1s 137ms/step - loss: 1.4960\n\nEpoch 00212: loss improved from 1.50164 to 1.49595, saving model to weights/weights_epoch_212_loss_1.4960.hdf5\nEpoch 213/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.4943\n\nEpoch 00213: loss improved from 1.49595 to 1.49431, saving model to weights/weights_epoch_213_loss_1.4943.hdf5\nEpoch 214/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4887\n\nEpoch 00214: loss improved from 1.49431 to 1.48868, saving model to weights/weights_epoch_214_loss_1.4887.hdf5\nEpoch 215/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.4829\n\nEpoch 00215: loss improved from 1.48868 to 1.48293, saving model to weights/weights_epoch_215_loss_1.4829.hdf5\nEpoch 216/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4770\n\nEpoch 00216: loss improved from 1.48293 to 1.47702, saving model to weights/weights_epoch_216_loss_1.4770.hdf5\nEpoch 217/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4620\n\nEpoch 00217: loss improved from 1.47702 to 1.46200, saving model to weights/weights_epoch_217_loss_1.4620.hdf5\nEpoch 218/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4567\n\nEpoch 00218: loss improved from 1.46200 to 1.45674, saving model to weights/weights_epoch_218_loss_1.4567.hdf5\nEpoch 219/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.4590\n\nEpoch 00219: loss did not improve from 1.45674\nEpoch 220/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4544\n\nEpoch 00220: loss improved from 1.45674 to 1.45439, saving model to weights/weights_epoch_220_loss_1.4544.hdf5\nEpoch 221/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4535\n\nMy Boof kour:\n! در قین وقت بی فرواد که از زندگی دنگا و او خاده اش مثل سیاه با دور در میان خونده بود و با حالت ترس و جوشانده بود. در میان لحظه ترسی مخصوصی در کندش باشی می کرد. که بچگی خودم را نمی شناختم. به قدر  یک صبا برفت و یا پرتره بود که موهای سرز و سرخ و بلند شد، در تاریکی روی سینه اش بود. ولی این سرفه های قویائی از اطاق بیرون می کشید. در صورتی که پیرمرد خنزرپنزری جلو اطاقم را به چه چشتهای گردت تر از همه به تر و پیرمرد خنزرپپنزری جلو اطاقم پیدا نبود، فقط از ضلع چپ، مرد قصاب را برداشتم، ولی برای اودم را بگی\n\nEpoch 00221: loss improved from 1.45439 to 1.45352, saving model to weights/weights_epoch_221_loss_1.4535.hdf5\nEpoch 222/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4531\n\nEpoch 00222: loss improved from 1.45352 to 1.45310, saving model to weights/weights_epoch_222_loss_1.4531.hdf5\nEpoch 223/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.4378\n\nEpoch 00223: loss improved from 1.45310 to 1.43785, saving model to weights/weights_epoch_223_loss_1.4378.hdf5\nEpoch 224/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.4364\n\nEpoch 00224: loss improved from 1.43785 to 1.43643, saving model to weights/weights_epoch_224_loss_1.4364.hdf5\nEpoch 225/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.4292\n\nEpoch 00225: loss improved from 1.43643 to 1.42924, saving model to weights/weights_epoch_225_loss_1.4292.hdf5\nEpoch 226/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4297\n\nEpoch 00226: loss did not improve from 1.42924\nEpoch 227/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.4322\n\nEpoch 00227: loss did not improve from 1.42924\nEpoch 228/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.4294\n\nEpoch 00228: loss did not improve from 1.42924\nEpoch 229/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4190\n\nEpoch 00229: loss improved from 1.42924 to 1.41896, saving model to weights/weights_epoch_229_loss_1.4190.hdf5\nEpoch 230/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.4181\n\nEpoch 00230: loss improved from 1.41896 to 1.41805, saving model to weights/weights_epoch_230_loss_1.4181.hdf5\nEpoch 231/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3990\n\nMy Boof kour:\nای مضاعف خودم را می بینم. اما برای اینکه بتوانم به جندا برود با در این اطاق بوده اند که یک دنیای نیا و نقاش را در من تولید کرد. من اطاب کردم که از زندگی من هم شه می دانستم در من انقا  را می دانست د. آن احمقا را می بانست که با من است از من می کرد. مثل اینکه به من انها شی شد شراه می زد. ولی همین احآناج به من در این دنیای را دیده بودم. یک دنیای در زندگی بودم را در اطاق کوچک می دادم و به عمام درد می خوردم. ولی تا مین مستله برای من بود. چون یک زنیگی مرامه و تصویری داشته باشد. اگر دایه هم نه در این مطد\n\nEpoch 00231: loss improved from 1.41805 to 1.39899, saving model to weights/weights_epoch_231_loss_1.3990.hdf5\nEpoch 232/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.4107\n\nEpoch 00232: loss did not improve from 1.39899\nEpoch 233/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.3878\n\nEpoch 00233: loss improved from 1.39899 to 1.38783, saving model to weights/weights_epoch_233_loss_1.3878.hdf5\nEpoch 234/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.4017\n\nEpoch 00234: loss did not improve from 1.38783\nEpoch 235/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.3823\n\nEpoch 00235: loss improved from 1.38783 to 1.38234, saving model to weights/weights_epoch_235_loss_1.3823.hdf5\nEpoch 236/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3686\n\nEpoch 00236: loss improved from 1.38234 to 1.36862, saving model to weights/weights_epoch_236_loss_1.3686.hdf5\nEpoch 237/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3904\n\nEpoch 00237: loss did not improve from 1.36862\nEpoch 238/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3845\n\nEpoch 00238: loss did not improve from 1.36862\nEpoch 239/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.3815\n\nEpoch 00239: loss did not improve from 1.36862\nEpoch 240/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.3682\n\nEpoch 00240: loss improved from 1.36862 to 1.36824, saving model to weights/weights_epoch_240_loss_1.3682.hdf5\nEpoch 241/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3584\n\nMy Boof kour:\nعمویم با موهای سفید، قیزهای کست به او پیشمرد بازیک دیده بودم. این احساس یک خیصیت مست کننده داشت و من در این منقه به من در دای می زد، بعد از آنکه آن دو ماهرچه را بی می کشیدم و در همین اطاق که مثل قبر هر خوابیده ام  فقط یک ارتای بدارم بود. مثل این بود که از زندگی او بشریدا نمی آورد. چشمهایم را بستم و دنباله آن او را بگیرم. ولی این سرفه آن من را در خیدم حس می کردم. تاهی ای که از روی رفتم است خیاه و مرا بود. در مقابل ترس از منگ بود، مثل اینکه در من به ای شکن بوده است دایتم و آهادش را با سییه ام اغدار\n\nEpoch 00241: loss improved from 1.36824 to 1.35845, saving model to weights/weights_epoch_241_loss_1.3584.hdf5\nEpoch 242/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3641\n\nEpoch 00242: loss did not improve from 1.35845\nEpoch 243/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3463\n\nEpoch 00243: loss improved from 1.35845 to 1.34635, saving model to weights/weights_epoch_243_loss_1.3463.hdf5\nEpoch 244/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.3315\n\nEpoch 00244: loss improved from 1.34635 to 1.33152, saving model to weights/weights_epoch_244_loss_1.3315.hdf5\nEpoch 245/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3496\n\nEpoch 00245: loss did not improve from 1.33152\nEpoch 246/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.3482\n\nEpoch 00246: loss did not improve from 1.33152\nEpoch 247/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3336\n\nEpoch 00247: loss did not improve from 1.33152\nEpoch 248/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.3265\n\nEpoch 00248: loss improved from 1.33152 to 1.32647, saving model to weights/weights_epoch_248_loss_1.3265.hdf5\nEpoch 249/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.3283\n\nEpoch 00249: loss did not improve from 1.32647\nEpoch 250/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.3175\n\nEpoch 00250: loss improved from 1.32647 to 1.31751, saving model to weights/weights_epoch_250_loss_1.3175.hdf5\nEpoch 251/301\n9/9 [==============================] - 1s 139ms/step - loss: 1.3335\n\nMy Boof kour:\n) می آمد و می خواستم خودم را تسلیم نمسته بشید. اون دروا ها به دنیای گمشده ای مرا بشود به او می گفتم و یا توی دو ترتب می گردم. با آن شد  صورت انگیز کازه دوشت که روی ماهر ان انداخته باشند. آنزدر کردم که یک زن لازه با انداخته، از مران سال خوشگ خنده می شود و گفت:\nاگه هم داخله می خودیدن خنزه تخر ک بلده می شد. همین لحظه ترای من بود، به هیچ وقت نبید از خون بهرون آمد با دن این دنیای پای در هن بود. این احساس بزرگ که اندازه چندین بود، بعد از سر جای خودم خودم آمد، یک رتته خوشه او را بای کایدم، از جایی که آر\n\nEpoch 00251: loss did not improve from 1.31751\nEpoch 252/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.2976\n\nEpoch 00252: loss improved from 1.31751 to 1.29761, saving model to weights/weights_epoch_252_loss_1.2976.hdf5\nEpoch 253/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3220\n\nEpoch 00253: loss did not improve from 1.29761\nEpoch 254/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.3024\n\nEpoch 00254: loss did not improve from 1.29761\nEpoch 255/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.3089\n\nEpoch 00255: loss did not improve from 1.29761\nEpoch 256/301\n9/9 [==============================] - 1s 137ms/step - loss: 1.2953\n\nEpoch 00256: loss improved from 1.29761 to 1.29535, saving model to weights/weights_epoch_256_loss_1.2953.hdf5\nEpoch 257/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.2894\n\nEpoch 00257: loss improved from 1.29535 to 1.28936, saving model to weights/weights_epoch_257_loss_1.2894.hdf5\nEpoch 258/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.2841\n\nEpoch 00258: loss improved from 1.28936 to 1.28413, saving model to weights/weights_epoch_258_loss_1.2841.hdf5\nEpoch 259/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.2897\n\nEpoch 00259: loss did not improve from 1.28413\nEpoch 260/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.2724\n\nEpoch 00260: loss improved from 1.28413 to 1.27238, saving model to weights/weights_epoch_260_loss_1.2724.hdf5\nEpoch 261/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.2680\n\nMy Boof kour:\nہایم می آود که طور می بهته شده و مراه برمده به او تالی خودم بیدار می کردم به قدری این مأثرر عمیق و پرکیف بود. به من سرزنش می کرد. باید حکایت خودم را بروی من ور بودم. در یک دنیای فریده و باموکی که به من مسلید داد. چون با حکمت خراب و تیره بود. من میان روزره هم می کردم! به مورم مردم نگاه کردم باید از تشم میشنیسم. شبه آرمم می توانستم گفتم  کنم، دو قرمه خراست که زندگی بچ ارد به هم آمیخته می شود و به خودش می چسبید.\nصوحت در بایه مرا به هم نرفته بود. من چمدان را روی زمین گذاشتم، پیرمرد کالسکه چی رویش را \n\nEpoch 00261: loss improved from 1.27238 to 1.26802, saving model to weights/weights_epoch_261_loss_1.2680.hdf5\nEpoch 262/301\n9/9 [==============================] - 1s 137ms/step - loss: 1.2704\n\nEpoch 00262: loss did not improve from 1.26802\nEpoch 263/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.2883\n\nEpoch 00263: loss did not improve from 1.26802\nEpoch 264/301\n9/9 [==============================] - 1s 138ms/step - loss: 1.2501\n\nEpoch 00264: loss improved from 1.26802 to 1.25006, saving model to weights/weights_epoch_264_loss_1.2501.hdf5\nEpoch 265/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.2475\n\nEpoch 00265: loss improved from 1.25006 to 1.24752, saving model to weights/weights_epoch_265_loss_1.2475.hdf5\nEpoch 266/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.2534\n\nEpoch 00266: loss did not improve from 1.24752\nEpoch 267/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.2640\n\nEpoch 00267: loss did not improve from 1.24752\nEpoch 268/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.2650\n\nEpoch 00268: loss did not improve from 1.24752\nEpoch 269/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.2338\n\nEpoch 00269: loss improved from 1.24752 to 1.23381, saving model to weights/weights_epoch_269_loss_1.2338.hdf5\nEpoch 270/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.2480\n\nEpoch 00270: loss did not improve from 1.23381\nEpoch 271/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.2429\n\nMy Boof kour:\nی کردم. از این کار نه خسته می کردم نمی توانستم قول از را به خبیر بیاورم. نقاشی برای دورد تر از هرگز نقاشی بود. من می ن روزنه ای مرا مرا هم هستیده بودند. صهرتش یک فراموشی حس می کردم و با یک نفر بی میری می شدم و در هین موقع نمی خواستم به نمای میگر به مو تملیم می کرد. به حالت که به خودم می گفتاد: «شره ی موقت عمقق ان از تن ای ته تی خواب به من نگاه کرده بود؟ مراص ابن او را درست به او نگاه می کردم من همه این سارات سرام نبود. در این وقت از طبیعت و دریای ظاهری کنده می شد، و حاضر بودم که در حال تنایت غریب\n\nEpoch 00271: loss did not improve from 1.23381\nEpoch 272/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.2271\n\nEpoch 00272: loss improved from 1.23381 to 1.22707, saving model to weights/weights_epoch_272_loss_1.2271.hdf5\nEpoch 273/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.2132\n\nEpoch 00273: loss improved from 1.22707 to 1.21322, saving model to weights/weights_epoch_273_loss_1.2132.hdf5\nEpoch 274/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.2260\n\nEpoch 00274: loss did not improve from 1.21322\nEpoch 275/301\n9/9 [==============================] - 1s 133ms/step - loss: 1.2258\n\nEpoch 00275: loss did not improve from 1.21322\nEpoch 276/301\n9/9 [==============================] - 1s 137ms/step - loss: 1.1977\n\nEpoch 00276: loss improved from 1.21322 to 1.19773, saving model to weights/weights_epoch_276_loss_1.1977.hdf5\nEpoch 277/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.1940\n\nEpoch 00277: loss improved from 1.19773 to 1.19396, saving model to weights/weights_epoch_277_loss_1.1940.hdf5\nEpoch 278/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.1988\n\nEpoch 00278: loss did not improve from 1.19396\nEpoch 279/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.1914\n\nEpoch 00279: loss improved from 1.19396 to 1.19144, saving model to weights/weights_epoch_279_loss_1.1914.hdf5\nEpoch 280/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.1944\n\nEpoch 00280: loss did not improve from 1.19144\nEpoch 281/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.2001\n\nMy Boof kour:\nل ناثرت هود و با حالت شماده ا با گایی از دست داده بود. به قدری او را گرفتم به بنگشن  افتاد شدم. مثل اینکه میخواستم از خودم بگریزم و سر وشتم را تغییب بدهم. ششمم را به پای سیوار کردم دیدم مثل اینکه حالم کهتر بودم بر آورد با هندانم و به صورت خودم در من بی ترثید را با دست دوردم. گوشا این حالت خراب و غریان به خود گذاشته بود. گفت: آنها را بر ایاط خودم پیریدم، در چمدان را نمی دادستم گرفتم و روتم تو پنتو او روی را گرسته بود، حالا که پستینه را در جستم حل کردم  به همان حالت بود به حالت غمیانگیزی به دست دار\n\nEpoch 00281: loss did not improve from 1.19144\nEpoch 282/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.1771\n\nEpoch 00282: loss improved from 1.19144 to 1.17706, saving model to weights/weights_epoch_282_loss_1.1771.hdf5\nEpoch 283/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1844\n\nEpoch 00283: loss did not improve from 1.17706\nEpoch 284/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1702\n\nEpoch 00284: loss improved from 1.17706 to 1.17017, saving model to weights/weights_epoch_284_loss_1.1702.hdf5\nEpoch 285/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.1672\n\nEpoch 00285: loss improved from 1.17017 to 1.16724, saving model to weights/weights_epoch_285_loss_1.1672.hdf5\nEpoch 286/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1558\n\nEpoch 00286: loss improved from 1.16724 to 1.15583, saving model to weights/weights_epoch_286_loss_1.1558.hdf5\nEpoch 287/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1637\n\nEpoch 00287: loss did not improve from 1.15583\nEpoch 288/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1579\n\nEpoch 00288: loss did not improve from 1.15583\nEpoch 289/301\n9/9 [==============================] - 1s 137ms/step - loss: 1.1530\n\nEpoch 00289: loss improved from 1.15583 to 1.15304, saving model to weights/weights_epoch_289_loss_1.1530.hdf5\nEpoch 290/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1622\n\nEpoch 00290: loss did not improve from 1.15304\nEpoch 291/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.1519\n\nMy Boof kour:\n- در چمدان - نگاه کنم. دور خودم را نمی شناختم. مثل یک صدای خارجی، یک خنده ای که تغلب بود الوه  بیچیده بود بیداشتم و به اورت او که حالت بچگی ه می توانستم بکنم؟ چونههمی این ا را به من تسکین داد، چون به حالت غریب و مرا می خورد  این احساس برایم مقن بود که در تن گی است، و بعد از آنکه من رست چشمهایش را به هم می گذارم سایه های محو و مخلوط شهر - آنچه که در من تأثیر کرده بود که شدیدای از انتار رود و به من نگاه می کرد. مشتهای خود را میل لونه در اواد خانه های لجیب و غریب به اشکال هندسی، منشور، مخروطی، مکعب \n\nEpoch 00291: loss improved from 1.15304 to 1.15194, saving model to weights/weights_epoch_291_loss_1.1519.hdf5\nEpoch 292/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.1395\n\nEpoch 00292: loss improved from 1.15194 to 1.13952, saving model to weights/weights_epoch_292_loss_1.1395.hdf5\nEpoch 293/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.1347\n\nEpoch 00293: loss improved from 1.13952 to 1.13466, saving model to weights/weights_epoch_293_loss_1.1347.hdf5\nEpoch 294/301\n9/9 [==============================] - 1s 139ms/step - loss: 1.1481\n\nEpoch 00294: loss did not improve from 1.13466\nEpoch 295/301\n9/9 [==============================] - 1s 136ms/step - loss: 1.1272\n\nEpoch 00295: loss improved from 1.13466 to 1.12725, saving model to weights/weights_epoch_295_loss_1.1272.hdf5\nEpoch 296/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1086\n\nEpoch 00296: loss improved from 1.12725 to 1.10861, saving model to weights/weights_epoch_296_loss_1.1086.hdf5\nEpoch 297/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1079\n\nEpoch 00297: loss improved from 1.10861 to 1.10788, saving model to weights/weights_epoch_297_loss_1.1079.hdf5\nEpoch 298/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1217\n\nEpoch 00298: loss did not improve from 1.10788\nEpoch 299/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1237\n\nEpoch 00299: loss did not improve from 1.10788\nEpoch 300/301\n9/9 [==============================] - 1s 135ms/step - loss: 1.1140\n\nEpoch 00300: loss did not improve from 1.10788\nEpoch 301/301\n9/9 [==============================] - 1s 134ms/step - loss: 1.1107\n\nMy Boof kour:\n. در هوای بارانی که زندگی زنده یا از سنگ آستان می کنند که طالاتا چسک تن من برد و به من ظرف می زد. باید حرارت خودم را به دایل خیعم رو به ما دیدم و به قصابهای من بود. در هوا صدا کرده استم و به زود زحاق روی زمین تغایر می کردم که به من تسکین داد و نا خاج می شدم و در هی  وقع مثل اینکه پر هوای من را به جلو من فرست و یا پیداخت از رنگ او با دست داده بودم می تواستم این دکوی که میتاماندم و در همان احتقاج به نظر می کنند به طاع می توانست با نا کا قد! این حس واتار بی کرد. می تواسد عیا از این حس لابتی بی بن دن\n\nEpoch 00301: loss did not improve from 1.10788\n",
     "output_type": "stream"
    },
    {
     "execution_count": 1,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fda6847fdd0>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "famous Farsi Poet work collection as input data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nfrom tensorflow.keras import layers, models, losses, optimizers\nimport numpy as np\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n\nraw_text = open('/kaggle/input/dlhwch13/roodaki_norm.txt', 'r', encoding=\"utf8\").read()\nraw_text = raw_text.lower()\n\nprint(raw_text[:200])\n\nall_words = raw_text.split()\nunique_words = list(set(all_words))\nprint(f'Number of unique words: {len(unique_words)}')\nn_chars = len(raw_text)\nprint(f'Total characters: {n_chars}')\n\nchars = sorted(list(set(raw_text)))\nn_vocab = len(chars)\nprint(f'Total vocabulary (unique characters): {n_vocab}')\nprint(chars)\n\nindex_to_char = dict((i, c) for i, c in enumerate(chars))\nchar_to_index = dict((c, i) for i, c in enumerate(chars))\nprint(char_to_index)\n\nseq_length = 160\nn_seq = int(n_chars / seq_length)\n\nX = np.zeros((n_seq, seq_length, n_vocab))\nY = np.zeros((n_seq, seq_length, n_vocab))\n\nfor i in range(n_seq):\n    x_sequence = raw_text[i * seq_length: (i + 1) * seq_length]\n    x_sequence_ohe = np.zeros((seq_length, n_vocab))\n    for j in range(seq_length):\n        char = x_sequence[j]\n        index = char_to_index[char]\n        x_sequence_ohe[j][index] = 1.\n    X[i] = x_sequence_ohe\n    y_sequence = raw_text[i * seq_length + 1: (i + 1) * seq_length + 1]\n    y_sequence_ohe = np.zeros((seq_length, n_vocab))\n    for j in range(seq_length):\n        char = y_sequence[j]\n        index = char_to_index[char]\n        y_sequence_ohe[j][index] = 1.\n    Y[i] = y_sequence_ohe\n\nprint(X.shape)\nprint(Y.shape)\n\ntf.random.set_seed(42)\nbatch_size = 100\nhidden_units = 700\nn_epoch = 301\ndropout = 0.4\n\nmodel = models.Sequential()\nmodel.add(layers.LSTM(hidden_units, input_shape=(None, n_vocab), return_sequences=True, dropout=dropout))\nmodel.add(layers.LSTM(hidden_units, return_sequences=True, dropout=dropout))\nmodel.add(layers.TimeDistributed(layers.Dense(n_vocab, activation='softmax')))\n\noptimizer = optimizers.RMSprop(learning_rate=0.001)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n\nprint(model.summary())\n\nfilepath = \"weights/weights_epoch_{epoch:03d}_loss_{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\nearly_stop = EarlyStopping(monitor='loss', min_delta=0, patience=50, verbose=1, mode='min')\n\n\ndef generate_text(model, gen_length, n_vocab, index_to_char):\n    index = np.random.randint(n_vocab)\n    y_char = [index_to_char[index]]\n    X = np.zeros((1, gen_length, n_vocab))\n    for i in range(gen_length):\n        X[0, i, index] = 1.\n        indices = np.argmax(model.predict(X[:, max(0, i - 99):i + 1, :])[0], 1)\n        index = indices[-1]\n        y_char.append(index_to_char[index])\n    return ''.join(y_char)\n\n\nclass ResultChecker(Callback):\n    def __init__(self, model, N, gen_length):\n        self.model = model\n        self.N = N\n        self.gen_length = gen_length\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.N == 0:\n            result = generate_text(self.model, self.gen_length, n_vocab, index_to_char)\n            print('\\nMy Roodaki colection:\\n' + result)\n\n\nresult_checker = ResultChecker(model, 10, 500)\n\nmodel.fit(X, Y, batch_size=batch_size, verbose=1, epochs=n_epoch, callbacks=[result_checker, checkpoint, early_stop])\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-16T11:00:11.507774Z",
     "iopub.execute_input": "2022-07-16T11:00:11.508230Z",
     "iopub.status.idle": "2022-07-16T11:17:37.954673Z",
     "shell.execute_reply.started": "2022-07-16T11:00:11.508198Z",
     "shell.execute_reply": "2022-07-16T11:17:37.953139Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "گر من این دوستی تو ببرم تا لب گور\nبزنم نعره ولیکن ز تو بینم هنرا\nاثر میر نخواهم که بماند به جهان\nمیر خواهم که بماند به جهان در اثرا\nهر که را رفت همی باید رفته شمری\nهر که را مرد همی باید مرده شمرا\nبه ح\nNumber of unique words: 3868\nTotal characters: 59205\nTotal vocabulary (unique characters): 35\n['\\n', ' ', 'آ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ی']\n{'\\n': 0, ' ': 1, 'آ': 2, 'ا': 3, 'ب': 4, 'ت': 5, 'ث': 6, 'ج': 7, 'ح': 8, 'خ': 9, 'د': 10, 'ذ': 11, 'ر': 12, 'ز': 13, 'س': 14, 'ش': 15, 'ص': 16, 'ض': 17, 'ط': 18, 'ظ': 19, 'ع': 20, 'غ': 21, 'ف': 22, 'ق': 23, 'ل': 24, 'م': 25, 'ن': 26, 'ه': 27, 'و': 28, 'پ': 29, 'چ': 30, 'ژ': 31, 'ک': 32, 'گ': 33, 'ی': 34}\n(370, 160, 35)\n(370, 160, 35)\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_4 (LSTM)                (None, None, 700)         2060800   \n_________________________________________________________________\nlstm_5 (LSTM)                (None, None, 700)         3922800   \n_________________________________________________________________\ntime_distributed_2 (TimeDist (None, None, 35)          24535     \n=================================================================\nTotal params: 6,008,135\nTrainable params: 6,008,135\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/301\n4/4 [==============================] - 4s 136ms/step - loss: 4.7950\n\nMy Roodaki colection:\nع                هههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههههه\n\nEpoch 00001: loss improved from inf to 4.79502, saving model to weights/weights_epoch_001_loss_4.7950.hdf5\nEpoch 2/301\n4/4 [==============================] - 1s 137ms/step - loss: 3.2283\n\nEpoch 00002: loss improved from 4.79502 to 3.22834, saving model to weights/weights_epoch_002_loss_3.2283.hdf5\nEpoch 3/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.9391\n\nEpoch 00003: loss improved from 3.22834 to 2.93915, saving model to weights/weights_epoch_003_loss_2.9391.hdf5\nEpoch 4/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.9325\n\nEpoch 00004: loss improved from 2.93915 to 2.93248, saving model to weights/weights_epoch_004_loss_2.9325.hdf5\nEpoch 5/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.9299\n\nEpoch 00005: loss improved from 2.93248 to 2.92991, saving model to weights/weights_epoch_005_loss_2.9299.hdf5\nEpoch 6/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.9392\n\nEpoch 00006: loss did not improve from 2.92991\nEpoch 7/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.9714\n\nEpoch 00007: loss did not improve from 2.92991\nEpoch 8/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.9524\n\nEpoch 00008: loss did not improve from 2.92991\nEpoch 9/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.9386\n\nEpoch 00009: loss did not improve from 2.92991\nEpoch 10/301\n4/4 [==============================] - 1s 146ms/step - loss: 2.9354\n\nEpoch 00010: loss did not improve from 2.92991\nEpoch 11/301\n4/4 [==============================] - 1s 149ms/step - loss: 2.9443\n\nMy Roodaki colection:\nص                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n\nEpoch 00011: loss did not improve from 2.92991\nEpoch 12/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.9287\n\nEpoch 00012: loss improved from 2.92991 to 2.92874, saving model to weights/weights_epoch_012_loss_2.9287.hdf5\nEpoch 13/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.9278\n\nEpoch 00013: loss improved from 2.92874 to 2.92781, saving model to weights/weights_epoch_013_loss_2.9278.hdf5\nEpoch 14/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.9293\n\nEpoch 00014: loss did not improve from 2.92781\nEpoch 15/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.9054\n\nEpoch 00015: loss improved from 2.92781 to 2.90543, saving model to weights/weights_epoch_015_loss_2.9054.hdf5\nEpoch 16/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.9076\n\nEpoch 00016: loss did not improve from 2.90543\nEpoch 17/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.8816\n\nEpoch 00017: loss improved from 2.90543 to 2.88157, saving model to weights/weights_epoch_017_loss_2.8816.hdf5\nEpoch 18/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.8797\n\nEpoch 00018: loss improved from 2.88157 to 2.87971, saving model to weights/weights_epoch_018_loss_2.8797.hdf5\nEpoch 19/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.8558\n\nEpoch 00019: loss improved from 2.87971 to 2.85579, saving model to weights/weights_epoch_019_loss_2.8558.hdf5\nEpoch 20/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.8325\n\nEpoch 00020: loss improved from 2.85579 to 2.83251, saving model to weights/weights_epoch_020_loss_2.8325.hdf5\nEpoch 21/301\n4/4 [==============================] - 1s 139ms/step - loss: 2.8003\n\nMy Roodaki colection:\nد  ا  ن ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  ا  ن ا  \n\nEpoch 00021: loss improved from 2.83251 to 2.80032, saving model to weights/weights_epoch_021_loss_2.8003.hdf5\nEpoch 22/301\n4/4 [==============================] - 1s 136ms/step - loss: 2.8013\n\nEpoch 00022: loss did not improve from 2.80032\nEpoch 23/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.7715\n\nEpoch 00023: loss improved from 2.80032 to 2.77151, saving model to weights/weights_epoch_023_loss_2.7715.hdf5\nEpoch 24/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.7660\n\nEpoch 00024: loss improved from 2.77151 to 2.76598, saving model to weights/weights_epoch_024_loss_2.7660.hdf5\nEpoch 25/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.7571\n\nEpoch 00025: loss improved from 2.76598 to 2.75710, saving model to weights/weights_epoch_025_loss_2.7571.hdf5\nEpoch 26/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.7389\n\nEpoch 00026: loss improved from 2.75710 to 2.73890, saving model to weights/weights_epoch_026_loss_2.7389.hdf5\nEpoch 27/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.7229\n\nEpoch 00027: loss improved from 2.73890 to 2.72295, saving model to weights/weights_epoch_027_loss_2.7229.hdf5\nEpoch 28/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.7336\n\nEpoch 00028: loss did not improve from 2.72295\nEpoch 29/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.7272\n\nEpoch 00029: loss did not improve from 2.72295\nEpoch 30/301\n4/4 [==============================] - 1s 162ms/step - loss: 2.7074\n\nEpoch 00030: loss improved from 2.72295 to 2.70738, saving model to weights/weights_epoch_030_loss_2.7074.hdf5\nEpoch 31/301\n4/4 [==============================] - 1s 139ms/step - loss: 2.6981\n\nMy Roodaki colection:\nصر ان با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با \n\nEpoch 00031: loss improved from 2.70738 to 2.69813, saving model to weights/weights_epoch_031_loss_2.6981.hdf5\nEpoch 32/301\n4/4 [==============================] - 1s 136ms/step - loss: 2.6967\n\nEpoch 00032: loss improved from 2.69813 to 2.69669, saving model to weights/weights_epoch_032_loss_2.6967.hdf5\nEpoch 33/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.6903\n\nEpoch 00033: loss improved from 2.69669 to 2.69025, saving model to weights/weights_epoch_033_loss_2.6903.hdf5\nEpoch 34/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.7080\n\nEpoch 00034: loss did not improve from 2.69025\nEpoch 35/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.6756\n\nEpoch 00035: loss improved from 2.69025 to 2.67556, saving model to weights/weights_epoch_035_loss_2.6756.hdf5\nEpoch 36/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.6566\n\nEpoch 00036: loss improved from 2.67556 to 2.65664, saving model to weights/weights_epoch_036_loss_2.6566.hdf5\nEpoch 37/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.6617\n\nEpoch 00037: loss did not improve from 2.65664\nEpoch 38/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.6524\n\nEpoch 00038: loss improved from 2.65664 to 2.65237, saving model to weights/weights_epoch_038_loss_2.6524.hdf5\nEpoch 39/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.6467\n\nEpoch 00039: loss improved from 2.65237 to 2.64672, saving model to weights/weights_epoch_039_loss_2.6467.hdf5\nEpoch 40/301\n4/4 [==============================] - 1s 144ms/step - loss: 2.6601\n\nEpoch 00040: loss did not improve from 2.64672\nEpoch 41/301\n4/4 [==============================] - 1s 144ms/step - loss: 2.6446\n\nMy Roodaki colection:\n\nار با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با\n\nEpoch 00041: loss improved from 2.64672 to 2.64458, saving model to weights/weights_epoch_041_loss_2.6446.hdf5\nEpoch 42/301\n4/4 [==============================] - 1s 130ms/step - loss: 2.6591\n\nEpoch 00042: loss did not improve from 2.64458\nEpoch 43/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.6387\n\nEpoch 00043: loss improved from 2.64458 to 2.63867, saving model to weights/weights_epoch_043_loss_2.6387.hdf5\nEpoch 44/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.6277\n\nEpoch 00044: loss improved from 2.63867 to 2.62768, saving model to weights/weights_epoch_044_loss_2.6277.hdf5\nEpoch 45/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.6170\n\nEpoch 00045: loss improved from 2.62768 to 2.61698, saving model to weights/weights_epoch_045_loss_2.6170.hdf5\nEpoch 46/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.6222\n\nEpoch 00046: loss did not improve from 2.61698\nEpoch 47/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.6203\n\nEpoch 00047: loss did not improve from 2.61698\nEpoch 48/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.6269\n\nEpoch 00048: loss did not improve from 2.61698\nEpoch 49/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.6157\n\nEpoch 00049: loss improved from 2.61698 to 2.61572, saving model to weights/weights_epoch_049_loss_2.6157.hdf5\nEpoch 50/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.6064\n\nEpoch 00050: loss improved from 2.61572 to 2.60641, saving model to weights/weights_epoch_050_loss_2.6064.hdf5\nEpoch 51/301\n4/4 [==============================] - 1s 144ms/step - loss: 2.6056\n\nMy Roodaki colection:\nز بر با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با ب\n\nEpoch 00051: loss improved from 2.60641 to 2.60561, saving model to weights/weights_epoch_051_loss_2.6056.hdf5\nEpoch 52/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.6010\n\nEpoch 00052: loss improved from 2.60561 to 2.60102, saving model to weights/weights_epoch_052_loss_2.6010.hdf5\nEpoch 53/301\n4/4 [==============================] - 1s 140ms/step - loss: 2.5961\n\nEpoch 00053: loss improved from 2.60102 to 2.59613, saving model to weights/weights_epoch_053_loss_2.5961.hdf5\nEpoch 54/301\n4/4 [==============================] - 1s 130ms/step - loss: 2.5977\n\nEpoch 00054: loss did not improve from 2.59613\nEpoch 55/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.5970\n\nEpoch 00055: loss did not improve from 2.59613\nEpoch 56/301\n4/4 [==============================] - 1s 130ms/step - loss: 2.5845\n\nEpoch 00056: loss improved from 2.59613 to 2.58446, saving model to weights/weights_epoch_056_loss_2.5845.hdf5\nEpoch 57/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5868\n\nEpoch 00057: loss did not improve from 2.58446\nEpoch 58/301\n4/4 [==============================] - 1s 130ms/step - loss: 2.5815\n\nEpoch 00058: loss improved from 2.58446 to 2.58153, saving model to weights/weights_epoch_058_loss_2.5815.hdf5\nEpoch 59/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.5768\n\nEpoch 00059: loss improved from 2.58153 to 2.57683, saving model to weights/weights_epoch_059_loss_2.5768.hdf5\nEpoch 60/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.5773\n\nEpoch 00060: loss did not improve from 2.57683\nEpoch 61/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.5682\n\nMy Roodaki colection:\nو بر در به باد برد باد برد باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد بر باد ب\n\nEpoch 00061: loss improved from 2.57683 to 2.56823, saving model to weights/weights_epoch_061_loss_2.5682.hdf5\nEpoch 62/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5680\n\nEpoch 00062: loss improved from 2.56823 to 2.56800, saving model to weights/weights_epoch_062_loss_2.5680.hdf5\nEpoch 63/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.5651\n\nEpoch 00063: loss improved from 2.56800 to 2.56507, saving model to weights/weights_epoch_063_loss_2.5651.hdf5\nEpoch 64/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.5578\n\nEpoch 00064: loss improved from 2.56507 to 2.55781, saving model to weights/weights_epoch_064_loss_2.5578.hdf5\nEpoch 65/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5623\n\nEpoch 00065: loss did not improve from 2.55781\nEpoch 66/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.5603\n\nEpoch 00066: loss did not improve from 2.55781\nEpoch 67/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5548\n\nEpoch 00067: loss improved from 2.55781 to 2.55484, saving model to weights/weights_epoch_067_loss_2.5548.hdf5\nEpoch 68/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.5479\n\nEpoch 00068: loss improved from 2.55484 to 2.54790, saving model to weights/weights_epoch_068_loss_2.5479.hdf5\nEpoch 69/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.5489\n\nEpoch 00069: loss did not improve from 2.54790\nEpoch 70/301\n4/4 [==============================] - 1s 138ms/step - loss: 2.5435\n\nEpoch 00070: loss improved from 2.54790 to 2.54353, saving model to weights/weights_epoch_070_loss_2.5435.hdf5\nEpoch 71/301\n4/4 [==============================] - 1s 141ms/step - loss: 2.5396\n\nMy Roodaki colection:\nفر به با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با \n\nEpoch 00071: loss improved from 2.54353 to 2.53956, saving model to weights/weights_epoch_071_loss_2.5396.hdf5\nEpoch 72/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.5305\n\nEpoch 00072: loss improved from 2.53956 to 2.53054, saving model to weights/weights_epoch_072_loss_2.5305.hdf5\nEpoch 73/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5270\n\nEpoch 00073: loss improved from 2.53054 to 2.52699, saving model to weights/weights_epoch_073_loss_2.5270.hdf5\nEpoch 74/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5387\n\nEpoch 00074: loss did not improve from 2.52699\nEpoch 75/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.5273\n\nEpoch 00075: loss did not improve from 2.52699\nEpoch 76/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5230\n\nEpoch 00076: loss improved from 2.52699 to 2.52305, saving model to weights/weights_epoch_076_loss_2.5230.hdf5\nEpoch 77/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5262\n\nEpoch 00077: loss did not improve from 2.52305\nEpoch 78/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.5157\n\nEpoch 00078: loss improved from 2.52305 to 2.51568, saving model to weights/weights_epoch_078_loss_2.5157.hdf5\nEpoch 79/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5151\n\nEpoch 00079: loss improved from 2.51568 to 2.51515, saving model to weights/weights_epoch_079_loss_2.5151.hdf5\nEpoch 80/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.5136\n\nEpoch 00080: loss improved from 2.51515 to 2.51360, saving model to weights/weights_epoch_080_loss_2.5136.hdf5\nEpoch 81/301\n4/4 [==============================] - 1s 145ms/step - loss: 2.5036\n\nMy Roodaki colection:\n\nان را به با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با\n\nEpoch 00081: loss improved from 2.51360 to 2.50364, saving model to weights/weights_epoch_081_loss_2.5036.hdf5\nEpoch 82/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.5097\n\nEpoch 00082: loss did not improve from 2.50364\nEpoch 83/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.5065\n\nEpoch 00083: loss did not improve from 2.50364\nEpoch 84/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.4953\n\nEpoch 00084: loss improved from 2.50364 to 2.49527, saving model to weights/weights_epoch_084_loss_2.4953.hdf5\nEpoch 85/301\n4/4 [==============================] - 1s 139ms/step - loss: 2.4878\n\nEpoch 00085: loss improved from 2.49527 to 2.48782, saving model to weights/weights_epoch_085_loss_2.4878.hdf5\nEpoch 86/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.4843\n\nEpoch 00086: loss improved from 2.48782 to 2.48433, saving model to weights/weights_epoch_086_loss_2.4843.hdf5\nEpoch 87/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4961\n\nEpoch 00087: loss did not improve from 2.48433\nEpoch 88/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.4858\n\nEpoch 00088: loss did not improve from 2.48433\nEpoch 89/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4802\n\nEpoch 00089: loss improved from 2.48433 to 2.48016, saving model to weights/weights_epoch_089_loss_2.4802.hdf5\nEpoch 90/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4838\n\nEpoch 00090: loss did not improve from 2.48016\nEpoch 91/301\n4/4 [==============================] - 1s 145ms/step - loss: 2.4835\n\nMy Roodaki colection:\nض و باد و به دود\nباد و به باد و به باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود و باد\nباد به دود و به دود \n\nEpoch 00091: loss did not improve from 2.48016\nEpoch 92/301\n4/4 [==============================] - 1s 141ms/step - loss: 2.4760\n\nEpoch 00092: loss improved from 2.48016 to 2.47602, saving model to weights/weights_epoch_092_loss_2.4760.hdf5\nEpoch 93/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.4748\n\nEpoch 00093: loss improved from 2.47602 to 2.47476, saving model to weights/weights_epoch_093_loss_2.4748.hdf5\nEpoch 94/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4649\n\nEpoch 00094: loss improved from 2.47476 to 2.46494, saving model to weights/weights_epoch_094_loss_2.4649.hdf5\nEpoch 95/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4678\n\nEpoch 00095: loss did not improve from 2.46494\nEpoch 96/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.4662\n\nEpoch 00096: loss did not improve from 2.46494\nEpoch 97/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.4655\n\nEpoch 00097: loss did not improve from 2.46494\nEpoch 98/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.4611\n\nEpoch 00098: loss improved from 2.46494 to 2.46107, saving model to weights/weights_epoch_098_loss_2.4611.hdf5\nEpoch 99/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.4553\n\nEpoch 00099: loss improved from 2.46107 to 2.45526, saving model to weights/weights_epoch_099_loss_2.4553.hdf5\nEpoch 100/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4616\n\nEpoch 00100: loss did not improve from 2.45526\nEpoch 101/301\n4/4 [==============================] - 1s 140ms/step - loss: 2.4590\n\nMy Roodaki colection:\nژ و با که با کا به با کا به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر به با کر\n\nEpoch 00101: loss did not improve from 2.45526\nEpoch 102/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.4480\n\nEpoch 00102: loss improved from 2.45526 to 2.44796, saving model to weights/weights_epoch_102_loss_2.4480.hdf5\nEpoch 103/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4400\n\nEpoch 00103: loss improved from 2.44796 to 2.44002, saving model to weights/weights_epoch_103_loss_2.4400.hdf5\nEpoch 104/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.4467\n\nEpoch 00104: loss did not improve from 2.44002\nEpoch 105/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.4414\n\nEpoch 00105: loss did not improve from 2.44002\nEpoch 106/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.4374\n\nEpoch 00106: loss improved from 2.44002 to 2.43736, saving model to weights/weights_epoch_106_loss_2.4374.hdf5\nEpoch 107/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4386\n\nEpoch 00107: loss did not improve from 2.43736\nEpoch 108/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.4398\n\nEpoch 00108: loss did not improve from 2.43736\nEpoch 109/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.4410\n\nEpoch 00109: loss did not improve from 2.43736\nEpoch 110/301\n4/4 [==============================] - 1s 136ms/step - loss: 2.4300\n\nEpoch 00110: loss improved from 2.43736 to 2.42998, saving model to weights/weights_epoch_110_loss_2.4300.hdf5\nEpoch 111/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4324\n\nMy Roodaki colection:\nل و بر کرد بر در بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر بر ب\n\nEpoch 00111: loss did not improve from 2.42998\nEpoch 112/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.4273\n\nEpoch 00112: loss improved from 2.42998 to 2.42728, saving model to weights/weights_epoch_112_loss_2.4273.hdf5\nEpoch 113/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4256\n\nEpoch 00113: loss improved from 2.42728 to 2.42557, saving model to weights/weights_epoch_113_loss_2.4256.hdf5\nEpoch 114/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.4183\n\nEpoch 00114: loss improved from 2.42557 to 2.41835, saving model to weights/weights_epoch_114_loss_2.4183.hdf5\nEpoch 115/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.4183\n\nEpoch 00115: loss improved from 2.41835 to 2.41827, saving model to weights/weights_epoch_115_loss_2.4183.hdf5\nEpoch 116/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.4253\n\nEpoch 00116: loss did not improve from 2.41827\nEpoch 117/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.4083\n\nEpoch 00117: loss improved from 2.41827 to 2.40833, saving model to weights/weights_epoch_117_loss_2.4083.hdf5\nEpoch 118/301\n4/4 [==============================] - 1s 130ms/step - loss: 2.4011\n\nEpoch 00118: loss improved from 2.40833 to 2.40108, saving model to weights/weights_epoch_118_loss_2.4011.hdf5\nEpoch 119/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.4072\n\nEpoch 00119: loss did not improve from 2.40108\nEpoch 120/301\n4/4 [==============================] - 1s 130ms/step - loss: 2.4190\n\nEpoch 00120: loss did not improve from 2.40108\nEpoch 121/301\n4/4 [==============================] - 1s 140ms/step - loss: 2.4036\n\nMy Roodaki colection:\nغ بر از به با با با با باد با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با با \n\nEpoch 00121: loss did not improve from 2.40108\nEpoch 122/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.3990\n\nEpoch 00122: loss improved from 2.40108 to 2.39897, saving model to weights/weights_epoch_122_loss_2.3990.hdf5\nEpoch 123/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3965\n\nEpoch 00123: loss improved from 2.39897 to 2.39647, saving model to weights/weights_epoch_123_loss_2.3965.hdf5\nEpoch 124/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.3861\n\nEpoch 00124: loss improved from 2.39647 to 2.38608, saving model to weights/weights_epoch_124_loss_2.3861.hdf5\nEpoch 125/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3943\n\nEpoch 00125: loss did not improve from 2.38608\nEpoch 126/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.3882\n\nEpoch 00126: loss did not improve from 2.38608\nEpoch 127/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3763\n\nEpoch 00127: loss improved from 2.38608 to 2.37630, saving model to weights/weights_epoch_127_loss_2.3763.hdf5\nEpoch 128/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3927\n\nEpoch 00128: loss did not improve from 2.37630\nEpoch 129/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.3947\n\nEpoch 00129: loss did not improve from 2.37630\nEpoch 130/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3718\n\nEpoch 00130: loss improved from 2.37630 to 2.37178, saving model to weights/weights_epoch_130_loss_2.3718.hdf5\nEpoch 131/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.3808\n\nMy Roodaki colection:\nی و بر او بر بر ان را به بر از بر بر بر از بر بر از بر بر بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر بر از بر\n\nEpoch 00131: loss did not improve from 2.37178\nEpoch 132/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.3787\n\nEpoch 00132: loss did not improve from 2.37178\nEpoch 133/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3750\n\nEpoch 00133: loss did not improve from 2.37178\nEpoch 134/301\n4/4 [==============================] - 1s 130ms/step - loss: 2.3663\n\nEpoch 00134: loss improved from 2.37178 to 2.36629, saving model to weights/weights_epoch_134_loss_2.3663.hdf5\nEpoch 135/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.3610\n\nEpoch 00135: loss improved from 2.36629 to 2.36101, saving model to weights/weights_epoch_135_loss_2.3610.hdf5\nEpoch 136/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.3717\n\nEpoch 00136: loss did not improve from 2.36101\nEpoch 137/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.3587\n\nEpoch 00137: loss improved from 2.36101 to 2.35874, saving model to weights/weights_epoch_137_loss_2.3587.hdf5\nEpoch 138/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.3607\n\nEpoch 00138: loss did not improve from 2.35874\nEpoch 139/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.3608\n\nEpoch 00139: loss did not improve from 2.35874\nEpoch 140/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.3600\n\nEpoch 00140: loss did not improve from 2.35874\nEpoch 141/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.3475\n\nMy Roodaki colection:\nت بر کرد به دان با دل به دار با در به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به دار با در تا به\n\nEpoch 00141: loss improved from 2.35874 to 2.34749, saving model to weights/weights_epoch_141_loss_2.3475.hdf5\nEpoch 142/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3460\n\nEpoch 00142: loss improved from 2.34749 to 2.34603, saving model to weights/weights_epoch_142_loss_2.3460.hdf5\nEpoch 143/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3516\n\nEpoch 00143: loss did not improve from 2.34603\nEpoch 144/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3311\n\nEpoch 00144: loss improved from 2.34603 to 2.33113, saving model to weights/weights_epoch_144_loss_2.3311.hdf5\nEpoch 145/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3298\n\nEpoch 00145: loss improved from 2.33113 to 2.32984, saving model to weights/weights_epoch_145_loss_2.3298.hdf5\nEpoch 146/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3364\n\nEpoch 00146: loss did not improve from 2.32984\nEpoch 147/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.3384\n\nEpoch 00147: loss did not improve from 2.32984\nEpoch 148/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.3248\n\nEpoch 00148: loss improved from 2.32984 to 2.32481, saving model to weights/weights_epoch_148_loss_2.3248.hdf5\nEpoch 149/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.3291\n\nEpoch 00149: loss did not improve from 2.32481\nEpoch 150/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.3231\n\nEpoch 00150: loss improved from 2.32481 to 2.32310, saving model to weights/weights_epoch_150_loss_2.3231.hdf5\nEpoch 151/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3126\n\nMy Roodaki colection:\nض و آن جام تو کرد\nکه باش اندر نیارد بر ان\nباد ان را به روی تو باری\nای خویش نیست و نی به تو بای\nبا خوش آن روی و باد و باد و بیر\nبا خوش آن که نیان و به نام نی\nبه خویش نیست و نی به تو باد\nبا خوش آن که نیان و به نام نی\nبه خوش آن روی و باد و باد\nبا به را نی به نیان و به نیان\nباد ان روی او نام و باد و بیر\nبا بر کن روی و باد و باد و بیر\nبا خوش آن روی و باد و باد\nبا بر کرد را به نیان آرد\nهمی خویش ن را نیاند به خور\nمی خویش ن خواه از خوش و خویش\nنی تو خوش و روی و باد و باد\nبا به خوش آن روی و باد و باد\nبا بر\n\nEpoch 00151: loss improved from 2.32310 to 2.31264, saving model to weights/weights_epoch_151_loss_2.3126.hdf5\nEpoch 152/301\n4/4 [==============================] - 1s 137ms/step - loss: 2.3228\n\nEpoch 00152: loss did not improve from 2.31264\nEpoch 153/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3160\n\nEpoch 00153: loss did not improve from 2.31264\nEpoch 154/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3129\n\nEpoch 00154: loss did not improve from 2.31264\nEpoch 155/301\n4/4 [==============================] - 1s 137ms/step - loss: 2.3162\n\nEpoch 00155: loss did not improve from 2.31264\nEpoch 156/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.3058\n\nEpoch 00156: loss improved from 2.31264 to 2.30578, saving model to weights/weights_epoch_156_loss_2.3058.hdf5\nEpoch 157/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.2966\n\nEpoch 00157: loss improved from 2.30578 to 2.29664, saving model to weights/weights_epoch_157_loss_2.2966.hdf5\nEpoch 158/301\n4/4 [==============================] - 1s 130ms/step - loss: 2.3084\n\nEpoch 00158: loss did not improve from 2.29664\nEpoch 159/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.3048\n\nEpoch 00159: loss did not improve from 2.29664\nEpoch 160/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.2903\n\nEpoch 00160: loss improved from 2.29664 to 2.29030, saving model to weights/weights_epoch_160_loss_2.2903.hdf5\nEpoch 161/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.2856\n\nMy Roodaki colection:\nق به کن بر بر به نیم و به باد\nبا بر که به می به من باند بود\nبه آن که به نیم و به مان بید و باد\nبا به باد و به باد و به نیک و باد\nبا به نیز و به نام و به نیک و باد\nبا بو کن که به نیک و باد و بود\nبا به نی به باد و به نیک و باد\nبا به نیز و به نیک و باد بود\nبا به نی به باد و به نیک و باد\nبا به نیز و به نیک و به نیک و باد\nبا بو کن که به نیک و باد و بود\nبا به که به نیز آن که باد و بود\nبا به که با اندر به داد و به دو نیان\nبه نی به بی به نیک و باد و بود\nبا به نیک و باد و به نیک و باد\nبا به نیز و به نیک و\n\nEpoch 00161: loss improved from 2.29030 to 2.28558, saving model to weights/weights_epoch_161_loss_2.2856.hdf5\nEpoch 162/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2836\n\nEpoch 00162: loss improved from 2.28558 to 2.28363, saving model to weights/weights_epoch_162_loss_2.2836.hdf5\nEpoch 163/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.2806\n\nEpoch 00163: loss improved from 2.28363 to 2.28059, saving model to weights/weights_epoch_163_loss_2.2806.hdf5\nEpoch 164/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.2791\n\nEpoch 00164: loss improved from 2.28059 to 2.27913, saving model to weights/weights_epoch_164_loss_2.2791.hdf5\nEpoch 165/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.2705\n\nEpoch 00165: loss improved from 2.27913 to 2.27050, saving model to weights/weights_epoch_165_loss_2.2705.hdf5\nEpoch 166/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2722\n\nEpoch 00166: loss did not improve from 2.27050\nEpoch 167/301\n4/4 [==============================] - 1s 142ms/step - loss: 2.2693\n\nEpoch 00167: loss improved from 2.27050 to 2.26926, saving model to weights/weights_epoch_167_loss_2.2693.hdf5\nEpoch 168/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.2633\n\nEpoch 00168: loss improved from 2.26926 to 2.26327, saving model to weights/weights_epoch_168_loss_2.2633.hdf5\nEpoch 169/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.2685\n\nEpoch 00169: loss did not improve from 2.26327\nEpoch 170/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.2605\n\nEpoch 00170: loss improved from 2.26327 to 2.26047, saving model to weights/weights_epoch_170_loss_2.2605.hdf5\nEpoch 171/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2549\n\nMy Roodaki colection:\nت بر کرد به مر بر مرد به در به را به در من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به من به\n\nEpoch 00171: loss improved from 2.26047 to 2.25491, saving model to weights/weights_epoch_171_loss_2.2549.hdf5\nEpoch 172/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2495\n\nEpoch 00172: loss improved from 2.25491 to 2.24952, saving model to weights/weights_epoch_172_loss_2.2495.hdf5\nEpoch 173/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2555\n\nEpoch 00173: loss did not improve from 2.24952\nEpoch 174/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2455\n\nEpoch 00174: loss improved from 2.24952 to 2.24553, saving model to weights/weights_epoch_174_loss_2.2455.hdf5\nEpoch 175/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.2349\n\nEpoch 00175: loss improved from 2.24553 to 2.23486, saving model to weights/weights_epoch_175_loss_2.2349.hdf5\nEpoch 176/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.2288\n\nEpoch 00176: loss improved from 2.23486 to 2.22884, saving model to weights/weights_epoch_176_loss_2.2288.hdf5\nEpoch 177/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2370\n\nEpoch 00177: loss did not improve from 2.22884\nEpoch 178/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2296\n\nEpoch 00178: loss did not improve from 2.22884\nEpoch 179/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2186\n\nEpoch 00179: loss improved from 2.22884 to 2.21855, saving model to weights/weights_epoch_179_loss_2.2186.hdf5\nEpoch 180/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.2288\n\nEpoch 00180: loss did not improve from 2.21855\nEpoch 181/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.2143\n\nMy Roodaki colection:\nژ و پاه بود\nآن که با مان و بود و به نام\nای به خان و بود و باد و به نام\nبا در آن که نود و باد و نه نام\nنی به خان و بود و باد و بان\nبا به نام و بود و آن به نیان\nباد به نام و بود و باد و نه نام\nنی به خان و نود و باد و به نام\nنی به نام و بود و نه نام نیان\nباد به نام و بود و باد و نه نام\nنی به خان و نود و باد و به نام\nنی به نام و بود و نه نام نیان\nباد به نام و بود و باد و نه نام\nنی به خان و نود و باد و به نام\nنی به نام و بود و نه نام نیان\nباد به نام و بود و باد و نه نام\nنی به خان و نود و باد و به نام\n\n\nEpoch 00181: loss improved from 2.21855 to 2.21435, saving model to weights/weights_epoch_181_loss_2.2143.hdf5\nEpoch 182/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.2161\n\nEpoch 00182: loss did not improve from 2.21435\nEpoch 183/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.2173\n\nEpoch 00183: loss did not improve from 2.21435\nEpoch 184/301\n4/4 [==============================] - 1s 131ms/step - loss: 2.2061\n\nEpoch 00184: loss improved from 2.21435 to 2.20611, saving model to weights/weights_epoch_184_loss_2.2061.hdf5\nEpoch 185/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.1942\n\nEpoch 00185: loss improved from 2.20611 to 2.19421, saving model to weights/weights_epoch_185_loss_2.1942.hdf5\nEpoch 186/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.1984\n\nEpoch 00186: loss did not improve from 2.19421\nEpoch 187/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1966\n\nEpoch 00187: loss did not improve from 2.19421\nEpoch 188/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.1834\n\nEpoch 00188: loss improved from 2.19421 to 2.18342, saving model to weights/weights_epoch_188_loss_2.1834.hdf5\nEpoch 189/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1978\n\nEpoch 00189: loss did not improve from 2.18342\nEpoch 190/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1761\n\nEpoch 00190: loss improved from 2.18342 to 2.17606, saving model to weights/weights_epoch_190_loss_2.1761.hdf5\nEpoch 191/301\n4/4 [==============================] - 1s 137ms/step - loss: 2.1822\n\nMy Roodaki colection:\nه به کار بر در چه با د دان\nبه به با گرد به دو روی به دو گرد\nبه بانگ به باد و به بانگ به نوی\nبه نبان که به نین و به جان و به نو ن\nبه که به گشت و به بانگ به نوی\nبا به نه به به نیک با دو به نیان\nبه که باشد به دو با گرد با در به نام نیابد به دانگ به بود\nبه به نام تو به نبین و به نیم\nبه نین آن نه نه به کو ن به دو نیان بود\nشد آن زمانه که او را که باشد بود\nبا در که نیان و نه به نین و به نیان\nبه نیان که به نین و به جان و به نیان\nبا دو روی مو به نام نیک و نه دو نیان بود\nهمیشه شاد و نه دو نی و به جان من به\n\nEpoch 00191: loss did not improve from 2.17606\nEpoch 192/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.1704\n\nEpoch 00192: loss improved from 2.17606 to 2.17041, saving model to weights/weights_epoch_192_loss_2.1704.hdf5\nEpoch 193/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.1692\n\nEpoch 00193: loss improved from 2.17041 to 2.16921, saving model to weights/weights_epoch_193_loss_2.1692.hdf5\nEpoch 194/301\n4/4 [==============================] - 1s 136ms/step - loss: 2.1563\n\nEpoch 00194: loss improved from 2.16921 to 2.15632, saving model to weights/weights_epoch_194_loss_2.1563.hdf5\nEpoch 195/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.1656\n\nEpoch 00195: loss did not improve from 2.15632\nEpoch 196/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1553\n\nEpoch 00196: loss improved from 2.15632 to 2.15531, saving model to weights/weights_epoch_196_loss_2.1553.hdf5\nEpoch 197/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.1430\n\nEpoch 00197: loss improved from 2.15531 to 2.14302, saving model to weights/weights_epoch_197_loss_2.1430.hdf5\nEpoch 198/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1447\n\nEpoch 00198: loss did not improve from 2.14302\nEpoch 199/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1481\n\nEpoch 00199: loss did not improve from 2.14302\nEpoch 200/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.1318\n\nEpoch 00200: loss improved from 2.14302 to 2.13181, saving model to weights/weights_epoch_200_loss_2.1318.hdf5\nEpoch 201/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1250\n\nMy Roodaki colection:\nک و خواهی ستا سان کرد با بر کرد روی بر در ان به نیان\nکه با در آن که نود آو که به نام نیاب\nکا بر که به نام نیابد بر در تو را به نام کن می نه که او نه ز اندان بود\nبه آن که که می می من از می به خوان\nبا من که او نام که ماه می به خور\nبا در که ای که میان که ماد با دل تو با خواهی که می که می به کند او به نیان کرد\nهمیشه شاد او به نیان آید همی\nبه جان مرا و جوی می من از به همه میام\nکه می به جهان مر از این جهان بود\nهمیشه شاد ان را نیان آیدان\nبود\nبه من از ای میان آمد میان\nبود به آن که می می می من من بود\nبه آ\n\nEpoch 00201: loss improved from 2.13181 to 2.12502, saving model to weights/weights_epoch_201_loss_2.1250.hdf5\nEpoch 202/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.1237\n\nEpoch 00202: loss improved from 2.12502 to 2.12368, saving model to weights/weights_epoch_202_loss_2.1237.hdf5\nEpoch 203/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1445\n\nEpoch 00203: loss did not improve from 2.12368\nEpoch 204/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1155\n\nEpoch 00204: loss improved from 2.12368 to 2.11545, saving model to weights/weights_epoch_204_loss_2.1155.hdf5\nEpoch 205/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.1104\n\nEpoch 00205: loss improved from 2.11545 to 2.11035, saving model to weights/weights_epoch_205_loss_2.1104.hdf5\nEpoch 206/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.1136\n\nEpoch 00206: loss did not improve from 2.11035\nEpoch 207/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.0999\n\nEpoch 00207: loss improved from 2.11035 to 2.09992, saving model to weights/weights_epoch_207_loss_2.0999.hdf5\nEpoch 208/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.0884\n\nEpoch 00208: loss improved from 2.09992 to 2.08837, saving model to weights/weights_epoch_208_loss_2.0884.hdf5\nEpoch 209/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.1007\n\nEpoch 00209: loss did not improve from 2.08837\nEpoch 210/301\n4/4 [==============================] - 1s 136ms/step - loss: 2.0918\n\nEpoch 00210: loss did not improve from 2.08837\nEpoch 211/301\n4/4 [==============================] - 1s 138ms/step - loss: 2.0869\n\nMy Roodaki colection:\nجان که مر مرد و نه دانی به کار اندر نهان به کل اندر نهان بود\nبه کن را که بیاند به کار مندر به نیان\nکه با داد و به دان و به نام نیست\nبه حان تو کو بود و به نامت نامت\nبه حان تو کو بود با همه نامت\nبه حان تو که بر نداند به کار\nتا به نیک و به کند و به نام\nتا به خانه بود و به سان و به تو با نیاند این میان که به نام و بود\nبا دو را که نیاند به داد و بود\nبه نیز که نیاند که دو نود و بود\nنیان تو نه نیاند که دو نو ند\nداد من به نیان و نه دو نو و بود\nشد آن زمانه که او شاع ان نان بود\nشد آن زمانه که او شاد به ناد\n\nEpoch 00211: loss improved from 2.08837 to 2.08689, saving model to weights/weights_epoch_211_loss_2.0869.hdf5\nEpoch 212/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.0796\n\nEpoch 00212: loss improved from 2.08689 to 2.07960, saving model to weights/weights_epoch_212_loss_2.0796.hdf5\nEpoch 213/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.0765\n\nEpoch 00213: loss improved from 2.07960 to 2.07646, saving model to weights/weights_epoch_213_loss_2.0765.hdf5\nEpoch 214/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.0609\n\nEpoch 00214: loss improved from 2.07646 to 2.06089, saving model to weights/weights_epoch_214_loss_2.0609.hdf5\nEpoch 215/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.0735\n\nEpoch 00215: loss did not improve from 2.06089\nEpoch 216/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.0668\n\nEpoch 00216: loss did not improve from 2.06089\nEpoch 217/301\n4/4 [==============================] - 1s 135ms/step - loss: 2.0630\n\nEpoch 00217: loss did not improve from 2.06089\nEpoch 218/301\n4/4 [==============================] - 1s 142ms/step - loss: 2.0350\n\nEpoch 00218: loss improved from 2.06089 to 2.03495, saving model to weights/weights_epoch_218_loss_2.0350.hdf5\nEpoch 219/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.0423\n\nEpoch 00219: loss did not improve from 2.03495\nEpoch 220/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.0580\n\nEpoch 00220: loss did not improve from 2.03495\nEpoch 221/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.0467\n\nMy Roodaki colection:\nطر ز به را بر در آن خوار\nبه خوش گرد با درد اندر پی خاد\nآن که به از اندر نه دید ر خوان\nما به خانه به جای خویش و غیدان\nور تو بر ار افت و آن په آن\nمن و آن که از دوار آید همی\nآن که به از از درم خام بر خوان\nمر تو کار و به جای من او ناد\nهم آن که از این دلم خام آمد\nباد\nبه هر که به مای ماد آمد آر به خوار\nاز در سرخ اندر میان آید همی\nآن که به از از درم خام بر خوان\nمر تو کار و به جای مندان بود\nبد آن زمانه که او شاعر اندر نبود\nبد آن زمانه که او شاد به جان خود بود\nشد آن زمانه که او شاد به سان نشود بود\nشد آن ز\n\nEpoch 00221: loss did not improve from 2.03495\nEpoch 222/301\n4/4 [==============================] - 1s 132ms/step - loss: 2.0353\n\nEpoch 00222: loss did not improve from 2.03495\nEpoch 223/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.0363\n\nEpoch 00223: loss did not improve from 2.03495\nEpoch 224/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.0156\n\nEpoch 00224: loss improved from 2.03495 to 2.01558, saving model to weights/weights_epoch_224_loss_2.0156.hdf5\nEpoch 225/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.0060\n\nEpoch 00225: loss improved from 2.01558 to 2.00600, saving model to weights/weights_epoch_225_loss_2.0060.hdf5\nEpoch 226/301\n4/4 [==============================] - 1s 144ms/step - loss: 2.0035\n\nEpoch 00226: loss improved from 2.00600 to 2.00349, saving model to weights/weights_epoch_226_loss_2.0035.hdf5\nEpoch 227/301\n4/4 [==============================] - 1s 134ms/step - loss: 2.0020\n\nEpoch 00227: loss improved from 2.00349 to 2.00202, saving model to weights/weights_epoch_227_loss_2.0020.hdf5\nEpoch 228/301\n4/4 [==============================] - 1s 133ms/step - loss: 2.0017\n\nEpoch 00228: loss improved from 2.00202 to 2.00168, saving model to weights/weights_epoch_228_loss_2.0017.hdf5\nEpoch 229/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.9909\n\nEpoch 00229: loss improved from 2.00168 to 1.99090, saving model to weights/weights_epoch_229_loss_1.9909.hdf5\nEpoch 230/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.9687\n\nEpoch 00230: loss improved from 1.99090 to 1.96872, saving model to weights/weights_epoch_230_loss_1.9687.hdf5\nEpoch 231/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.9732\n\nMy Roodaki colection:\nان بید سوان\nخواجه کن مرد و به جام ای به خوی\nآن کن که ای نیارد که در تو به دید\nهمیشه چشمم بر از فر و پیدان بود\nهمیشه شاد و نا این جهان مرد خویش\nآن خواهی که باد با دل تو را که باد\nتا به نیچ و باد باد و به جای خویش\nن گویی اندر نیان با دل تو تا تو بد نی به بیان کا به داد باد\nبا در تا به جان تو با درد و بین\nتا به نیچ و با دلم بی دان تو به نیانی به نیم خواب کرد\nسویی تو باد و به باد و به دل تو به خان گشت و نا ه باد و بی تو به نام تو به نیاند بود\nکه به آن که بر ناخت بر درد و به داد\nبا روز تو چون بر در او\n\nEpoch 00231: loss did not improve from 1.96872\nEpoch 232/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.9821\n\nEpoch 00232: loss did not improve from 1.96872\nEpoch 233/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.9610\n\nEpoch 00233: loss improved from 1.96872 to 1.96097, saving model to weights/weights_epoch_233_loss_1.9610.hdf5\nEpoch 234/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.9633\n\nEpoch 00234: loss did not improve from 1.96097\nEpoch 235/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.9502\n\nEpoch 00235: loss improved from 1.96097 to 1.95019, saving model to weights/weights_epoch_235_loss_1.9502.hdf5\nEpoch 236/301\n4/4 [==============================] - 1s 130ms/step - loss: 1.9686\n\nEpoch 00236: loss did not improve from 1.95019\nEpoch 237/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.9584\n\nEpoch 00237: loss did not improve from 1.95019\nEpoch 238/301\n4/4 [==============================] - 1s 129ms/step - loss: 1.9454\n\nEpoch 00238: loss improved from 1.95019 to 1.94540, saving model to weights/weights_epoch_238_loss_1.9454.hdf5\nEpoch 239/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.9424\n\nEpoch 00239: loss improved from 1.94540 to 1.94240, saving model to weights/weights_epoch_239_loss_1.9424.hdf5\nEpoch 240/301\n4/4 [==============================] - 1s 130ms/step - loss: 1.9203\n\nEpoch 00240: loss improved from 1.94240 to 1.92034, saving model to weights/weights_epoch_240_loss_1.9203.hdf5\nEpoch 241/301\n4/4 [==============================] - 1s 141ms/step - loss: 1.9382\n\nMy Roodaki colection:\nطر به هزار غغاغ آر سر سر سرد به با گوی باز\nکار باد بر ار نه با باد و به را به را به زیر باد و به زند ز به داد\nبا در که باد اندرون بود و به صال\nبا در مه به خوابد و باد و به صل با دل به به بخوار بود و بخ رو به تو با نباد و به زار باد\nبا خور و باد اندر به ناد و به نال\nتا باز تو بی اندر نشین و نه خود\nباز تر سر و باد و با  ای بود\nبه روز در می نه زمان آید همی\nآی  به جای ای خواهی به جوی\nمی  را به روز اندر آید همی\nآی  با گار اید و بخار مرد\nبه سان خواهی به من از به دل ان\nرا به خاش آندر از باد و به روز\nتو \n\nEpoch 00241: loss did not improve from 1.92034\nEpoch 242/301\n4/4 [==============================] - 1s 139ms/step - loss: 1.9389\n\nEpoch 00242: loss did not improve from 1.92034\nEpoch 243/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.9388\n\nEpoch 00243: loss did not improve from 1.92034\nEpoch 244/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.9034\n\nEpoch 00244: loss improved from 1.92034 to 1.90343, saving model to weights/weights_epoch_244_loss_1.9034.hdf5\nEpoch 245/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.9014\n\nEpoch 00245: loss improved from 1.90343 to 1.90136, saving model to weights/weights_epoch_245_loss_1.9014.hdf5\nEpoch 246/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.8958\n\nEpoch 00246: loss improved from 1.90136 to 1.89583, saving model to weights/weights_epoch_246_loss_1.8958.hdf5\nEpoch 247/301\n4/4 [==============================] - 1s 135ms/step - loss: 1.8925\n\nEpoch 00247: loss improved from 1.89583 to 1.89251, saving model to weights/weights_epoch_247_loss_1.8925.hdf5\nEpoch 248/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.9049\n\nEpoch 00248: loss did not improve from 1.89251\nEpoch 249/301\n4/4 [==============================] - 1s 135ms/step - loss: 1.8760\n\nEpoch 00249: loss improved from 1.89251 to 1.87595, saving model to weights/weights_epoch_249_loss_1.8760.hdf5\nEpoch 250/301\n4/4 [==============================] - 1s 145ms/step - loss: 1.8785\n\nEpoch 00250: loss did not improve from 1.87595\nEpoch 251/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.8682\n\nMy Roodaki colection:\nذ و شاده رو\nاز جهان ترا نیز و ناند به خواب\nما بر در و جان و خود آید سرا\n\nان که نید ری به درد خردش\nکه بر ور  با درد و بر درد\nگر به را که به دیت برود\nدر تو را که به نیاند بروز\nدر تشم زید و گران آرد بوی\nآن که به گرگ اندرون آمد بود\nآن که به گویی از مندر خورد\nبود مرغ بر در  ن از دراغ بود\nبه دام ترگ اندر همی بردار باد\nبا در که بر ار نه بدو در تو را\nشاد زند و زیر و نیز و غیبان\nبود\nبدان زمانه که او شاد باد\nداد برد بدان زمانه که او شاد\nباز در خران زوی و نوار و نه ن\nباز به نیز و نه دوی آن نه نه نیاب\nبا در \n\nEpoch 00251: loss improved from 1.87595 to 1.86821, saving model to weights/weights_epoch_251_loss_1.8682.hdf5\nEpoch 252/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.8720\n\nEpoch 00252: loss did not improve from 1.86821\nEpoch 253/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.8583\n\nEpoch 00253: loss improved from 1.86821 to 1.85828, saving model to weights/weights_epoch_253_loss_1.8583.hdf5\nEpoch 254/301\n4/4 [==============================] - 1s 135ms/step - loss: 1.8564\n\nEpoch 00254: loss improved from 1.85828 to 1.85635, saving model to weights/weights_epoch_254_loss_1.8564.hdf5\nEpoch 255/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.8702\n\nEpoch 00255: loss did not improve from 1.85635\nEpoch 256/301\n4/4 [==============================] - 1s 137ms/step - loss: 1.8283\n\nEpoch 00256: loss improved from 1.85635 to 1.82827, saving model to weights/weights_epoch_256_loss_1.8283.hdf5\nEpoch 257/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.8335\n\nEpoch 00257: loss did not improve from 1.82827\nEpoch 258/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.8400\n\nEpoch 00258: loss did not improve from 1.82827\nEpoch 259/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.8349\n\nEpoch 00259: loss did not improve from 1.82827\nEpoch 260/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.8220\n\nEpoch 00260: loss improved from 1.82827 to 1.82197, saving model to weights/weights_epoch_260_loss_1.8220.hdf5\nEpoch 261/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.8121\n\nMy Roodaki colection:\nغ بر تو بر از با ملک از کی بیاری\nای پاه ای نام از تو بگندان\nبا دا تا به تو نگاری از پی از خرم از در تا نگاری\nاز پا خان به دا تو بر اندان\nکه با داد اندرون با دار و نه راه\nبه راز در دی نیاند از به نوز از به چنگ اندرون بود و نه دان\nگر به را اندر به زیر و نهان\nباز به رازگان به زند نیز در از در چه رنگ اندرون بود\nبه چشم خوش نی ز ان و چنگ خوب\nکا به آن چنان که درخ می\nبه سر کان که در نیان مرد\nکاد در از در نهان آرد نا\nاگر از در گرد نی بار نگار\nکا دان در در آن آن پا دون\nکا دا گشت و در در آن آرد\nفر کر کا با \n\nEpoch 00261: loss improved from 1.82197 to 1.81213, saving model to weights/weights_epoch_261_loss_1.8121.hdf5\nEpoch 262/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.8056\n\nEpoch 00262: loss improved from 1.81213 to 1.80557, saving model to weights/weights_epoch_262_loss_1.8056.hdf5\nEpoch 263/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.7854\n\nEpoch 00263: loss improved from 1.80557 to 1.78540, saving model to weights/weights_epoch_263_loss_1.7854.hdf5\nEpoch 264/301\n4/4 [==============================] - 1s 141ms/step - loss: 1.8041\n\nEpoch 00264: loss did not improve from 1.78540\nEpoch 265/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.8004\n\nEpoch 00265: loss did not improve from 1.78540\nEpoch 266/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.7794\n\nEpoch 00266: loss improved from 1.78540 to 1.77944, saving model to weights/weights_epoch_266_loss_1.7794.hdf5\nEpoch 267/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.7589\n\nEpoch 00267: loss improved from 1.77944 to 1.75894, saving model to weights/weights_epoch_267_loss_1.7589.hdf5\nEpoch 268/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.7942\n\nEpoch 00268: loss did not improve from 1.75894\nEpoch 269/301\n4/4 [==============================] - 1s 135ms/step - loss: 1.7452\n\nEpoch 00269: loss improved from 1.75894 to 1.74520, saving model to weights/weights_epoch_269_loss_1.7452.hdf5\nEpoch 270/301\n4/4 [==============================] - 1s 135ms/step - loss: 1.7688\n\nEpoch 00270: loss did not improve from 1.74520\nEpoch 271/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.7352\n\nMy Roodaki colection:\nصذذش\nان جهان همی می می که که که از نیز اندر نگار\nبه که آخر بر من از راز را\nبه نه از رهان آرد از پی دو رنگ از درگ اندر آید آید\nهمه آن که بخوار آید زمی\nآن که باد اندرو خیش آید\nآن گه به آن که به آویش مند\nزیر پاک و خواهی به جوان\nمر  سر ار و جهان پیچ آر\nچا با آن را بباشد بی بیر\nزیر گرد آن را که اید خرد\nآن کنا ر گر نیم آمد بار\nآن که به روز رفت آندازی\nبه جای اندر نیان آرد همی\nآن جه ناواند به گیر خردان\nبود که ارغ به دان آرد آزد\nآن که با گار نیار خردم\nبه جای اندر همی باد بار\nآن را نه به آن که بیشاری\nبر پر\n\nEpoch 00271: loss improved from 1.74520 to 1.73523, saving model to weights/weights_epoch_271_loss_1.7352.hdf5\nEpoch 272/301\n4/4 [==============================] - 1s 135ms/step - loss: 1.7353\n\nEpoch 00272: loss did not improve from 1.73523\nEpoch 273/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.7469\n\nEpoch 00273: loss did not improve from 1.73523\nEpoch 274/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.7397\n\nEpoch 00274: loss did not improve from 1.73523\nEpoch 275/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.7380\n\nEpoch 00275: loss did not improve from 1.73523\nEpoch 276/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.7345\n\nEpoch 00276: loss improved from 1.73523 to 1.73454, saving model to weights/weights_epoch_276_loss_1.7345.hdf5\nEpoch 277/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.7050\n\nEpoch 00277: loss improved from 1.73454 to 1.70498, saving model to weights/weights_epoch_277_loss_1.7050.hdf5\nEpoch 278/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.7209\n\nEpoch 00278: loss did not improve from 1.70498\nEpoch 279/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.7093\n\nEpoch 00279: loss did not improve from 1.70498\nEpoch 280/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.7015\n\nEpoch 00280: loss improved from 1.70498 to 1.70155, saving model to weights/weights_epoch_280_loss_1.7015.hdf5\nEpoch 281/301\n4/4 [==============================] - 1s 135ms/step - loss: 1.6918\n\nMy Roodaki colection:\nن کرد\nرا بر مرد و خوب با نیان بود\nهمیشه شعر و از میل کیوان بود\nشد آن زمانه که او شاد به جان ر بود\nشد آن زمانه که او شاد به کردان بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او شاد به کردار بود\nشد آن زمانه که او \n\nEpoch 00281: loss improved from 1.70155 to 1.69181, saving model to weights/weights_epoch_281_loss_1.6918.hdf5\nEpoch 282/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.6705\n\nEpoch 00282: loss improved from 1.69181 to 1.67052, saving model to weights/weights_epoch_282_loss_1.6705.hdf5\nEpoch 283/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.6568\n\nEpoch 00283: loss improved from 1.67052 to 1.65676, saving model to weights/weights_epoch_283_loss_1.6568.hdf5\nEpoch 284/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.6611\n\nEpoch 00284: loss did not improve from 1.65676\nEpoch 285/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.6544\n\nEpoch 00285: loss improved from 1.65676 to 1.65436, saving model to weights/weights_epoch_285_loss_1.6544.hdf5\nEpoch 286/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.7007\n\nEpoch 00286: loss did not improve from 1.65436\nEpoch 287/301\n4/4 [==============================] - 1s 142ms/step - loss: 1.6830\n\nEpoch 00287: loss did not improve from 1.65436\nEpoch 288/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.6460\n\nEpoch 00288: loss improved from 1.65436 to 1.64598, saving model to weights/weights_epoch_288_loss_1.6460.hdf5\nEpoch 289/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.6557\n\nEpoch 00289: loss did not improve from 1.64598\nEpoch 290/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.6250\n\nEpoch 00290: loss improved from 1.64598 to 1.62504, saving model to weights/weights_epoch_290_loss_1.6250.hdf5\nEpoch 291/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.6176\n\nMy Roodaki colection:\n و دوست چو پیشکار تواند\nنین مرا  و تو فراند به میش\nهمچو نشم ای خواه دودت بود\nمن ترا باشد و نام آید غمی\nآن جه نادوار خواه آید همی\nآن جه نام از و خور آید هیر\nآن سه به توان آمد آمد میر آی مهر خواهی آمد\nآم آن ده با دل و خلله بی آن آو\n\nه به جنگ اندر نیان آید همی\nآه جای مول و خواه بدین\nای  سان را بهان آید همی\nآیر جای مود و خوار می من\nخو به جای و منان آید همی\nآیر جهان مال و خواه آید\nهمی آه جهین و میلوان آید همی\nآه جهی مار میروان آید همی\nآی جهان مام و مواو آید همی\nآه جهین مام و میرو آیماه\nآن مه به گوسر و\n\nEpoch 00291: loss improved from 1.62504 to 1.61764, saving model to weights/weights_epoch_291_loss_1.6176.hdf5\nEpoch 292/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.6198\n\nEpoch 00292: loss did not improve from 1.61764\nEpoch 293/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.6169\n\nEpoch 00293: loss improved from 1.61764 to 1.61695, saving model to weights/weights_epoch_293_loss_1.6169.hdf5\nEpoch 294/301\n4/4 [==============================] - 1s 143ms/step - loss: 1.6073\n\nEpoch 00294: loss improved from 1.61695 to 1.60733, saving model to weights/weights_epoch_294_loss_1.6073.hdf5\nEpoch 295/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.6051\n\nEpoch 00295: loss improved from 1.60733 to 1.60514, saving model to weights/weights_epoch_295_loss_1.6051.hdf5\nEpoch 296/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.5863\n\nEpoch 00296: loss improved from 1.60514 to 1.58629, saving model to weights/weights_epoch_296_loss_1.5863.hdf5\nEpoch 297/301\n4/4 [==============================] - 1s 133ms/step - loss: 1.5659\n\nEpoch 00297: loss improved from 1.58629 to 1.56591, saving model to weights/weights_epoch_297_loss_1.5659.hdf5\nEpoch 298/301\n4/4 [==============================] - 1s 134ms/step - loss: 1.6103\n\nEpoch 00298: loss did not improve from 1.56591\nEpoch 299/301\n4/4 [==============================] - 1s 135ms/step - loss: 1.5634\n\nEpoch 00299: loss improved from 1.56591 to 1.56342, saving model to weights/weights_epoch_299_loss_1.5634.hdf5\nEpoch 300/301\n4/4 [==============================] - 1s 137ms/step - loss: 1.5897\n\nEpoch 00300: loss did not improve from 1.56342\nEpoch 301/301\n4/4 [==============================] - 1s 132ms/step - loss: 1.5566\n\nMy Roodaki colection:\nق بر از بوش روشن گشت چشم خرد بران آوان کوی باشد او به راز چون ای\nمی خوشه و نا روی بی کنی\nچون گه روی و به نام آن آوی\nکون که بر و یاد و نه ن بود\nشد آن زمانه که او شاع او بندان بود\nهمیشه شعر ورا زی ملوک دیوان بود\nشد آن زمانه که شعرش همه جانان بود\nشد آن زمانه که او شاعر خرم ن بود\nشد آن زمانه که او شاع بود و خرم بود\nشد آن زمانه که شعرش همه جانان بود\nشد آن زمانه که او شاعر خرم ن بود\nشد آن زمانه که او شاع  را نانان بود\nشد آن زمانه که شعرش همه جانان بود\nشد آن زمانه که او شاعر خرم ن بود\nشد آن زمانه که او \n\nEpoch 00301: loss improved from 1.56342 to 1.55658, saving model to weights/weights_epoch_301_loss_1.5566.hdf5\n",
     "output_type": "stream"
    },
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fda79786590>"
     },
     "metadata": {}
    }
   ]
  }
 ]
}